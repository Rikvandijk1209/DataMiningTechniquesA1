{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60ca131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the path (/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1) to sys.path\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ./initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf5c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_loading import DataPreprocessor\n",
    "from mood_RNN_classifier import RNNClassifier, MoodDataset, OrdinalLabelSmoothingLoss, objective, train_epoch, train_final_model, evaluate, predict, plot_mood_predictions\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c42a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 19 outliers from 968 observations. Percentage: 1.96%\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataPreprocessor()\n",
    "train_df_split, val_df_split, test_df = data_loader.load_and_preprocess_data(\"1d\", 0.25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6928e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume train_df and test_df are loaded and preprocessed\n",
    "id_map = {id_: idx for idx, id_ in enumerate(train_df_split['id'].unique())}\n",
    "input_dim = train_df_split.drop(columns=['id', 'mood', 'date']).shape[1]\n",
    "id_count = len(id_map)\n",
    "output_dim = train_df_split['mood'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c89292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 16:58:10,079] A new study created in memory with name: no-name-12377c72-89f3-48e4-a655-436da9eaac97\n",
      "[I 2025-04-18 16:58:15,758] Trial 0 finished with value: 0.7340586334466934 and parameters: {'hidden_dim': 60, 'id_embed_dim': 8, 'lr': 0.002592281317321675, 'batch_size': 64, 'alpha': 0.20320930401716364}. Best is trial 0 with value: 0.7340586334466934.\n",
      "[I 2025-04-18 16:58:15,942] Trial 1 finished with value: 0.8947119861841202 and parameters: {'hidden_dim': 101, 'id_embed_dim': 5, 'lr': 0.0005991287825388612, 'batch_size': 32, 'alpha': 0.2272982415687634}. Best is trial 0 with value: 0.7340586334466934.\n",
      "[I 2025-04-18 16:58:16,036] Trial 2 finished with value: 0.7086691856384277 and parameters: {'hidden_dim': 86, 'id_embed_dim': 10, 'lr': 0.008837554845971044, 'batch_size': 128, 'alpha': 0.1935049821736441}. Best is trial 2 with value: 0.7086691856384277.\n",
      "[I 2025-04-18 16:58:16,220] Trial 3 finished with value: 0.6934059709310532 and parameters: {'hidden_dim': 112, 'id_embed_dim': 8, 'lr': 0.0038670579833555685, 'batch_size': 32, 'alpha': 0.18645768397910564}. Best is trial 3 with value: 0.6934059709310532.\n",
      "[I 2025-04-18 16:58:16,311] Trial 4 finished with value: 1.5169647336006165 and parameters: {'hidden_dim': 62, 'id_embed_dim': 9, 'lr': 0.00012964040865771567, 'batch_size': 128, 'alpha': 0.12531116340194656}. Best is trial 3 with value: 0.6934059709310532.\n",
      "[I 2025-04-18 16:58:16,497] Trial 5 finished with value: 1.3380450010299683 and parameters: {'hidden_dim': 119, 'id_embed_dim': 5, 'lr': 0.00015720146297851142, 'batch_size': 32, 'alpha': 0.1522424561725258}. Best is trial 3 with value: 0.6934059709310532.\n",
      "[I 2025-04-18 16:58:16,590] Trial 6 finished with value: 0.6344474852085114 and parameters: {'hidden_dim': 34, 'id_embed_dim': 4, 'lr': 0.001130699027341317, 'batch_size': 128, 'alpha': 0.04141377019863276}. Best is trial 6 with value: 0.6344474852085114.\n",
      "[I 2025-04-18 16:58:16,775] Trial 7 finished with value: 0.7415253221988678 and parameters: {'hidden_dim': 111, 'id_embed_dim': 12, 'lr': 0.006641514689187065, 'batch_size': 32, 'alpha': 0.163834869240702}. Best is trial 6 with value: 0.6344474852085114.\n",
      "[I 2025-04-18 16:58:16,898] Trial 8 finished with value: 0.637920193374157 and parameters: {'hidden_dim': 105, 'id_embed_dim': 12, 'lr': 0.0016918460004166887, 'batch_size': 64, 'alpha': 0.13879127992149645}. Best is trial 6 with value: 0.6344474852085114.\n",
      "[I 2025-04-18 16:58:16,994] Trial 9 finished with value: 0.8975580632686615 and parameters: {'hidden_dim': 100, 'id_embed_dim': 11, 'lr': 0.00232360192321885, 'batch_size': 128, 'alpha': 0.2589683109018255}. Best is trial 6 with value: 0.6344474852085114.\n",
      "[I 2025-04-18 16:58:17,090] Trial 10 finished with value: 0.9013516902923584 and parameters: {'hidden_dim': 33, 'id_embed_dim': 16, 'lr': 0.000556701031011697, 'batch_size': 128, 'alpha': 0.04136045972714887}. Best is trial 6 with value: 0.6344474852085114.\n",
      "[I 2025-04-18 16:58:17,208] Trial 11 finished with value: 0.23858970403671265 and parameters: {'hidden_dim': 39, 'id_embed_dim': 14, 'lr': 0.0010819809096540834, 'batch_size': 64, 'alpha': 0.0232008231629985}. Best is trial 11 with value: 0.23858970403671265.\n",
      "[I 2025-04-18 16:58:17,328] Trial 12 finished with value: 0.18862121552228928 and parameters: {'hidden_dim': 38, 'id_embed_dim': 15, 'lr': 0.0008902903316044949, 'batch_size': 64, 'alpha': 0.013498327510772304}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:17,449] Trial 13 finished with value: 1.0323155969381332 and parameters: {'hidden_dim': 53, 'id_embed_dim': 16, 'lr': 0.00034886913743756087, 'batch_size': 64, 'alpha': 0.08333153856714082}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:17,572] Trial 14 finished with value: 0.20584945753216743 and parameters: {'hidden_dim': 46, 'id_embed_dim': 14, 'lr': 0.0008636287400404457, 'batch_size': 64, 'alpha': 0.01714069237293095}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:17,697] Trial 15 finished with value: 0.9830055981874466 and parameters: {'hidden_dim': 77, 'id_embed_dim': 14, 'lr': 0.0003113533206305078, 'batch_size': 64, 'alpha': 0.0782569988722704}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:17,819] Trial 16 finished with value: 0.23566099628806114 and parameters: {'hidden_dim': 47, 'id_embed_dim': 14, 'lr': 0.0005800947122157269, 'batch_size': 64, 'alpha': 0.011155951119906554}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:17,942] Trial 17 finished with value: 1.1473689675331116 and parameters: {'hidden_dim': 72, 'id_embed_dim': 15, 'lr': 0.0002950226797438737, 'batch_size': 64, 'alpha': 0.09188205705058092}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,063] Trial 18 finished with value: 0.4906550571322441 and parameters: {'hidden_dim': 45, 'id_embed_dim': 13, 'lr': 0.0008179973164227517, 'batch_size': 64, 'alpha': 0.0571153318028047}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,188] Trial 19 finished with value: 0.5094858035445213 and parameters: {'hidden_dim': 65, 'id_embed_dim': 16, 'lr': 0.001523956150367348, 'batch_size': 64, 'alpha': 0.10865156471300814}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,309] Trial 20 finished with value: 0.32709040492773056 and parameters: {'hidden_dim': 52, 'id_embed_dim': 12, 'lr': 0.003999705459478575, 'batch_size': 64, 'alpha': 0.06409207777197609}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,429] Trial 21 finished with value: 0.21868368983268738 and parameters: {'hidden_dim': 47, 'id_embed_dim': 14, 'lr': 0.0006066896287536204, 'batch_size': 64, 'alpha': 0.011125769270418967}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,548] Trial 22 finished with value: 0.41634607315063477 and parameters: {'hidden_dim': 42, 'id_embed_dim': 15, 'lr': 0.0004332396435113478, 'batch_size': 64, 'alpha': 0.013335046493934204}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,670] Trial 23 finished with value: 0.9693223387002945 and parameters: {'hidden_dim': 54, 'id_embed_dim': 13, 'lr': 0.00018883111460443722, 'batch_size': 64, 'alpha': 0.04064800222955415}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,789] Trial 24 finished with value: 1.2061688899993896 and parameters: {'hidden_dim': 39, 'id_embed_dim': 15, 'lr': 0.0008684429825719806, 'batch_size': 64, 'alpha': 0.28527570444931993}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:18,912] Trial 25 finished with value: 0.8524539768695831 and parameters: {'hidden_dim': 70, 'id_embed_dim': 13, 'lr': 0.00021852218248424402, 'batch_size': 64, 'alpha': 0.03886106924162809}. Best is trial 12 with value: 0.18862121552228928.\n",
      "[I 2025-04-18 16:58:19,038] Trial 26 finished with value: 0.07528097927570343 and parameters: {'hidden_dim': 87, 'id_embed_dim': 11, 'lr': 0.0015042726528088635, 'batch_size': 64, 'alpha': 0.010442883007001878}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:19,186] Trial 27 finished with value: 0.3611394092440605 and parameters: {'hidden_dim': 86, 'id_embed_dim': 10, 'lr': 0.0017876650672381672, 'batch_size': 64, 'alpha': 0.05808610132015447}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:19,377] Trial 28 finished with value: 0.5006497912108898 and parameters: {'hidden_dim': 90, 'id_embed_dim': 11, 'lr': 0.001158898781343903, 'batch_size': 32, 'alpha': 0.10041196236395124}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:19,500] Trial 29 finished with value: 0.21880364418029785 and parameters: {'hidden_dim': 59, 'id_embed_dim': 7, 'lr': 0.003066631947818909, 'batch_size': 64, 'alpha': 0.030820398054622153}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:19,629] Trial 30 finished with value: 0.37598784267902374 and parameters: {'hidden_dim': 94, 'id_embed_dim': 7, 'lr': 0.0023782484091873684, 'batch_size': 64, 'alpha': 0.06526443411871338}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:19,750] Trial 31 finished with value: 0.21584510430693626 and parameters: {'hidden_dim': 48, 'id_embed_dim': 14, 'lr': 0.0007384831073320544, 'batch_size': 64, 'alpha': 0.015187362865865322}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:19,876] Trial 32 finished with value: 0.28303317725658417 and parameters: {'hidden_dim': 79, 'id_embed_dim': 15, 'lr': 0.0008286871112404343, 'batch_size': 64, 'alpha': 0.030402111195134203}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:19,995] Trial 33 finished with value: 0.8840639740228653 and parameters: {'hidden_dim': 32, 'id_embed_dim': 13, 'lr': 0.001495324603784985, 'batch_size': 64, 'alpha': 0.2215723223957386}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:20,116] Trial 34 finished with value: 0.5334306284785271 and parameters: {'hidden_dim': 56, 'id_embed_dim': 11, 'lr': 0.0007591686313258093, 'batch_size': 64, 'alpha': 0.05649323640632289}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:20,298] Trial 35 finished with value: 0.15560152288526297 and parameters: {'hidden_dim': 68, 'id_embed_dim': 9, 'lr': 0.0012264634427297532, 'batch_size': 32, 'alpha': 0.022622951330073117}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:20,477] Trial 36 finished with value: 0.5276691131293774 and parameters: {'hidden_dim': 68, 'id_embed_dim': 8, 'lr': 0.0013871983331563578, 'batch_size': 32, 'alpha': 0.11016309460931445}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:20,661] Trial 37 finished with value: 0.37162589840590954 and parameters: {'hidden_dim': 85, 'id_embed_dim': 9, 'lr': 0.002044534468143851, 'batch_size': 32, 'alpha': 0.0728825716479646}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:20,842] Trial 38 finished with value: 0.2533875424414873 and parameters: {'hidden_dim': 74, 'id_embed_dim': 9, 'lr': 0.0032867376973712203, 'batch_size': 32, 'alpha': 0.047158044248272406}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:21,022] Trial 39 finished with value: 1.6059443652629852 and parameters: {'hidden_dim': 64, 'id_embed_dim': 10, 'lr': 0.00010094876018471545, 'batch_size': 32, 'alpha': 0.1782632084764554}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:21,124] Trial 40 finished with value: 0.6769543588161469 and parameters: {'hidden_dim': 95, 'id_embed_dim': 7, 'lr': 0.0004214606779136092, 'batch_size': 128, 'alpha': 0.026182374970080444}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:21,300] Trial 41 finished with value: 0.16270069032907486 and parameters: {'hidden_dim': 49, 'id_embed_dim': 12, 'lr': 0.0011891781742280887, 'batch_size': 32, 'alpha': 0.022041736674350887}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:21,475] Trial 42 finished with value: 0.1905879331752658 and parameters: {'hidden_dim': 39, 'id_embed_dim': 12, 'lr': 0.001167733723049256, 'batch_size': 32, 'alpha': 0.02660431778144306}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:21,649] Trial 43 finished with value: 0.2883978635072708 and parameters: {'hidden_dim': 38, 'id_embed_dim': 11, 'lr': 0.0012673654717207416, 'batch_size': 32, 'alpha': 0.04848508621308737}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:21,827] Trial 44 finished with value: 0.204516114667058 and parameters: {'hidden_dim': 58, 'id_embed_dim': 12, 'lr': 0.005259711412957641, 'batch_size': 32, 'alpha': 0.03131392946793236}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,010] Trial 45 finished with value: 0.18637305591255426 and parameters: {'hidden_dim': 82, 'id_embed_dim': 10, 'lr': 0.001017729274754174, 'batch_size': 32, 'alpha': 0.02541530694162209}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,193] Trial 46 finished with value: 0.3043054286390543 and parameters: {'hidden_dim': 80, 'id_embed_dim': 8, 'lr': 0.0019162120334184328, 'batch_size': 32, 'alpha': 0.04922299757302952}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,383] Trial 47 finished with value: 0.6677863895893097 and parameters: {'hidden_dim': 121, 'id_embed_dim': 10, 'lr': 0.0006311310096099177, 'batch_size': 32, 'alpha': 0.1343319426192303}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,570] Trial 48 finished with value: 0.43447013944387436 and parameters: {'hidden_dim': 105, 'id_embed_dim': 9, 'lr': 0.0009732762450061391, 'batch_size': 32, 'alpha': 0.07503368282454725}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,669] Trial 49 finished with value: 0.17649776488542557 and parameters: {'hidden_dim': 82, 'id_embed_dim': 11, 'lr': 0.002482712289305799, 'batch_size': 128, 'alpha': 0.022703474749276493}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,767] Trial 50 finished with value: 0.4510400891304016 and parameters: {'hidden_dim': 83, 'id_embed_dim': 11, 'lr': 0.0027700177916526944, 'batch_size': 128, 'alpha': 0.08675917988756143}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,868] Trial 51 finished with value: 0.2829280197620392 and parameters: {'hidden_dim': 76, 'id_embed_dim': 10, 'lr': 0.0021961085388382436, 'batch_size': 128, 'alpha': 0.03591185076336642}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:22,969] Trial 52 finished with value: 0.3266196846961975 and parameters: {'hidden_dim': 90, 'id_embed_dim': 9, 'lr': 0.0009844225601916402, 'batch_size': 128, 'alpha': 0.021721079217784982}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:23,069] Trial 53 finished with value: 0.1842823550105095 and parameters: {'hidden_dim': 90, 'id_embed_dim': 11, 'lr': 0.0017792615546925687, 'batch_size': 128, 'alpha': 0.020879260271956218}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:23,171] Trial 54 finished with value: 0.20587309449911118 and parameters: {'hidden_dim': 90, 'id_embed_dim': 11, 'lr': 0.00164690094203623, 'batch_size': 128, 'alpha': 0.020926339654998828}. Best is trial 26 with value: 0.07528097927570343.\n",
      "[I 2025-04-18 16:58:23,272] Trial 55 finished with value: 0.06029924936592579 and parameters: {'hidden_dim': 100, 'id_embed_dim': 12, 'lr': 0.0040830347392131466, 'batch_size': 128, 'alpha': 0.010027551380637196}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:23,378] Trial 56 finished with value: 0.2598414421081543 and parameters: {'hidden_dim': 103, 'id_embed_dim': 12, 'lr': 0.005090497208337178, 'batch_size': 128, 'alpha': 0.04347647990239157}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:23,483] Trial 57 finished with value: 0.07274459674954414 and parameters: {'hidden_dim': 99, 'id_embed_dim': 11, 'lr': 0.009458382888542914, 'batch_size': 128, 'alpha': 0.010406798968788766}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:23,588] Trial 58 finished with value: 0.7159789502620697 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.008830605162849952, 'batch_size': 128, 'alpha': 0.20772026392948098}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:23,689] Trial 59 finished with value: 0.06241209618747234 and parameters: {'hidden_dim': 100, 'id_embed_dim': 12, 'lr': 0.009305990987089426, 'batch_size': 128, 'alpha': 0.01113205229033995}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:23,792] Trial 60 finished with value: 0.7989849746227264 and parameters: {'hidden_dim': 99, 'id_embed_dim': 13, 'lr': 0.008586762924677775, 'batch_size': 128, 'alpha': 0.24427153230758306}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:23,896] Trial 61 finished with value: 0.09119591861963272 and parameters: {'hidden_dim': 110, 'id_embed_dim': 12, 'lr': 0.006796159567284036, 'batch_size': 128, 'alpha': 0.013030158656362957}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:24,000] Trial 62 finished with value: 0.07348278537392616 and parameters: {'hidden_dim': 116, 'id_embed_dim': 12, 'lr': 0.0069850419117914245, 'batch_size': 128, 'alpha': 0.01175166848744986}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:24,106] Trial 63 finished with value: 0.07951466366648674 and parameters: {'hidden_dim': 128, 'id_embed_dim': 13, 'lr': 0.0071840272932909835, 'batch_size': 128, 'alpha': 0.01112167558173196}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:24,214] Trial 64 finished with value: 0.073518555611372 and parameters: {'hidden_dim': 126, 'id_embed_dim': 13, 'lr': 0.007117521591610465, 'batch_size': 128, 'alpha': 0.01020794290493904}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:24,321] Trial 65 finished with value: 0.0938117541372776 and parameters: {'hidden_dim': 126, 'id_embed_dim': 13, 'lr': 0.009887004296037227, 'batch_size': 128, 'alpha': 0.0127978442593315}. Best is trial 55 with value: 0.06029924936592579.\n",
      "[I 2025-04-18 16:58:24,444] Trial 66 finished with value: 0.059058818966150284 and parameters: {'hidden_dim': 118, 'id_embed_dim': 13, 'lr': 0.007073717613676895, 'batch_size': 128, 'alpha': 0.010153582670224658}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:24,557] Trial 67 finished with value: 0.21295543760061264 and parameters: {'hidden_dim': 116, 'id_embed_dim': 14, 'lr': 0.005348886670209178, 'batch_size': 128, 'alpha': 0.03495985263203556}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:24,670] Trial 68 finished with value: 0.21762806922197342 and parameters: {'hidden_dim': 120, 'id_embed_dim': 12, 'lr': 0.006026662073522493, 'batch_size': 128, 'alpha': 0.03752279625888531}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:24,784] Trial 69 finished with value: 0.06624341197311878 and parameters: {'hidden_dim': 107, 'id_embed_dim': 13, 'lr': 0.004326859953529298, 'batch_size': 128, 'alpha': 0.010056519263737752}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:24,896] Trial 70 finished with value: 0.32521188259124756 and parameters: {'hidden_dim': 107, 'id_embed_dim': 13, 'lr': 0.004091589625295956, 'batch_size': 128, 'alpha': 0.05405741597058424}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,013] Trial 71 finished with value: 0.08079934492707253 and parameters: {'hidden_dim': 123, 'id_embed_dim': 13, 'lr': 0.00804765179227572, 'batch_size': 128, 'alpha': 0.011934209182622335}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,127] Trial 72 finished with value: 0.061750881373882294 and parameters: {'hidden_dim': 114, 'id_embed_dim': 12, 'lr': 0.004571773561577807, 'batch_size': 128, 'alpha': 0.010106409527937898}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,242] Trial 73 finished with value: 0.17710834741592407 and parameters: {'hidden_dim': 116, 'id_embed_dim': 14, 'lr': 0.00750976721396767, 'batch_size': 128, 'alpha': 0.032032524134357004}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,357] Trial 74 finished with value: 0.3538159132003784 and parameters: {'hidden_dim': 110, 'id_embed_dim': 12, 'lr': 0.004577329143100064, 'batch_size': 128, 'alpha': 0.06662216056409694}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,473] Trial 75 finished with value: 0.2547650933265686 and parameters: {'hidden_dim': 117, 'id_embed_dim': 13, 'lr': 0.006105918488196197, 'batch_size': 128, 'alpha': 0.042198667019147366}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,587] Trial 76 finished with value: 0.9014892280101776 and parameters: {'hidden_dim': 98, 'id_embed_dim': 14, 'lr': 0.0034375410653723184, 'batch_size': 128, 'alpha': 0.29423886610638317}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,703] Trial 77 finished with value: 0.17513835430145264 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.006194924046479462, 'batch_size': 128, 'alpha': 0.030039344062243706}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,817] Trial 78 finished with value: 0.6425836384296417 and parameters: {'hidden_dim': 108, 'id_embed_dim': 13, 'lr': 0.009843574227539034, 'batch_size': 128, 'alpha': 0.1540391896331999}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:25,933] Trial 79 finished with value: 0.11476259306073189 and parameters: {'hidden_dim': 125, 'id_embed_dim': 11, 'lr': 0.00454143265984632, 'batch_size': 128, 'alpha': 0.018568375173414343}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,046] Trial 80 finished with value: 0.25790441036224365 and parameters: {'hidden_dim': 101, 'id_embed_dim': 12, 'lr': 0.007806706724383856, 'batch_size': 128, 'alpha': 0.037124433198629785}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,156] Trial 81 finished with value: 0.09141510725021362 and parameters: {'hidden_dim': 97, 'id_embed_dim': 11, 'lr': 0.004654378082472113, 'batch_size': 128, 'alpha': 0.010656441600511558}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,271] Trial 82 finished with value: 0.11837126314640045 and parameters: {'hidden_dim': 105, 'id_embed_dim': 12, 'lr': 0.0037929468124261612, 'batch_size': 128, 'alpha': 0.017013668819127876}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,382] Trial 83 finished with value: 0.1978932023048401 and parameters: {'hidden_dim': 94, 'id_embed_dim': 5, 'lr': 0.005654825123550041, 'batch_size': 128, 'alpha': 0.028009761144295996}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,498] Trial 84 finished with value: 0.13946427404880524 and parameters: {'hidden_dim': 118, 'id_embed_dim': 12, 'lr': 0.007019634147051022, 'batch_size': 128, 'alpha': 0.018423484004132414}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,612] Trial 85 finished with value: 0.2592837065458298 and parameters: {'hidden_dim': 103, 'id_embed_dim': 13, 'lr': 0.009215425508570255, 'batch_size': 128, 'alpha': 0.043953828460569305}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,730] Trial 86 finished with value: 0.0741063542664051 and parameters: {'hidden_dim': 113, 'id_embed_dim': 11, 'lr': 0.008236337794122355, 'batch_size': 128, 'alpha': 0.010149008644593761}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,845] Trial 87 finished with value: 0.16483384370803833 and parameters: {'hidden_dim': 115, 'id_embed_dim': 13, 'lr': 0.008084067851239022, 'batch_size': 128, 'alpha': 0.028685651170689944}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:26,961] Trial 88 finished with value: 0.6369418203830719 and parameters: {'hidden_dim': 122, 'id_embed_dim': 11, 'lr': 0.006467939590617136, 'batch_size': 128, 'alpha': 0.16866932166194157}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,070] Trial 89 finished with value: 0.2637675032019615 and parameters: {'hidden_dim': 112, 'id_embed_dim': 14, 'lr': 0.008755907068775291, 'batch_size': 128, 'alpha': 0.04984196696115561}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,187] Trial 90 finished with value: 0.10987704247236252 and parameters: {'hidden_dim': 114, 'id_embed_dim': 12, 'lr': 0.004890976472324971, 'batch_size': 128, 'alpha': 0.017718399991873295}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,301] Trial 91 finished with value: 0.1536930799484253 and parameters: {'hidden_dim': 108, 'id_embed_dim': 11, 'lr': 0.005632707882842657, 'batch_size': 128, 'alpha': 0.025086386714517333}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,418] Trial 92 finished with value: 0.07006129249930382 and parameters: {'hidden_dim': 119, 'id_embed_dim': 10, 'lr': 0.003963324169570166, 'batch_size': 128, 'alpha': 0.01075368746837826}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,535] Trial 93 finished with value: 0.12277771905064583 and parameters: {'hidden_dim': 118, 'id_embed_dim': 10, 'lr': 0.00285031907802615, 'batch_size': 128, 'alpha': 0.016643792693910975}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,654] Trial 94 finished with value: 0.07254114747047424 and parameters: {'hidden_dim': 124, 'id_embed_dim': 10, 'lr': 0.0038231974433283193, 'batch_size': 128, 'alpha': 0.010129584781412309}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,770] Trial 95 finished with value: 0.22124817222356796 and parameters: {'hidden_dim': 124, 'id_embed_dim': 10, 'lr': 0.0035965788806141485, 'batch_size': 128, 'alpha': 0.03137077194228217}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:27,883] Trial 96 finished with value: 0.15470878779888153 and parameters: {'hidden_dim': 128, 'id_embed_dim': 10, 'lr': 0.004259162234011842, 'batch_size': 128, 'alpha': 0.023529102223266814}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,001] Trial 97 finished with value: 0.2194065824151039 and parameters: {'hidden_dim': 119, 'id_embed_dim': 12, 'lr': 0.003151769936226764, 'batch_size': 128, 'alpha': 0.03704804969829033}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,119] Trial 98 finished with value: 0.12573742121458054 and parameters: {'hidden_dim': 122, 'id_embed_dim': 13, 'lr': 0.006979846890977979, 'batch_size': 128, 'alpha': 0.01740472411352368}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,236] Trial 99 finished with value: 0.1778276488184929 and parameters: {'hidden_dim': 120, 'id_embed_dim': 10, 'lr': 0.005437616201747872, 'batch_size': 128, 'alpha': 0.025775224705477273}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,355] Trial 100 finished with value: 0.06718962639570236 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.004191239702124632, 'batch_size': 128, 'alpha': 0.010579207436593708}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,472] Trial 101 finished with value: 0.11496759206056595 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.004010665642137281, 'batch_size': 128, 'alpha': 0.017928453134376736}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,586] Trial 102 finished with value: 0.07124835252761841 and parameters: {'hidden_dim': 110, 'id_embed_dim': 12, 'lr': 0.0029129297929687235, 'batch_size': 128, 'alpha': 0.010314496691381028}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,698] Trial 103 finished with value: 0.0852292999625206 and parameters: {'hidden_dim': 102, 'id_embed_dim': 4, 'lr': 0.003785086228649195, 'batch_size': 128, 'alpha': 0.010131254165802956}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,812] Trial 104 finished with value: 0.2228090763092041 and parameters: {'hidden_dim': 110, 'id_embed_dim': 12, 'lr': 0.0029741874444552804, 'batch_size': 128, 'alpha': 0.0324116025633105}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:28,930] Trial 105 finished with value: 0.15834452211856842 and parameters: {'hidden_dim': 105, 'id_embed_dim': 11, 'lr': 0.0026625426013174545, 'batch_size': 128, 'alpha': 0.021652666737579217}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,042] Trial 106 finished with value: 0.1282638981938362 and parameters: {'hidden_dim': 107, 'id_embed_dim': 9, 'lr': 0.00425092557440335, 'batch_size': 128, 'alpha': 0.016241195242534846}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,160] Trial 107 finished with value: 0.25678621232509613 and parameters: {'hidden_dim': 115, 'id_embed_dim': 12, 'lr': 0.003340652025636201, 'batch_size': 128, 'alpha': 0.041016447044125055}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,274] Trial 108 finished with value: 0.1644057258963585 and parameters: {'hidden_dim': 97, 'id_embed_dim': 11, 'lr': 0.00502488141109669, 'batch_size': 128, 'alpha': 0.025959600361811948}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,420] Trial 109 finished with value: 0.19230222702026367 and parameters: {'hidden_dim': 110, 'id_embed_dim': 10, 'lr': 0.005753117484352188, 'batch_size': 128, 'alpha': 0.03416183856945429}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,542] Trial 110 finished with value: 0.1552637629210949 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.0035562744876744875, 'batch_size': 128, 'alpha': 0.021750785551716043}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,662] Trial 111 finished with value: 0.066826730966568 and parameters: {'hidden_dim': 125, 'id_embed_dim': 13, 'lr': 0.006611316208190484, 'batch_size': 128, 'alpha': 0.010281569126793072}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,780] Trial 112 finished with value: 0.10718744993209839 and parameters: {'hidden_dim': 123, 'id_embed_dim': 13, 'lr': 0.006618042159389312, 'batch_size': 128, 'alpha': 0.015802957482341787}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:29,900] Trial 113 finished with value: 0.07469987869262695 and parameters: {'hidden_dim': 117, 'id_embed_dim': 12, 'lr': 0.0044173899540564734, 'batch_size': 128, 'alpha': 0.01035669144345412}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,019] Trial 114 finished with value: 0.17323002219200134 and parameters: {'hidden_dim': 119, 'id_embed_dim': 12, 'lr': 0.004938870616837648, 'batch_size': 128, 'alpha': 0.02920806496683681}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,139] Trial 115 finished with value: 0.10421489924192429 and parameters: {'hidden_dim': 124, 'id_embed_dim': 13, 'lr': 0.006138453048349633, 'batch_size': 128, 'alpha': 0.01627772378261759}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,256] Trial 116 finished with value: 0.14432930201292038 and parameters: {'hidden_dim': 128, 'id_embed_dim': 12, 'lr': 0.009451073006946026, 'batch_size': 128, 'alpha': 0.02298382970092655}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,372] Trial 117 finished with value: 0.07669862732291222 and parameters: {'hidden_dim': 105, 'id_embed_dim': 13, 'lr': 0.007459502729631287, 'batch_size': 128, 'alpha': 0.010198087326322283}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,484] Trial 118 finished with value: 0.8772614002227783 and parameters: {'hidden_dim': 100, 'id_embed_dim': 10, 'lr': 0.003865908175727371, 'batch_size': 128, 'alpha': 0.2649911860795717}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,594] Trial 119 finished with value: 0.5300539880990982 and parameters: {'hidden_dim': 92, 'id_embed_dim': 14, 'lr': 0.0047374152216905704, 'batch_size': 128, 'alpha': 0.12027727630338499}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,713] Trial 120 finished with value: 0.18161476403474808 and parameters: {'hidden_dim': 111, 'id_embed_dim': 12, 'lr': 0.00309059636076844, 'batch_size': 128, 'alpha': 0.026509407460998495}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,832] Trial 121 finished with value: 0.1025506779551506 and parameters: {'hidden_dim': 126, 'id_embed_dim': 13, 'lr': 0.007308767743335513, 'batch_size': 128, 'alpha': 0.015810107059090844}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:30,951] Trial 122 finished with value: 0.11847677081823349 and parameters: {'hidden_dim': 121, 'id_embed_dim': 14, 'lr': 0.0064970194781338465, 'batch_size': 128, 'alpha': 0.02060884757083778}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,069] Trial 123 finished with value: 0.0628751888871193 and parameters: {'hidden_dim': 126, 'id_embed_dim': 13, 'lr': 0.008351810899157408, 'batch_size': 128, 'alpha': 0.010375040284649058}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,187] Trial 124 finished with value: 0.07292567193508148 and parameters: {'hidden_dim': 124, 'id_embed_dim': 12, 'lr': 0.008702738817438774, 'batch_size': 128, 'alpha': 0.010179430720141158}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,306] Trial 125 finished with value: 0.2366766482591629 and parameters: {'hidden_dim': 124, 'id_embed_dim': 13, 'lr': 0.009997834991889899, 'batch_size': 128, 'alpha': 0.03480426928529194}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,426] Trial 126 finished with value: 0.13006088510155678 and parameters: {'hidden_dim': 122, 'id_embed_dim': 13, 'lr': 0.00863636955110347, 'batch_size': 128, 'alpha': 0.02019821081623057}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,543] Trial 127 finished with value: 0.09491373226046562 and parameters: {'hidden_dim': 128, 'id_embed_dim': 11, 'lr': 0.008086323656182044, 'batch_size': 128, 'alpha': 0.015314772952382087}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,660] Trial 128 finished with value: 0.15472611784934998 and parameters: {'hidden_dim': 125, 'id_embed_dim': 12, 'lr': 0.008984605818959168, 'batch_size': 128, 'alpha': 0.02562757986170286}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,776] Trial 129 finished with value: 0.21498645842075348 and parameters: {'hidden_dim': 120, 'id_embed_dim': 14, 'lr': 0.0023356883302464455, 'batch_size': 128, 'alpha': 0.030660493015466313}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:31,895] Trial 130 finished with value: 0.2642704024910927 and parameters: {'hidden_dim': 118, 'id_embed_dim': 10, 'lr': 0.007741081684197336, 'batch_size': 128, 'alpha': 0.046053279170238784}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,014] Trial 131 finished with value: 0.09558432921767235 and parameters: {'hidden_dim': 115, 'id_embed_dim': 12, 'lr': 0.005546160029406583, 'batch_size': 128, 'alpha': 0.014129025317110737}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,133] Trial 132 finished with value: 0.0767911784350872 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.008968404358678468, 'batch_size': 128, 'alpha': 0.010749023480945384}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,246] Trial 133 finished with value: 0.06441276334226131 and parameters: {'hidden_dim': 108, 'id_embed_dim': 13, 'lr': 0.006030857162201309, 'batch_size': 128, 'alpha': 0.010272106893576771}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,359] Trial 134 finished with value: 0.14104391261935234 and parameters: {'hidden_dim': 103, 'id_embed_dim': 13, 'lr': 0.00423428440894939, 'batch_size': 128, 'alpha': 0.020736477661318392}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,474] Trial 135 finished with value: 0.105876874178648 and parameters: {'hidden_dim': 108, 'id_embed_dim': 13, 'lr': 0.005209651583480141, 'batch_size': 128, 'alpha': 0.017188584473062707}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,594] Trial 136 finished with value: 0.062202345579862595 and parameters: {'hidden_dim': 126, 'id_embed_dim': 13, 'lr': 0.006105275761365088, 'batch_size': 128, 'alpha': 0.010297021355127367}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,705] Trial 137 finished with value: 0.16025538742542267 and parameters: {'hidden_dim': 95, 'id_embed_dim': 14, 'lr': 0.006203417802720728, 'batch_size': 128, 'alpha': 0.026699418710138884}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,827] Trial 138 finished with value: 0.2427326813340187 and parameters: {'hidden_dim': 126, 'id_embed_dim': 13, 'lr': 0.0036090077893066322, 'batch_size': 128, 'alpha': 0.0379234477626425}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:32,944] Trial 139 finished with value: 0.125713761895895 and parameters: {'hidden_dim': 122, 'id_embed_dim': 13, 'lr': 0.0046589672585069875, 'batch_size': 128, 'alpha': 0.02072036107832337}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,061] Trial 140 finished with value: 0.09917083010077477 and parameters: {'hidden_dim': 106, 'id_embed_dim': 14, 'lr': 0.005655772224855157, 'batch_size': 128, 'alpha': 0.01609557840034018}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,181] Trial 141 finished with value: 0.11822378262877464 and parameters: {'hidden_dim': 127, 'id_embed_dim': 13, 'lr': 0.008023805377198295, 'batch_size': 128, 'alpha': 0.011481482892468822}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,300] Trial 142 finished with value: 0.13561441749334335 and parameters: {'hidden_dim': 123, 'id_embed_dim': 11, 'lr': 0.006664387446895565, 'batch_size': 128, 'alpha': 0.022342836003608837}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,419] Trial 143 finished with value: 0.709709644317627 and parameters: {'hidden_dim': 124, 'id_embed_dim': 12, 'lr': 0.005194926989418768, 'batch_size': 128, 'alpha': 0.19630992694738675}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,538] Trial 144 finished with value: 0.0714806579053402 and parameters: {'hidden_dim': 120, 'id_embed_dim': 9, 'lr': 0.00390955649488515, 'batch_size': 128, 'alpha': 0.01004831439489566}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,660] Trial 145 finished with value: 0.20648832619190216 and parameters: {'hidden_dim': 118, 'id_embed_dim': 9, 'lr': 0.003952711787212253, 'batch_size': 128, 'alpha': 0.027815194252541676}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,776] Trial 146 finished with value: 0.12447496131062508 and parameters: {'hidden_dim': 111, 'id_embed_dim': 9, 'lr': 0.00321423734202614, 'batch_size': 128, 'alpha': 0.016088368701365645}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:33,893] Trial 147 finished with value: 0.15614959597587585 and parameters: {'hidden_dim': 120, 'id_embed_dim': 8, 'lr': 0.004108601669639611, 'batch_size': 128, 'alpha': 0.02197941119630572}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,008] Trial 148 finished with value: 0.078069057315588 and parameters: {'hidden_dim': 109, 'id_embed_dim': 9, 'lr': 0.0036552134783547387, 'batch_size': 128, 'alpha': 0.010994102877347752}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,119] Trial 149 finished with value: 0.204849474132061 and parameters: {'hidden_dim': 102, 'id_embed_dim': 10, 'lr': 0.004746511434119208, 'batch_size': 128, 'alpha': 0.031039824234901654}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,232] Trial 150 finished with value: 0.09346016496419907 and parameters: {'hidden_dim': 99, 'id_embed_dim': 13, 'lr': 0.005888214994947124, 'batch_size': 128, 'alpha': 0.016499804924939174}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,352] Trial 151 finished with value: 0.0894499309360981 and parameters: {'hidden_dim': 125, 'id_embed_dim': 12, 'lr': 0.007450359044756148, 'batch_size': 128, 'alpha': 0.014515676199096668}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,469] Trial 152 finished with value: 0.07820342481136322 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.008620980316646295, 'batch_size': 128, 'alpha': 0.011364163036120771}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,588] Trial 153 finished with value: 0.15725311636924744 and parameters: {'hidden_dim': 121, 'id_embed_dim': 13, 'lr': 0.009867718936539439, 'batch_size': 128, 'alpha': 0.023273565071160227}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,705] Trial 154 finished with value: 0.06693362444639206 and parameters: {'hidden_dim': 114, 'id_embed_dim': 12, 'lr': 0.004550521942271165, 'batch_size': 128, 'alpha': 0.010723150840600581}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,823] Trial 155 finished with value: 0.13545705378055573 and parameters: {'hidden_dim': 114, 'id_embed_dim': 15, 'lr': 0.0033590328385016913, 'batch_size': 128, 'alpha': 0.019545946871670765}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:34,966] Trial 156 finished with value: 0.16671937704086304 and parameters: {'hidden_dim': 116, 'id_embed_dim': 12, 'lr': 0.004488668174437288, 'batch_size': 64, 'alpha': 0.027371121285542484}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:35,083] Trial 157 finished with value: 0.6754023730754852 and parameters: {'hidden_dim': 113, 'id_embed_dim': 6, 'lr': 0.0041471900834908, 'batch_size': 128, 'alpha': 0.14646784579400382}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:35,204] Trial 158 finished with value: 0.0660371407866478 and parameters: {'hidden_dim': 119, 'id_embed_dim': 13, 'lr': 0.005252696685279973, 'batch_size': 128, 'alpha': 0.010008671741786464}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:35,322] Trial 159 finished with value: 0.07052065432071686 and parameters: {'hidden_dim': 119, 'id_embed_dim': 13, 'lr': 0.0051378168589353065, 'batch_size': 128, 'alpha': 0.010261379559150212}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:35,442] Trial 160 finished with value: 0.09373938292264938 and parameters: {'hidden_dim': 119, 'id_embed_dim': 13, 'lr': 0.004967215134635729, 'batch_size': 128, 'alpha': 0.016520664374719565}. Best is trial 66 with value: 0.059058818966150284.\n",
      "[I 2025-04-18 16:58:35,557] Trial 161 finished with value: 0.05820774286985397 and parameters: {'hidden_dim': 116, 'id_embed_dim': 13, 'lr': 0.005284843555211225, 'batch_size': 128, 'alpha': 0.010730393510437343}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:35,666] Trial 162 finished with value: 0.12721993401646614 and parameters: {'hidden_dim': 112, 'id_embed_dim': 14, 'lr': 0.0058414496823020814, 'batch_size': 128, 'alpha': 0.019441329661289995}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:35,781] Trial 163 finished with value: 0.06518150493502617 and parameters: {'hidden_dim': 115, 'id_embed_dim': 13, 'lr': 0.005269578087538229, 'batch_size': 128, 'alpha': 0.010661977344193274}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:35,898] Trial 164 finished with value: 0.12262677401304245 and parameters: {'hidden_dim': 115, 'id_embed_dim': 13, 'lr': 0.005310514125021035, 'batch_size': 128, 'alpha': 0.016427884925990417}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,017] Trial 165 finished with value: 0.15244590118527412 and parameters: {'hidden_dim': 116, 'id_embed_dim': 13, 'lr': 0.006434011976537348, 'batch_size': 128, 'alpha': 0.024078616079153403}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,129] Trial 166 finished with value: 0.10960371047258377 and parameters: {'hidden_dim': 112, 'id_embed_dim': 13, 'lr': 0.005290368986687814, 'batch_size': 128, 'alpha': 0.017581142881543665}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,247] Trial 167 finished with value: 0.06530999019742012 and parameters: {'hidden_dim': 118, 'id_embed_dim': 13, 'lr': 0.004716886259514359, 'batch_size': 128, 'alpha': 0.010032852608697171}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,469] Trial 168 finished with value: 0.16271080449223518 and parameters: {'hidden_dim': 117, 'id_embed_dim': 14, 'lr': 0.004353048353663664, 'batch_size': 32, 'alpha': 0.026379058921955932}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,589] Trial 169 finished with value: 0.20014919340610504 and parameters: {'hidden_dim': 119, 'id_embed_dim': 13, 'lr': 0.004794177178586749, 'batch_size': 128, 'alpha': 0.03263412711860863}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,703] Trial 170 finished with value: 0.06913669221103191 and parameters: {'hidden_dim': 114, 'id_embed_dim': 13, 'lr': 0.005733992697916989, 'batch_size': 128, 'alpha': 0.010214181239197118}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,822] Trial 171 finished with value: 0.10670264810323715 and parameters: {'hidden_dim': 114, 'id_embed_dim': 13, 'lr': 0.005987981569695743, 'batch_size': 128, 'alpha': 0.015336984925108502}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:36,940] Trial 172 finished with value: 0.06463193520903587 and parameters: {'hidden_dim': 117, 'id_embed_dim': 14, 'lr': 0.00696504098437112, 'batch_size': 128, 'alpha': 0.010258187443873955}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:37,057] Trial 173 finished with value: 0.12174941599369049 and parameters: {'hidden_dim': 115, 'id_embed_dim': 14, 'lr': 0.006999862492419409, 'batch_size': 128, 'alpha': 0.019957598284706675}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:37,175] Trial 174 finished with value: 0.12207026034593582 and parameters: {'hidden_dim': 117, 'id_embed_dim': 14, 'lr': 0.006541521874565017, 'batch_size': 128, 'alpha': 0.022538458688218528}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:37,290] Trial 175 finished with value: 0.09721633419394493 and parameters: {'hidden_dim': 112, 'id_embed_dim': 15, 'lr': 0.005671965149817515, 'batch_size': 128, 'alpha': 0.01588928996098331}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:37,408] Trial 176 finished with value: 0.5689010620117188 and parameters: {'hidden_dim': 122, 'id_embed_dim': 13, 'lr': 0.00022476554642987886, 'batch_size': 128, 'alpha': 0.01483493687485651}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:37,524] Trial 177 finished with value: 0.1282338872551918 and parameters: {'hidden_dim': 114, 'id_embed_dim': 13, 'lr': 0.006151577704917989, 'batch_size': 128, 'alpha': 0.021116497479762703}. Best is trial 161 with value: 0.05820774286985397.\n",
      "[I 2025-04-18 16:58:37,640] Trial 178 finished with value: 0.055677879601716995 and parameters: {'hidden_dim': 108, 'id_embed_dim': 14, 'lr': 0.0045497638395559075, 'batch_size': 128, 'alpha': 0.010144583780345379}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:37,751] Trial 179 finished with value: 0.1664707213640213 and parameters: {'hidden_dim': 108, 'id_embed_dim': 14, 'lr': 0.007009903501131716, 'batch_size': 128, 'alpha': 0.02638578551455667}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:37,868] Trial 180 finished with value: 0.769484281539917 and parameters: {'hidden_dim': 106, 'id_embed_dim': 14, 'lr': 0.004666383082164981, 'batch_size': 128, 'alpha': 0.2271606653799441}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:37,985] Trial 181 finished with value: 0.064230527728796 and parameters: {'hidden_dim': 111, 'id_embed_dim': 13, 'lr': 0.005323725371274077, 'batch_size': 128, 'alpha': 0.010162234871449946}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,099] Trial 182 finished with value: 0.09919766336679459 and parameters: {'hidden_dim': 110, 'id_embed_dim': 13, 'lr': 0.005387820715661397, 'batch_size': 128, 'alpha': 0.01522144386331344}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,217] Trial 183 finished with value: 0.06820085644721985 and parameters: {'hidden_dim': 111, 'id_embed_dim': 13, 'lr': 0.004924446187353254, 'batch_size': 128, 'alpha': 0.010003220488784543}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,331] Trial 184 finished with value: 0.12854411825537682 and parameters: {'hidden_dim': 109, 'id_embed_dim': 13, 'lr': 0.004523904302562097, 'batch_size': 128, 'alpha': 0.018825950673781622}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,447] Trial 185 finished with value: 0.456538587808609 and parameters: {'hidden_dim': 104, 'id_embed_dim': 14, 'lr': 0.004960954292789619, 'batch_size': 128, 'alpha': 0.09733866433964114}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,563] Trial 186 finished with value: 0.060347191989421844 and parameters: {'hidden_dim': 111, 'id_embed_dim': 13, 'lr': 0.006379177907028456, 'batch_size': 128, 'alpha': 0.01057072234787841}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,702] Trial 187 finished with value: 0.15115640312433243 and parameters: {'hidden_dim': 108, 'id_embed_dim': 13, 'lr': 0.0076849278420215465, 'batch_size': 64, 'alpha': 0.021706702845934102}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,816] Trial 188 finished with value: 0.05956968851387501 and parameters: {'hidden_dim': 112, 'id_embed_dim': 12, 'lr': 0.006426467657042448, 'batch_size': 128, 'alpha': 0.01012976515563831}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:38,929] Trial 189 finished with value: 0.10893289744853973 and parameters: {'hidden_dim': 112, 'id_embed_dim': 12, 'lr': 0.006583145393300777, 'batch_size': 128, 'alpha': 0.015743799316213954}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,046] Trial 190 finished with value: 0.17735844105482101 and parameters: {'hidden_dim': 107, 'id_embed_dim': 14, 'lr': 0.006169164914090104, 'batch_size': 128, 'alpha': 0.026390390007639514}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,164] Trial 191 finished with value: 0.06215636990964413 and parameters: {'hidden_dim': 128, 'id_embed_dim': 12, 'lr': 0.007003024998498114, 'batch_size': 128, 'alpha': 0.010550194211989335}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,280] Trial 192 finished with value: 0.06809069961309433 and parameters: {'hidden_dim': 128, 'id_embed_dim': 12, 'lr': 0.0072841446813603, 'batch_size': 128, 'alpha': 0.010138042751342478}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,398] Trial 193 finished with value: 0.10259253531694412 and parameters: {'hidden_dim': 116, 'id_embed_dim': 13, 'lr': 0.006761920389766524, 'batch_size': 128, 'alpha': 0.01630510727088198}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,515] Trial 194 finished with value: 0.11844588816165924 and parameters: {'hidden_dim': 110, 'id_embed_dim': 15, 'lr': 0.007751373550925225, 'batch_size': 128, 'alpha': 0.01980628000395092}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,634] Trial 195 finished with value: 0.06552604958415031 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.005745921303447316, 'batch_size': 128, 'alpha': 0.010081426052582085}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,748] Trial 196 finished with value: 0.10126028209924698 and parameters: {'hidden_dim': 112, 'id_embed_dim': 13, 'lr': 0.006172731034346444, 'batch_size': 128, 'alpha': 0.0166358736833557}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,864] Trial 197 finished with value: 0.1497718095779419 and parameters: {'hidden_dim': 106, 'id_embed_dim': 13, 'lr': 0.005639248039293822, 'batch_size': 128, 'alpha': 0.023014664907803246}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:39,981] Trial 198 finished with value: 0.10493120551109314 and parameters: {'hidden_dim': 109, 'id_embed_dim': 12, 'lr': 0.0069448381593221, 'batch_size': 128, 'alpha': 0.015466572719902334}. Best is trial 178 with value: 0.055677879601716995.\n",
      "[I 2025-04-18 16:58:40,100] Trial 199 finished with value: 0.19614391773939133 and parameters: {'hidden_dim': 117, 'id_embed_dim': 16, 'lr': 0.005533973939381404, 'batch_size': 128, 'alpha': 0.029592473911807318}. Best is trial 178 with value: 0.055677879601716995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'hidden_dim': 108, 'id_embed_dim': 14, 'lr': 0.0045497638395559075, 'batch_size': 128, 'alpha': 0.010144583780345379}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, train_df_split, val_df_split, id_map, input_dim, id_count, output_dim, device), n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparams:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c89fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "model = RNNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    id_count=id_count,\n",
    "    id_embed_dim=best_params['id_embed_dim'],\n",
    "    output_dim=output_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = OrdinalLabelSmoothingLoss(num_classes=output_dim, alpha=best_params['alpha'])\n",
    "\n",
    "train_loader = DataLoader(MoodDataset(train_df_split, id_map), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(MoodDataset(val_df_split, id_map), batch_size=best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0bfbe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.4581, val loss = 0.3782\n",
      "Epoch 2: train loss = 0.2933, val loss = 0.2096\n",
      "Epoch 3: train loss = 0.1503, val loss = 0.1210\n",
      "Epoch 4: train loss = 0.0937, val loss = 0.0903\n",
      "Epoch 5: train loss = 0.0682, val loss = 0.0780\n",
      "Epoch 6: train loss = 0.0574, val loss = 0.0729\n",
      "Epoch 7: train loss = 0.0513, val loss = 0.0684\n",
      "Epoch 8: train loss = 0.0473, val loss = 0.0655\n",
      "Epoch 9: train loss = 0.0444, val loss = 0.0626\n",
      "Epoch 10: train loss = 0.0423, val loss = 0.0616\n",
      "Epoch 11: train loss = 0.0408, val loss = 0.0603\n",
      "Epoch 12: train loss = 0.0394, val loss = 0.0602\n",
      "Epoch 13: train loss = 0.0377, val loss = 0.0601\n",
      "Epoch 14: train loss = 0.0368, val loss = 0.0603\n",
      "Epoch 15: train loss = 0.0356, val loss = 0.0597\n",
      "Epoch 16: train loss = 0.0347, val loss = 0.0601\n",
      "Epoch 17: train loss = 0.0331, val loss = 0.0625\n",
      "Epoch 18: train loss = 0.0322, val loss = 0.0608\n",
      "Epoch 19: train loss = 0.0309, val loss = 0.0626\n",
      "Epoch 20: train loss = 0.0296, val loss = 0.0627\n",
      "Epoch 21: train loss = 0.0285, val loss = 0.0619\n",
      "Epoch 22: train loss = 0.0274, val loss = 0.0614\n",
      "Epoch 23: train loss = 0.0260, val loss = 0.0645\n",
      "Epoch 24: train loss = 0.0251, val loss = 0.0661\n",
      "Epoch 25: train loss = 0.0241, val loss = 0.0637\n",
      "Epoch 26: train loss = 0.0232, val loss = 0.0632\n",
      "Epoch 27: train loss = 0.0220, val loss = 0.0677\n",
      "Epoch 28: train loss = 0.0212, val loss = 0.0671\n",
      "Epoch 29: train loss = 0.0202, val loss = 0.0643\n",
      "Epoch 30: train loss = 0.0198, val loss = 0.0674\n",
      "Epoch 31: train loss = 0.0186, val loss = 0.0647\n",
      "Epoch 32: train loss = 0.0177, val loss = 0.0671\n",
      "Epoch 33: train loss = 0.0167, val loss = 0.0659\n",
      "Epoch 34: train loss = 0.0160, val loss = 0.0653\n",
      "Epoch 35: train loss = 0.0152, val loss = 0.0652\n",
      "Epoch 36: train loss = 0.0145, val loss = 0.0660\n",
      "Epoch 37: train loss = 0.0140, val loss = 0.0655\n",
      "Epoch 38: train loss = 0.0136, val loss = 0.0665\n",
      "Epoch 39: train loss = 0.0133, val loss = 0.0669\n",
      "Epoch 40: train loss = 0.0122, val loss = 0.0656\n",
      "Epoch 41: train loss = 0.0118, val loss = 0.0675\n",
      "Epoch 42: train loss = 0.0110, val loss = 0.0675\n",
      "Epoch 43: train loss = 0.0107, val loss = 0.0675\n",
      "Epoch 44: train loss = 0.0100, val loss = 0.0675\n",
      "Epoch 45: train loss = 0.0097, val loss = 0.0680\n",
      "Epoch 46: train loss = 0.0091, val loss = 0.0679\n",
      "Epoch 47: train loss = 0.0088, val loss = 0.0672\n",
      "Epoch 48: train loss = 0.0082, val loss = 0.0685\n",
      "Epoch 49: train loss = 0.0080, val loss = 0.0701\n",
      "Epoch 50: train loss = 0.0077, val loss = 0.0714\n",
      "Epoch 51: train loss = 0.0072, val loss = 0.0746\n",
      "Epoch 52: train loss = 0.0069, val loss = 0.0721\n",
      "Epoch 53: train loss = 0.0066, val loss = 0.0740\n",
      "Epoch 54: train loss = 0.0064, val loss = 0.0742\n",
      "Epoch 55: train loss = 0.0062, val loss = 0.0757\n",
      "Epoch 56: train loss = 0.0064, val loss = 0.0738\n",
      "Epoch 57: train loss = 0.0057, val loss = 0.0717\n",
      "Epoch 58: train loss = 0.0053, val loss = 0.0743\n",
      "Epoch 59: train loss = 0.0051, val loss = 0.0740\n",
      "Epoch 60: train loss = 0.0049, val loss = 0.0743\n",
      "Epoch 61: train loss = 0.0046, val loss = 0.0736\n",
      "Epoch 62: train loss = 0.0043, val loss = 0.0735\n",
      "Epoch 63: train loss = 0.0043, val loss = 0.0713\n",
      "Epoch 64: train loss = 0.0040, val loss = 0.0736\n",
      "Epoch 65: train loss = 0.0038, val loss = 0.0719\n",
      "Epoch 66: train loss = 0.0037, val loss = 0.0732\n",
      "Epoch 67: train loss = 0.0034, val loss = 0.0718\n",
      "Epoch 68: train loss = 0.0033, val loss = 0.0740\n",
      "Epoch 69: train loss = 0.0031, val loss = 0.0744\n",
      "Epoch 70: train loss = 0.0030, val loss = 0.0746\n",
      "Epoch 71: train loss = 0.0029, val loss = 0.0750\n",
      "Epoch 72: train loss = 0.0028, val loss = 0.0739\n",
      "Epoch 73: train loss = 0.0026, val loss = 0.0760\n",
      "Epoch 74: train loss = 0.0024, val loss = 0.0762\n",
      "Epoch 75: train loss = 0.0024, val loss = 0.0764\n",
      "Epoch 76: train loss = 0.0023, val loss = 0.0776\n",
      "Epoch 77: train loss = 0.0022, val loss = 0.0784\n",
      "Epoch 78: train loss = 0.0021, val loss = 0.0782\n",
      "Epoch 79: train loss = 0.0020, val loss = 0.0777\n",
      "Epoch 80: train loss = 0.0020, val loss = 0.0790\n",
      "Epoch 81: train loss = 0.0019, val loss = 0.0791\n",
      "Epoch 82: train loss = 0.0018, val loss = 0.0787\n",
      "Epoch 83: train loss = 0.0018, val loss = 0.0800\n",
      "Epoch 84: train loss = 0.0017, val loss = 0.0797\n",
      "Epoch 85: train loss = 0.0016, val loss = 0.0797\n",
      "Epoch 86: train loss = 0.0016, val loss = 0.0803\n",
      "Epoch 87: train loss = 0.0015, val loss = 0.0807\n",
      "Epoch 88: train loss = 0.0014, val loss = 0.0809\n",
      "Epoch 89: train loss = 0.0014, val loss = 0.0814\n",
      "Epoch 90: train loss = 0.0014, val loss = 0.0827\n",
      "Epoch 91: train loss = 0.0014, val loss = 0.0813\n",
      "Epoch 92: train loss = 0.0013, val loss = 0.0841\n",
      "Epoch 93: train loss = 0.0013, val loss = 0.0811\n",
      "Epoch 94: train loss = 0.0012, val loss = 0.0838\n",
      "Epoch 95: train loss = 0.0012, val loss = 0.0843\n",
      "Epoch 96: train loss = 0.0013, val loss = 0.0834\n",
      "Epoch 97: train loss = 0.0014, val loss = 0.0863\n",
      "Epoch 98: train loss = 0.0013, val loss = 0.0839\n",
      "Epoch 99: train loss = 0.0013, val loss = 0.0860\n",
      "Epoch 100: train loss = 0.0011, val loss = 0.0856\n",
      "Epoch 101: train loss = 0.0010, val loss = 0.0861\n",
      "Epoch 102: train loss = 0.0010, val loss = 0.0862\n",
      "Epoch 103: train loss = 0.0009, val loss = 0.0873\n",
      "Epoch 104: train loss = 0.0009, val loss = 0.0872\n",
      "Epoch 105: train loss = 0.0009, val loss = 0.0878\n",
      "Epoch 106: train loss = 0.0008, val loss = 0.0883\n",
      "Epoch 107: train loss = 0.0008, val loss = 0.0881\n",
      "Epoch 108: train loss = 0.0008, val loss = 0.0883\n",
      "Epoch 109: train loss = 0.0008, val loss = 0.0898\n",
      "Epoch 110: train loss = 0.0008, val loss = 0.0882\n",
      "Epoch 111: train loss = 0.0008, val loss = 0.0895\n",
      "Epoch 112: train loss = 0.0007, val loss = 0.0893\n",
      "Epoch 113: train loss = 0.0007, val loss = 0.0901\n",
      "Epoch 114: train loss = 0.0007, val loss = 0.0901\n",
      "Epoch 115: train loss = 0.0007, val loss = 0.0916\n",
      "Epoch 116: train loss = 0.0007, val loss = 0.0898\n",
      "Epoch 117: train loss = 0.0006, val loss = 0.0915\n",
      "Epoch 118: train loss = 0.0006, val loss = 0.0906\n",
      "Epoch 119: train loss = 0.0006, val loss = 0.0901\n",
      "Epoch 120: train loss = 0.0006, val loss = 0.0910\n",
      "Epoch 121: train loss = 0.0006, val loss = 0.0914\n",
      "Epoch 122: train loss = 0.0007, val loss = 0.0915\n",
      "Epoch 123: train loss = 0.0006, val loss = 0.0917\n",
      "Epoch 124: train loss = 0.0006, val loss = 0.0916\n",
      "Epoch 125: train loss = 0.0005, val loss = 0.0917\n",
      "Epoch 126: train loss = 0.0005, val loss = 0.0918\n",
      "Epoch 127: train loss = 0.0006, val loss = 0.0908\n",
      "Epoch 128: train loss = 0.0006, val loss = 0.0936\n",
      "Epoch 129: train loss = 0.0005, val loss = 0.0923\n",
      "Epoch 130: train loss = 0.0006, val loss = 0.0930\n",
      "Epoch 131: train loss = 0.0005, val loss = 0.0922\n",
      "Epoch 132: train loss = 0.0006, val loss = 0.0914\n",
      "Epoch 133: train loss = 0.0005, val loss = 0.0930\n",
      "Epoch 134: train loss = 0.0005, val loss = 0.0921\n",
      "Epoch 135: train loss = 0.0005, val loss = 0.0931\n",
      "Epoch 136: train loss = 0.0005, val loss = 0.0927\n",
      "Epoch 137: train loss = 0.0004, val loss = 0.0915\n",
      "Epoch 138: train loss = 0.0004, val loss = 0.0933\n",
      "Epoch 139: train loss = 0.0005, val loss = 0.0938\n",
      "Epoch 140: train loss = 0.0004, val loss = 0.0921\n",
      "Epoch 141: train loss = 0.0004, val loss = 0.0927\n",
      "Epoch 142: train loss = 0.0004, val loss = 0.0956\n",
      "Epoch 143: train loss = 0.0004, val loss = 0.0923\n",
      "Epoch 144: train loss = 0.0004, val loss = 0.0939\n",
      "Epoch 145: train loss = 0.0004, val loss = 0.0942\n",
      "Epoch 146: train loss = 0.0004, val loss = 0.0930\n",
      "Epoch 147: train loss = 0.0004, val loss = 0.0933\n",
      "Epoch 148: train loss = 0.0004, val loss = 0.0933\n",
      "Epoch 149: train loss = 0.0004, val loss = 0.0937\n",
      "Epoch 150: train loss = 0.0003, val loss = 0.0940\n",
      "Epoch 151: train loss = 0.0004, val loss = 0.0938\n",
      "Epoch 152: train loss = 0.0003, val loss = 0.0946\n",
      "Epoch 153: train loss = 0.0003, val loss = 0.0939\n",
      "Epoch 154: train loss = 0.0003, val loss = 0.0941\n",
      "Epoch 155: train loss = 0.0004, val loss = 0.0948\n",
      "Epoch 156: train loss = 0.0004, val loss = 0.0931\n",
      "Epoch 157: train loss = 0.0005, val loss = 0.0962\n",
      "Epoch 158: train loss = 0.0005, val loss = 0.0941\n",
      "Epoch 159: train loss = 0.0004, val loss = 0.0946\n",
      "Epoch 160: train loss = 0.0004, val loss = 0.0956\n",
      "Epoch 161: train loss = 0.0004, val loss = 0.0946\n",
      "Epoch 162: train loss = 0.0003, val loss = 0.0944\n",
      "Epoch 163: train loss = 0.0003, val loss = 0.0939\n",
      "Epoch 164: train loss = 0.0004, val loss = 0.0936\n",
      "Epoch 165: train loss = 0.0003, val loss = 0.0945\n",
      "Epoch 166: train loss = 0.0003, val loss = 0.0955\n",
      "Epoch 167: train loss = 0.0003, val loss = 0.0938\n",
      "Epoch 168: train loss = 0.0003, val loss = 0.0948\n",
      "Epoch 169: train loss = 0.0003, val loss = 0.0941\n",
      "Epoch 170: train loss = 0.0003, val loss = 0.0939\n",
      "Epoch 171: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 172: train loss = 0.0002, val loss = 0.0943\n",
      "Epoch 173: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 174: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 175: train loss = 0.0002, val loss = 0.0943\n",
      "Epoch 176: train loss = 0.0002, val loss = 0.0947\n",
      "Epoch 177: train loss = 0.0002, val loss = 0.0961\n",
      "Epoch 178: train loss = 0.0002, val loss = 0.0942\n",
      "Epoch 179: train loss = 0.0002, val loss = 0.0938\n",
      "Epoch 180: train loss = 0.0003, val loss = 0.0941\n",
      "Epoch 181: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 182: train loss = 0.0002, val loss = 0.0933\n",
      "Epoch 183: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 184: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 185: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 186: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 187: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 188: train loss = 0.0002, val loss = 0.0969\n",
      "Epoch 189: train loss = 0.0002, val loss = 0.0947\n",
      "Epoch 190: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 191: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 192: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 193: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 194: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 195: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 196: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 197: train loss = 0.0002, val loss = 0.0969\n",
      "Epoch 198: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 199: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 200: train loss = 0.0001, val loss = 0.0955\n",
      "Epoch 201: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 202: train loss = 0.0002, val loss = 0.0976\n",
      "Epoch 203: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 204: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 205: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 206: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 207: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 208: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 209: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 210: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 211: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 212: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 213: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 214: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 215: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 216: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 217: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 218: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 219: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 220: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 221: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 222: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 223: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 224: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 225: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 226: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 227: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 228: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 229: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 230: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 231: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 232: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 233: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 234: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 235: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 236: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 237: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 238: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 239: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 240: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 241: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 242: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 243: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 244: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 245: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 246: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 247: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 248: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 249: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 250: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 251: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 252: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 253: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 254: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 255: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 256: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 257: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 258: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 259: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 260: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 261: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 262: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 263: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 264: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 265: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 266: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 267: train loss = 0.0002, val loss = 0.0974\n",
      "Epoch 268: train loss = 0.0002, val loss = 0.0981\n",
      "Epoch 269: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 270: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 271: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 272: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 273: train loss = 0.0002, val loss = 0.0979\n",
      "Epoch 274: train loss = 0.0003, val loss = 0.0986\n",
      "Epoch 275: train loss = 0.0003, val loss = 0.0962\n",
      "Epoch 276: train loss = 0.0003, val loss = 0.0977\n",
      "Epoch 277: train loss = 0.0004, val loss = 0.0971\n",
      "Epoch 278: train loss = 0.0004, val loss = 0.0987\n",
      "Epoch 279: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 280: train loss = 0.0003, val loss = 0.0980\n",
      "Epoch 281: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 282: train loss = 0.0004, val loss = 0.0959\n",
      "Epoch 283: train loss = 0.0004, val loss = 0.0960\n",
      "Epoch 284: train loss = 0.0004, val loss = 0.0953\n",
      "Epoch 285: train loss = 0.0004, val loss = 0.0965\n",
      "Epoch 286: train loss = 0.0004, val loss = 0.0949\n",
      "Epoch 287: train loss = 0.0005, val loss = 0.0947\n",
      "Epoch 288: train loss = 0.0005, val loss = 0.0969\n",
      "Epoch 289: train loss = 0.0005, val loss = 0.0968\n",
      "Epoch 290: train loss = 0.0004, val loss = 0.0957\n",
      "Epoch 291: train loss = 0.0004, val loss = 0.0962\n",
      "Epoch 292: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 293: train loss = 0.0004, val loss = 0.0950\n",
      "Epoch 294: train loss = 0.0004, val loss = 0.0945\n",
      "Epoch 295: train loss = 0.0003, val loss = 0.0956\n",
      "Epoch 296: train loss = 0.0003, val loss = 0.0955\n",
      "Epoch 297: train loss = 0.0003, val loss = 0.0959\n",
      "Epoch 298: train loss = 0.0003, val loss = 0.0952\n",
      "Epoch 299: train loss = 0.0003, val loss = 0.0961\n",
      "Epoch 300: train loss = 0.0003, val loss = 0.0954\n",
      "Epoch 301: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 302: train loss = 0.0002, val loss = 0.0939\n",
      "Epoch 303: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 304: train loss = 0.0002, val loss = 0.0944\n",
      "Epoch 305: train loss = 0.0002, val loss = 0.0932\n",
      "Epoch 306: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 307: train loss = 0.0002, val loss = 0.0935\n",
      "Epoch 308: train loss = 0.0002, val loss = 0.0946\n",
      "Epoch 309: train loss = 0.0002, val loss = 0.0941\n",
      "Epoch 310: train loss = 0.0002, val loss = 0.0949\n",
      "Epoch 311: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 312: train loss = 0.0002, val loss = 0.0934\n",
      "Epoch 313: train loss = 0.0002, val loss = 0.0929\n",
      "Epoch 314: train loss = 0.0002, val loss = 0.0943\n",
      "Epoch 315: train loss = 0.0002, val loss = 0.0946\n",
      "Epoch 316: train loss = 0.0002, val loss = 0.0918\n",
      "Epoch 317: train loss = 0.0002, val loss = 0.0956\n",
      "Epoch 318: train loss = 0.0002, val loss = 0.0942\n",
      "Epoch 319: train loss = 0.0002, val loss = 0.0956\n",
      "Epoch 320: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 321: train loss = 0.0002, val loss = 0.0938\n",
      "Epoch 322: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 323: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 324: train loss = 0.0002, val loss = 0.0925\n",
      "Epoch 325: train loss = 0.0003, val loss = 0.0930\n",
      "Epoch 326: train loss = 0.0002, val loss = 0.0928\n",
      "Epoch 327: train loss = 0.0002, val loss = 0.0926\n",
      "Epoch 328: train loss = 0.0002, val loss = 0.0937\n",
      "Epoch 329: train loss = 0.0002, val loss = 0.0931\n",
      "Epoch 330: train loss = 0.0002, val loss = 0.0940\n",
      "Epoch 331: train loss = 0.0002, val loss = 0.0933\n",
      "Epoch 332: train loss = 0.0002, val loss = 0.0935\n",
      "Epoch 333: train loss = 0.0002, val loss = 0.0937\n",
      "Epoch 334: train loss = 0.0003, val loss = 0.0920\n",
      "Epoch 335: train loss = 0.0003, val loss = 0.0946\n",
      "Epoch 336: train loss = 0.0003, val loss = 0.0933\n",
      "Epoch 337: train loss = 0.0003, val loss = 0.0932\n",
      "Epoch 338: train loss = 0.0003, val loss = 0.0943\n",
      "Epoch 339: train loss = 0.0003, val loss = 0.0942\n",
      "Epoch 340: train loss = 0.0003, val loss = 0.0919\n",
      "Epoch 341: train loss = 0.0004, val loss = 0.0944\n",
      "Epoch 342: train loss = 0.0004, val loss = 0.0929\n",
      "Epoch 343: train loss = 0.0004, val loss = 0.0921\n",
      "Epoch 344: train loss = 0.0004, val loss = 0.0939\n",
      "Epoch 345: train loss = 0.0004, val loss = 0.0890\n",
      "Epoch 346: train loss = 0.0004, val loss = 0.0933\n",
      "Epoch 347: train loss = 0.0003, val loss = 0.0910\n",
      "Epoch 348: train loss = 0.0003, val loss = 0.0925\n",
      "Epoch 349: train loss = 0.0003, val loss = 0.0924\n",
      "Epoch 350: train loss = 0.0003, val loss = 0.0919\n",
      "Epoch 351: train loss = 0.0003, val loss = 0.0918\n",
      "Epoch 352: train loss = 0.0003, val loss = 0.0903\n",
      "Epoch 353: train loss = 0.0003, val loss = 0.0922\n",
      "Epoch 354: train loss = 0.0003, val loss = 0.0915\n",
      "Epoch 355: train loss = 0.0002, val loss = 0.0920\n",
      "Epoch 356: train loss = 0.0002, val loss = 0.0930\n",
      "Epoch 357: train loss = 0.0002, val loss = 0.0915\n",
      "Epoch 358: train loss = 0.0002, val loss = 0.0915\n",
      "Epoch 359: train loss = 0.0002, val loss = 0.0923\n",
      "Epoch 360: train loss = 0.0002, val loss = 0.0920\n",
      "Epoch 361: train loss = 0.0002, val loss = 0.0917\n",
      "Epoch 362: train loss = 0.0002, val loss = 0.0935\n",
      "Epoch 363: train loss = 0.0002, val loss = 0.0921\n",
      "Epoch 364: train loss = 0.0002, val loss = 0.0925\n",
      "Epoch 365: train loss = 0.0002, val loss = 0.0922\n",
      "Epoch 366: train loss = 0.0002, val loss = 0.0925\n",
      "Epoch 367: train loss = 0.0002, val loss = 0.0926\n",
      "Epoch 368: train loss = 0.0002, val loss = 0.0920\n",
      "Epoch 369: train loss = 0.0002, val loss = 0.0925\n",
      "Epoch 370: train loss = 0.0002, val loss = 0.0911\n",
      "Epoch 371: train loss = 0.0002, val loss = 0.0918\n",
      "Epoch 372: train loss = 0.0001, val loss = 0.0914\n",
      "Epoch 373: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 374: train loss = 0.0001, val loss = 0.0906\n",
      "Epoch 375: train loss = 0.0001, val loss = 0.0923\n",
      "Epoch 376: train loss = 0.0001, val loss = 0.0922\n",
      "Epoch 377: train loss = 0.0001, val loss = 0.0917\n",
      "Epoch 378: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 379: train loss = 0.0001, val loss = 0.0923\n",
      "Epoch 380: train loss = 0.0001, val loss = 0.0912\n",
      "Epoch 381: train loss = 0.0001, val loss = 0.0917\n",
      "Epoch 382: train loss = 0.0001, val loss = 0.0914\n",
      "Epoch 383: train loss = 0.0001, val loss = 0.0905\n",
      "Epoch 384: train loss = 0.0001, val loss = 0.0916\n",
      "Epoch 385: train loss = 0.0001, val loss = 0.0914\n",
      "Epoch 386: train loss = 0.0001, val loss = 0.0915\n",
      "Epoch 387: train loss = 0.0001, val loss = 0.0917\n",
      "Epoch 388: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 389: train loss = 0.0001, val loss = 0.0911\n",
      "Epoch 390: train loss = 0.0001, val loss = 0.0917\n",
      "Epoch 391: train loss = 0.0001, val loss = 0.0911\n",
      "Epoch 392: train loss = 0.0001, val loss = 0.0918\n",
      "Epoch 393: train loss = 0.0001, val loss = 0.0913\n",
      "Epoch 394: train loss = 0.0001, val loss = 0.0918\n",
      "Epoch 395: train loss = 0.0001, val loss = 0.0911\n",
      "Epoch 396: train loss = 0.0001, val loss = 0.0921\n",
      "Epoch 397: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 398: train loss = 0.0001, val loss = 0.0914\n",
      "Epoch 399: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 400: train loss = 0.0001, val loss = 0.0907\n",
      "Epoch 401: train loss = 0.0001, val loss = 0.0912\n",
      "Epoch 402: train loss = 0.0001, val loss = 0.0913\n",
      "Epoch 403: train loss = 0.0001, val loss = 0.0913\n",
      "Epoch 404: train loss = 0.0001, val loss = 0.0913\n",
      "Epoch 405: train loss = 0.0001, val loss = 0.0917\n",
      "Epoch 406: train loss = 0.0001, val loss = 0.0916\n",
      "Epoch 407: train loss = 0.0001, val loss = 0.0917\n",
      "Epoch 408: train loss = 0.0001, val loss = 0.0910\n",
      "Epoch 409: train loss = 0.0001, val loss = 0.0906\n",
      "Epoch 410: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 411: train loss = 0.0001, val loss = 0.0924\n",
      "Epoch 412: train loss = 0.0001, val loss = 0.0914\n",
      "Epoch 413: train loss = 0.0001, val loss = 0.0908\n",
      "Epoch 414: train loss = 0.0001, val loss = 0.0912\n",
      "Epoch 415: train loss = 0.0001, val loss = 0.0911\n",
      "Epoch 416: train loss = 0.0001, val loss = 0.0911\n",
      "Epoch 417: train loss = 0.0001, val loss = 0.0918\n",
      "Epoch 418: train loss = 0.0001, val loss = 0.0908\n",
      "Epoch 419: train loss = 0.0001, val loss = 0.0910\n",
      "Epoch 420: train loss = 0.0001, val loss = 0.0916\n",
      "Epoch 421: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 422: train loss = 0.0001, val loss = 0.0908\n",
      "Epoch 423: train loss = 0.0001, val loss = 0.0904\n",
      "Epoch 424: train loss = 0.0001, val loss = 0.0917\n",
      "Epoch 425: train loss = 0.0001, val loss = 0.0905\n",
      "Epoch 426: train loss = 0.0001, val loss = 0.0910\n",
      "Epoch 427: train loss = 0.0001, val loss = 0.0919\n",
      "Epoch 428: train loss = 0.0002, val loss = 0.0912\n",
      "Epoch 429: train loss = 0.0002, val loss = 0.0902\n",
      "Epoch 430: train loss = 0.0002, val loss = 0.0899\n",
      "Epoch 431: train loss = 0.0003, val loss = 0.0915\n",
      "Epoch 432: train loss = 0.0003, val loss = 0.0909\n",
      "Epoch 433: train loss = 0.0003, val loss = 0.0905\n",
      "Epoch 434: train loss = 0.0003, val loss = 0.0901\n",
      "Epoch 435: train loss = 0.0003, val loss = 0.0894\n",
      "Epoch 436: train loss = 0.0004, val loss = 0.0895\n",
      "Epoch 437: train loss = 0.0005, val loss = 0.0911\n",
      "Epoch 438: train loss = 0.0005, val loss = 0.0902\n",
      "Epoch 439: train loss = 0.0005, val loss = 0.0891\n",
      "Epoch 440: train loss = 0.0006, val loss = 0.0871\n",
      "Epoch 441: train loss = 0.0005, val loss = 0.0900\n",
      "Epoch 442: train loss = 0.0006, val loss = 0.0872\n",
      "Epoch 443: train loss = 0.0006, val loss = 0.0915\n",
      "Epoch 444: train loss = 0.0005, val loss = 0.0890\n",
      "Epoch 445: train loss = 0.0005, val loss = 0.0871\n",
      "Epoch 446: train loss = 0.0004, val loss = 0.0863\n",
      "Epoch 447: train loss = 0.0004, val loss = 0.0904\n",
      "Epoch 448: train loss = 0.0004, val loss = 0.0878\n",
      "Epoch 449: train loss = 0.0005, val loss = 0.0880\n",
      "Epoch 450: train loss = 0.0005, val loss = 0.0852\n",
      "Epoch 451: train loss = 0.0005, val loss = 0.0889\n",
      "Epoch 452: train loss = 0.0006, val loss = 0.0879\n",
      "Epoch 453: train loss = 0.0006, val loss = 0.0859\n",
      "Epoch 454: train loss = 0.0006, val loss = 0.0866\n",
      "Epoch 455: train loss = 0.0005, val loss = 0.0893\n",
      "Epoch 456: train loss = 0.0006, val loss = 0.0863\n",
      "Epoch 457: train loss = 0.0006, val loss = 0.0861\n",
      "Epoch 458: train loss = 0.0006, val loss = 0.0892\n",
      "Epoch 459: train loss = 0.0005, val loss = 0.0869\n",
      "Epoch 460: train loss = 0.0004, val loss = 0.0848\n",
      "Epoch 461: train loss = 0.0004, val loss = 0.0877\n",
      "Epoch 462: train loss = 0.0003, val loss = 0.0854\n",
      "Epoch 463: train loss = 0.0003, val loss = 0.0868\n",
      "Epoch 464: train loss = 0.0003, val loss = 0.0872\n",
      "Epoch 465: train loss = 0.0003, val loss = 0.0871\n",
      "Epoch 466: train loss = 0.0003, val loss = 0.0846\n",
      "Epoch 467: train loss = 0.0002, val loss = 0.0847\n",
      "Epoch 468: train loss = 0.0002, val loss = 0.0861\n",
      "Epoch 469: train loss = 0.0002, val loss = 0.0864\n",
      "Epoch 470: train loss = 0.0002, val loss = 0.0863\n",
      "Epoch 471: train loss = 0.0001, val loss = 0.0860\n",
      "Epoch 472: train loss = 0.0002, val loss = 0.0865\n",
      "Epoch 473: train loss = 0.0001, val loss = 0.0867\n",
      "Epoch 474: train loss = 0.0001, val loss = 0.0856\n",
      "Epoch 475: train loss = 0.0001, val loss = 0.0857\n",
      "Epoch 476: train loss = 0.0001, val loss = 0.0849\n",
      "Epoch 477: train loss = 0.0001, val loss = 0.0875\n",
      "Epoch 478: train loss = 0.0002, val loss = 0.0857\n",
      "Epoch 479: train loss = 0.0002, val loss = 0.0863\n",
      "Epoch 480: train loss = 0.0001, val loss = 0.0858\n",
      "Epoch 481: train loss = 0.0001, val loss = 0.0864\n",
      "Epoch 482: train loss = 0.0001, val loss = 0.0859\n",
      "Epoch 483: train loss = 0.0001, val loss = 0.0857\n",
      "Epoch 484: train loss = 0.0001, val loss = 0.0857\n",
      "Epoch 485: train loss = 0.0001, val loss = 0.0862\n",
      "Epoch 486: train loss = 0.0001, val loss = 0.0859\n",
      "Epoch 487: train loss = 0.0001, val loss = 0.0864\n",
      "Epoch 488: train loss = 0.0001, val loss = 0.0854\n",
      "Epoch 489: train loss = 0.0001, val loss = 0.0866\n",
      "Epoch 490: train loss = 0.0001, val loss = 0.0866\n",
      "Epoch 491: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 492: train loss = 0.0001, val loss = 0.0854\n",
      "Epoch 493: train loss = 0.0001, val loss = 0.0860\n",
      "Epoch 494: train loss = 0.0001, val loss = 0.0862\n",
      "Epoch 495: train loss = 0.0001, val loss = 0.0857\n",
      "Epoch 496: train loss = 0.0001, val loss = 0.0860\n",
      "Epoch 497: train loss = 0.0001, val loss = 0.0863\n",
      "Epoch 498: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 499: train loss = 0.0000, val loss = 0.0869\n",
      "Epoch 500: train loss = 0.0000, val loss = 0.0862\n",
      "Epoch 501: train loss = 0.0000, val loss = 0.0865\n",
      "Epoch 502: train loss = 0.0000, val loss = 0.0864\n",
      "Epoch 503: train loss = 0.0000, val loss = 0.0861\n",
      "Epoch 504: train loss = 0.0000, val loss = 0.0861\n",
      "Epoch 505: train loss = 0.0000, val loss = 0.0863\n",
      "Epoch 506: train loss = 0.0000, val loss = 0.0863\n",
      "Epoch 507: train loss = 0.0000, val loss = 0.0863\n",
      "Epoch 508: train loss = 0.0000, val loss = 0.0862\n",
      "Epoch 509: train loss = 0.0000, val loss = 0.0860\n",
      "Epoch 510: train loss = 0.0000, val loss = 0.0862\n",
      "Epoch 511: train loss = 0.0000, val loss = 0.0861\n",
      "Epoch 512: train loss = 0.0000, val loss = 0.0860\n",
      "Epoch 513: train loss = 0.0000, val loss = 0.0861\n",
      "Epoch 514: train loss = 0.0000, val loss = 0.0864\n",
      "Epoch 515: train loss = 0.0000, val loss = 0.0864\n",
      "Epoch 516: train loss = 0.0000, val loss = 0.0860\n",
      "Epoch 517: train loss = 0.0000, val loss = 0.0864\n",
      "Epoch 518: train loss = 0.0000, val loss = 0.0861\n",
      "Epoch 519: train loss = 0.0001, val loss = 0.0868\n",
      "Epoch 520: train loss = 0.0000, val loss = 0.0862\n",
      "Epoch 521: train loss = 0.0000, val loss = 0.0861\n",
      "Epoch 522: train loss = 0.0000, val loss = 0.0861\n",
      "Epoch 523: train loss = 0.0001, val loss = 0.0867\n",
      "Epoch 524: train loss = 0.0001, val loss = 0.0858\n",
      "Epoch 525: train loss = 0.0001, val loss = 0.0867\n",
      "Epoch 526: train loss = 0.0001, val loss = 0.0865\n",
      "Epoch 527: train loss = 0.0001, val loss = 0.0863\n",
      "Epoch 528: train loss = 0.0000, val loss = 0.0862\n",
      "Epoch 529: train loss = 0.0000, val loss = 0.0858\n",
      "Epoch 530: train loss = 0.0001, val loss = 0.0865\n",
      "Epoch 531: train loss = 0.0000, val loss = 0.0867\n",
      "Epoch 532: train loss = 0.0001, val loss = 0.0867\n",
      "Epoch 533: train loss = 0.0001, val loss = 0.0863\n",
      "Epoch 534: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 535: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 536: train loss = 0.0001, val loss = 0.0873\n",
      "Epoch 537: train loss = 0.0001, val loss = 0.0858\n",
      "Epoch 538: train loss = 0.0001, val loss = 0.0864\n",
      "Epoch 539: train loss = 0.0001, val loss = 0.0865\n",
      "Epoch 540: train loss = 0.0001, val loss = 0.0869\n",
      "Epoch 541: train loss = 0.0001, val loss = 0.0860\n",
      "Epoch 542: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 543: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 544: train loss = 0.0001, val loss = 0.0864\n",
      "Epoch 545: train loss = 0.0001, val loss = 0.0864\n",
      "Epoch 546: train loss = 0.0001, val loss = 0.0860\n",
      "Epoch 547: train loss = 0.0001, val loss = 0.0864\n",
      "Epoch 548: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 549: train loss = 0.0001, val loss = 0.0858\n",
      "Epoch 550: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 551: train loss = 0.0001, val loss = 0.0863\n",
      "Epoch 552: train loss = 0.0001, val loss = 0.0862\n",
      "Epoch 553: train loss = 0.0001, val loss = 0.0862\n",
      "Epoch 554: train loss = 0.0001, val loss = 0.0869\n",
      "Epoch 555: train loss = 0.0001, val loss = 0.0861\n",
      "Epoch 556: train loss = 0.0001, val loss = 0.0852\n",
      "Epoch 557: train loss = 0.0001, val loss = 0.0873\n",
      "Epoch 558: train loss = 0.0002, val loss = 0.0851\n",
      "Epoch 559: train loss = 0.0002, val loss = 0.0853\n",
      "Epoch 560: train loss = 0.0002, val loss = 0.0863\n",
      "Epoch 561: train loss = 0.0003, val loss = 0.0861\n",
      "Epoch 562: train loss = 0.0002, val loss = 0.0856\n",
      "Epoch 563: train loss = 0.0003, val loss = 0.0862\n",
      "Epoch 564: train loss = 0.0003, val loss = 0.0883\n",
      "Epoch 565: train loss = 0.0004, val loss = 0.0852\n",
      "Epoch 566: train loss = 0.0004, val loss = 0.0858\n",
      "Epoch 567: train loss = 0.0003, val loss = 0.0858\n",
      "Epoch 568: train loss = 0.0003, val loss = 0.0854\n",
      "Epoch 569: train loss = 0.0004, val loss = 0.0851\n",
      "Epoch 570: train loss = 0.0004, val loss = 0.0855\n",
      "Epoch 571: train loss = 0.0004, val loss = 0.0832\n",
      "Epoch 572: train loss = 0.0006, val loss = 0.0847\n",
      "Epoch 573: train loss = 0.0005, val loss = 0.0834\n",
      "Epoch 574: train loss = 0.0006, val loss = 0.0846\n",
      "Epoch 575: train loss = 0.0006, val loss = 0.0849\n",
      "Epoch 576: train loss = 0.0006, val loss = 0.0847\n",
      "Epoch 577: train loss = 0.0005, val loss = 0.0835\n",
      "Epoch 578: train loss = 0.0005, val loss = 0.0843\n",
      "Epoch 579: train loss = 0.0007, val loss = 0.0849\n",
      "Epoch 580: train loss = 0.0007, val loss = 0.0832\n",
      "Epoch 581: train loss = 0.0006, val loss = 0.0850\n",
      "Epoch 582: train loss = 0.0007, val loss = 0.0819\n",
      "Epoch 583: train loss = 0.0007, val loss = 0.0839\n",
      "Epoch 584: train loss = 0.0006, val loss = 0.0825\n",
      "Epoch 585: train loss = 0.0007, val loss = 0.0842\n",
      "Epoch 586: train loss = 0.0006, val loss = 0.0832\n",
      "Epoch 587: train loss = 0.0006, val loss = 0.0825\n",
      "Epoch 588: train loss = 0.0005, val loss = 0.0814\n",
      "Epoch 589: train loss = 0.0005, val loss = 0.0818\n",
      "Epoch 590: train loss = 0.0004, val loss = 0.0827\n",
      "Epoch 591: train loss = 0.0005, val loss = 0.0817\n",
      "Epoch 592: train loss = 0.0004, val loss = 0.0816\n",
      "Epoch 593: train loss = 0.0004, val loss = 0.0828\n",
      "Epoch 594: train loss = 0.0004, val loss = 0.0813\n",
      "Epoch 595: train loss = 0.0004, val loss = 0.0810\n",
      "Epoch 596: train loss = 0.0004, val loss = 0.0810\n",
      "Epoch 597: train loss = 0.0003, val loss = 0.0828\n",
      "Epoch 598: train loss = 0.0003, val loss = 0.0815\n",
      "Epoch 599: train loss = 0.0004, val loss = 0.0818\n",
      "Epoch 600: train loss = 0.0003, val loss = 0.0815\n",
      "Epoch 601: train loss = 0.0002, val loss = 0.0828\n",
      "Epoch 602: train loss = 0.0002, val loss = 0.0814\n",
      "Epoch 603: train loss = 0.0002, val loss = 0.0821\n",
      "Epoch 604: train loss = 0.0002, val loss = 0.0819\n",
      "Epoch 605: train loss = 0.0002, val loss = 0.0825\n",
      "Epoch 606: train loss = 0.0002, val loss = 0.0819\n",
      "Epoch 607: train loss = 0.0001, val loss = 0.0824\n",
      "Epoch 608: train loss = 0.0001, val loss = 0.0822\n",
      "Epoch 609: train loss = 0.0001, val loss = 0.0820\n",
      "Epoch 610: train loss = 0.0001, val loss = 0.0817\n",
      "Epoch 611: train loss = 0.0001, val loss = 0.0813\n",
      "Epoch 612: train loss = 0.0001, val loss = 0.0821\n",
      "Epoch 613: train loss = 0.0001, val loss = 0.0822\n",
      "Epoch 614: train loss = 0.0001, val loss = 0.0818\n",
      "Epoch 615: train loss = 0.0001, val loss = 0.0824\n",
      "Epoch 616: train loss = 0.0001, val loss = 0.0821\n",
      "Epoch 617: train loss = 0.0001, val loss = 0.0819\n",
      "Epoch 618: train loss = 0.0001, val loss = 0.0825\n",
      "Epoch 619: train loss = 0.0001, val loss = 0.0819\n",
      "Epoch 620: train loss = 0.0001, val loss = 0.0818\n",
      "Epoch 621: train loss = 0.0001, val loss = 0.0820\n",
      "Epoch 622: train loss = 0.0001, val loss = 0.0817\n",
      "Epoch 623: train loss = 0.0001, val loss = 0.0828\n",
      "Epoch 624: train loss = 0.0001, val loss = 0.0819\n",
      "Epoch 625: train loss = 0.0001, val loss = 0.0825\n",
      "Epoch 626: train loss = 0.0001, val loss = 0.0818\n",
      "Epoch 627: train loss = 0.0001, val loss = 0.0822\n",
      "Epoch 628: train loss = 0.0001, val loss = 0.0820\n",
      "Epoch 629: train loss = 0.0001, val loss = 0.0820\n",
      "Epoch 630: train loss = 0.0001, val loss = 0.0826\n",
      "Epoch 631: train loss = 0.0001, val loss = 0.0823\n",
      "Epoch 632: train loss = 0.0001, val loss = 0.0818\n",
      "Epoch 633: train loss = 0.0001, val loss = 0.0826\n",
      "Epoch 634: train loss = 0.0001, val loss = 0.0819\n",
      "Epoch 635: train loss = 0.0001, val loss = 0.0823\n",
      "Epoch 636: train loss = 0.0001, val loss = 0.0821\n",
      "Epoch 637: train loss = 0.0001, val loss = 0.0821\n",
      "Epoch 638: train loss = 0.0001, val loss = 0.0817\n",
      "Epoch 639: train loss = 0.0000, val loss = 0.0825\n",
      "Epoch 640: train loss = 0.0001, val loss = 0.0817\n",
      "Epoch 641: train loss = 0.0001, val loss = 0.0823\n",
      "Epoch 642: train loss = 0.0001, val loss = 0.0818\n",
      "Epoch 643: train loss = 0.0001, val loss = 0.0820\n",
      "Epoch 644: train loss = 0.0001, val loss = 0.0821\n",
      "Epoch 645: train loss = 0.0001, val loss = 0.0814\n",
      "Epoch 646: train loss = 0.0001, val loss = 0.0822\n",
      "Epoch 647: train loss = 0.0001, val loss = 0.0822\n",
      "Epoch 648: train loss = 0.0001, val loss = 0.0814\n",
      "Epoch 649: train loss = 0.0001, val loss = 0.0827\n",
      "Epoch 650: train loss = 0.0001, val loss = 0.0820\n",
      "Epoch 651: train loss = 0.0001, val loss = 0.0817\n",
      "Epoch 652: train loss = 0.0001, val loss = 0.0819\n",
      "Epoch 653: train loss = 0.0001, val loss = 0.0822\n",
      "Epoch 654: train loss = 0.0001, val loss = 0.0819\n",
      "Epoch 655: train loss = 0.0001, val loss = 0.0821\n",
      "Epoch 656: train loss = 0.0000, val loss = 0.0811\n",
      "Epoch 657: train loss = 0.0000, val loss = 0.0823\n",
      "Epoch 658: train loss = 0.0000, val loss = 0.0821\n",
      "Epoch 659: train loss = 0.0000, val loss = 0.0819\n",
      "Epoch 660: train loss = 0.0001, val loss = 0.0812\n",
      "Epoch 661: train loss = 0.0001, val loss = 0.0816\n",
      "Epoch 662: train loss = 0.0001, val loss = 0.0823\n",
      "Epoch 663: train loss = 0.0001, val loss = 0.0807\n",
      "Epoch 664: train loss = 0.0001, val loss = 0.0822\n",
      "Epoch 665: train loss = 0.0001, val loss = 0.0818\n",
      "Epoch 666: train loss = 0.0001, val loss = 0.0823\n",
      "Epoch 667: train loss = 0.0001, val loss = 0.0821\n",
      "Epoch 668: train loss = 0.0002, val loss = 0.0824\n",
      "Epoch 669: train loss = 0.0003, val loss = 0.0817\n",
      "Epoch 670: train loss = 0.0003, val loss = 0.0822\n",
      "Epoch 671: train loss = 0.0004, val loss = 0.0816\n",
      "Epoch 672: train loss = 0.0004, val loss = 0.0810\n",
      "Epoch 673: train loss = 0.0004, val loss = 0.0823\n",
      "Epoch 674: train loss = 0.0004, val loss = 0.0834\n",
      "Epoch 675: train loss = 0.0004, val loss = 0.0806\n",
      "Epoch 676: train loss = 0.0004, val loss = 0.0817\n",
      "Epoch 677: train loss = 0.0004, val loss = 0.0807\n",
      "Epoch 678: train loss = 0.0003, val loss = 0.0822\n",
      "Epoch 679: train loss = 0.0004, val loss = 0.0822\n",
      "Epoch 680: train loss = 0.0004, val loss = 0.0823\n",
      "Epoch 681: train loss = 0.0003, val loss = 0.0806\n",
      "Epoch 682: train loss = 0.0003, val loss = 0.0809\n",
      "Epoch 683: train loss = 0.0002, val loss = 0.0812\n",
      "Epoch 684: train loss = 0.0002, val loss = 0.0810\n",
      "Epoch 685: train loss = 0.0001, val loss = 0.0814\n",
      "Epoch 686: train loss = 0.0001, val loss = 0.0796\n",
      "Epoch 687: train loss = 0.0001, val loss = 0.0812\n",
      "Epoch 688: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 689: train loss = 0.0001, val loss = 0.0803\n",
      "Epoch 690: train loss = 0.0001, val loss = 0.0820\n",
      "Epoch 691: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 692: train loss = 0.0001, val loss = 0.0809\n",
      "Epoch 693: train loss = 0.0001, val loss = 0.0812\n",
      "Epoch 694: train loss = 0.0001, val loss = 0.0811\n",
      "Epoch 695: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 696: train loss = 0.0001, val loss = 0.0815\n",
      "Epoch 697: train loss = 0.0001, val loss = 0.0806\n",
      "Epoch 698: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 699: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 700: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 701: train loss = 0.0001, val loss = 0.0807\n",
      "Epoch 702: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 703: train loss = 0.0001, val loss = 0.0803\n",
      "Epoch 704: train loss = 0.0001, val loss = 0.0814\n",
      "Epoch 705: train loss = 0.0001, val loss = 0.0814\n",
      "Epoch 706: train loss = 0.0001, val loss = 0.0803\n",
      "Epoch 707: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 708: train loss = 0.0001, val loss = 0.0804\n",
      "Epoch 709: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 710: train loss = 0.0001, val loss = 0.0814\n",
      "Epoch 711: train loss = 0.0001, val loss = 0.0800\n",
      "Epoch 712: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 713: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 714: train loss = 0.0001, val loss = 0.0800\n",
      "Epoch 715: train loss = 0.0002, val loss = 0.0812\n",
      "Epoch 716: train loss = 0.0001, val loss = 0.0794\n",
      "Epoch 717: train loss = 0.0001, val loss = 0.0801\n",
      "Epoch 718: train loss = 0.0001, val loss = 0.0819\n",
      "Epoch 719: train loss = 0.0001, val loss = 0.0808\n",
      "Epoch 720: train loss = 0.0001, val loss = 0.0808\n",
      "Epoch 721: train loss = 0.0001, val loss = 0.0807\n",
      "Epoch 722: train loss = 0.0001, val loss = 0.0813\n",
      "Epoch 723: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 724: train loss = 0.0001, val loss = 0.0812\n",
      "Epoch 725: train loss = 0.0001, val loss = 0.0805\n",
      "Epoch 726: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 727: train loss = 0.0001, val loss = 0.0807\n",
      "Epoch 728: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 729: train loss = 0.0001, val loss = 0.0805\n",
      "Epoch 730: train loss = 0.0001, val loss = 0.0810\n",
      "Epoch 731: train loss = 0.0001, val loss = 0.0807\n",
      "Epoch 732: train loss = 0.0001, val loss = 0.0809\n",
      "Epoch 733: train loss = 0.0001, val loss = 0.0812\n",
      "Epoch 734: train loss = 0.0001, val loss = 0.0807\n",
      "Epoch 735: train loss = 0.0001, val loss = 0.0808\n",
      "Epoch 736: train loss = 0.0001, val loss = 0.0805\n",
      "Epoch 737: train loss = 0.0001, val loss = 0.0808\n",
      "Epoch 738: train loss = 0.0001, val loss = 0.0805\n",
      "Epoch 739: train loss = 0.0001, val loss = 0.0800\n",
      "Epoch 740: train loss = 0.0001, val loss = 0.0803\n",
      "Epoch 741: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 742: train loss = 0.0001, val loss = 0.0806\n",
      "Epoch 743: train loss = 0.0000, val loss = 0.0804\n",
      "Epoch 744: train loss = 0.0000, val loss = 0.0808\n",
      "Epoch 745: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 746: train loss = 0.0001, val loss = 0.0802\n",
      "Epoch 747: train loss = 0.0001, val loss = 0.0799\n",
      "Epoch 748: train loss = 0.0001, val loss = 0.0801\n",
      "Epoch 749: train loss = 0.0001, val loss = 0.0801\n",
      "Epoch 750: train loss = 0.0001, val loss = 0.0808\n",
      "Epoch 751: train loss = 0.0001, val loss = 0.0806\n",
      "Epoch 752: train loss = 0.0001, val loss = 0.0797\n",
      "Epoch 753: train loss = 0.0001, val loss = 0.0798\n",
      "Epoch 754: train loss = 0.0001, val loss = 0.0818\n",
      "Epoch 755: train loss = 0.0002, val loss = 0.0797\n",
      "Epoch 756: train loss = 0.0002, val loss = 0.0799\n",
      "Epoch 757: train loss = 0.0002, val loss = 0.0807\n",
      "Epoch 758: train loss = 0.0002, val loss = 0.0794\n",
      "Epoch 759: train loss = 0.0002, val loss = 0.0807\n",
      "Epoch 760: train loss = 0.0002, val loss = 0.0802\n",
      "Epoch 761: train loss = 0.0002, val loss = 0.0793\n",
      "Epoch 762: train loss = 0.0002, val loss = 0.0799\n",
      "Epoch 763: train loss = 0.0002, val loss = 0.0799\n",
      "Epoch 764: train loss = 0.0002, val loss = 0.0801\n",
      "Epoch 765: train loss = 0.0002, val loss = 0.0793\n",
      "Epoch 766: train loss = 0.0002, val loss = 0.0802\n",
      "Epoch 767: train loss = 0.0002, val loss = 0.0812\n",
      "Epoch 768: train loss = 0.0002, val loss = 0.0801\n",
      "Epoch 769: train loss = 0.0002, val loss = 0.0799\n",
      "Epoch 770: train loss = 0.0002, val loss = 0.0808\n",
      "Epoch 771: train loss = 0.0002, val loss = 0.0782\n",
      "Epoch 772: train loss = 0.0002, val loss = 0.0802\n",
      "Epoch 773: train loss = 0.0003, val loss = 0.0800\n",
      "Epoch 774: train loss = 0.0003, val loss = 0.0791\n",
      "Epoch 775: train loss = 0.0003, val loss = 0.0813\n",
      "Epoch 776: train loss = 0.0002, val loss = 0.0802\n",
      "Epoch 777: train loss = 0.0002, val loss = 0.0794\n",
      "Epoch 778: train loss = 0.0002, val loss = 0.0802\n",
      "Epoch 779: train loss = 0.0002, val loss = 0.0784\n",
      "Epoch 780: train loss = 0.0002, val loss = 0.0796\n",
      "Epoch 781: train loss = 0.0002, val loss = 0.0792\n",
      "Epoch 782: train loss = 0.0002, val loss = 0.0791\n",
      "Epoch 783: train loss = 0.0002, val loss = 0.0808\n",
      "Epoch 784: train loss = 0.0002, val loss = 0.0787\n",
      "Epoch 785: train loss = 0.0002, val loss = 0.0803\n",
      "Epoch 786: train loss = 0.0002, val loss = 0.0796\n",
      "Epoch 787: train loss = 0.0002, val loss = 0.0788\n",
      "Epoch 788: train loss = 0.0001, val loss = 0.0795\n",
      "Epoch 789: train loss = 0.0001, val loss = 0.0786\n",
      "Epoch 790: train loss = 0.0001, val loss = 0.0790\n",
      "Epoch 791: train loss = 0.0001, val loss = 0.0791\n",
      "Epoch 792: train loss = 0.0001, val loss = 0.0792\n",
      "Epoch 793: train loss = 0.0002, val loss = 0.0787\n",
      "Epoch 794: train loss = 0.0002, val loss = 0.0797\n",
      "Epoch 795: train loss = 0.0002, val loss = 0.0793\n",
      "Epoch 796: train loss = 0.0003, val loss = 0.0805\n",
      "Epoch 797: train loss = 0.0004, val loss = 0.0792\n",
      "Epoch 798: train loss = 0.0003, val loss = 0.0797\n",
      "Epoch 799: train loss = 0.0003, val loss = 0.0793\n",
      "Epoch 800: train loss = 0.0003, val loss = 0.0803\n",
      "Epoch 801: train loss = 0.0003, val loss = 0.0770\n",
      "Epoch 802: train loss = 0.0003, val loss = 0.0795\n",
      "Epoch 803: train loss = 0.0003, val loss = 0.0787\n",
      "Epoch 804: train loss = 0.0003, val loss = 0.0794\n",
      "Epoch 805: train loss = 0.0004, val loss = 0.0791\n",
      "Epoch 806: train loss = 0.0004, val loss = 0.0787\n",
      "Epoch 807: train loss = 0.0003, val loss = 0.0784\n",
      "Epoch 808: train loss = 0.0003, val loss = 0.0790\n",
      "Epoch 809: train loss = 0.0004, val loss = 0.0777\n",
      "Epoch 810: train loss = 0.0004, val loss = 0.0813\n",
      "Epoch 811: train loss = 0.0004, val loss = 0.0782\n",
      "Epoch 812: train loss = 0.0004, val loss = 0.0789\n",
      "Epoch 813: train loss = 0.0003, val loss = 0.0776\n",
      "Epoch 814: train loss = 0.0003, val loss = 0.0790\n",
      "Epoch 815: train loss = 0.0002, val loss = 0.0783\n",
      "Epoch 816: train loss = 0.0002, val loss = 0.0793\n",
      "Epoch 817: train loss = 0.0002, val loss = 0.0784\n",
      "Epoch 818: train loss = 0.0002, val loss = 0.0790\n",
      "Epoch 819: train loss = 0.0001, val loss = 0.0790\n",
      "Epoch 820: train loss = 0.0001, val loss = 0.0780\n",
      "Epoch 821: train loss = 0.0001, val loss = 0.0788\n",
      "Epoch 822: train loss = 0.0001, val loss = 0.0778\n",
      "Epoch 823: train loss = 0.0001, val loss = 0.0790\n",
      "Epoch 824: train loss = 0.0001, val loss = 0.0779\n",
      "Epoch 825: train loss = 0.0001, val loss = 0.0783\n",
      "Epoch 826: train loss = 0.0001, val loss = 0.0784\n",
      "Epoch 827: train loss = 0.0000, val loss = 0.0783\n",
      "Epoch 828: train loss = 0.0000, val loss = 0.0788\n",
      "Epoch 829: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 830: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 831: train loss = 0.0000, val loss = 0.0783\n",
      "Epoch 832: train loss = 0.0000, val loss = 0.0791\n",
      "Epoch 833: train loss = 0.0000, val loss = 0.0788\n",
      "Epoch 834: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 835: train loss = 0.0000, val loss = 0.0783\n",
      "Epoch 836: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 837: train loss = 0.0000, val loss = 0.0787\n",
      "Epoch 838: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 839: train loss = 0.0000, val loss = 0.0782\n",
      "Epoch 840: train loss = 0.0000, val loss = 0.0787\n",
      "Epoch 841: train loss = 0.0000, val loss = 0.0787\n",
      "Epoch 842: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 843: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 844: train loss = 0.0000, val loss = 0.0783\n",
      "Epoch 845: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 846: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 847: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 848: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 849: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 850: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 851: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 852: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 853: train loss = 0.0000, val loss = 0.0782\n",
      "Epoch 854: train loss = 0.0000, val loss = 0.0787\n",
      "Epoch 855: train loss = 0.0000, val loss = 0.0783\n",
      "Epoch 856: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 857: train loss = 0.0000, val loss = 0.0789\n",
      "Epoch 858: train loss = 0.0000, val loss = 0.0782\n",
      "Epoch 859: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 860: train loss = 0.0000, val loss = 0.0788\n",
      "Epoch 861: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 862: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 863: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 864: train loss = 0.0000, val loss = 0.0787\n",
      "Epoch 865: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 866: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 867: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 868: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 869: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 870: train loss = 0.0000, val loss = 0.0783\n",
      "Epoch 871: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 872: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 873: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 874: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 875: train loss = 0.0000, val loss = 0.0782\n",
      "Epoch 876: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 877: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 878: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 879: train loss = 0.0000, val loss = 0.0783\n",
      "Epoch 880: train loss = 0.0000, val loss = 0.0789\n",
      "Epoch 881: train loss = 0.0000, val loss = 0.0779\n",
      "Epoch 882: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 883: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 884: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 885: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 886: train loss = 0.0000, val loss = 0.0788\n",
      "Epoch 887: train loss = 0.0000, val loss = 0.0785\n",
      "Epoch 888: train loss = 0.0000, val loss = 0.0786\n",
      "Epoch 889: train loss = 0.0000, val loss = 0.0781\n",
      "Epoch 890: train loss = 0.0000, val loss = 0.0784\n",
      "Epoch 891: train loss = 0.0001, val loss = 0.0787\n",
      "Epoch 892: train loss = 0.0001, val loss = 0.0787\n",
      "Epoch 893: train loss = 0.0001, val loss = 0.0783\n",
      "Epoch 894: train loss = 0.0001, val loss = 0.0789\n",
      "Epoch 895: train loss = 0.0001, val loss = 0.0779\n",
      "Epoch 896: train loss = 0.0001, val loss = 0.0785\n",
      "Epoch 897: train loss = 0.0001, val loss = 0.0781\n",
      "Epoch 898: train loss = 0.0001, val loss = 0.0789\n",
      "Epoch 899: train loss = 0.0001, val loss = 0.0782\n",
      "Epoch 900: train loss = 0.0001, val loss = 0.0787\n",
      "Epoch 901: train loss = 0.0001, val loss = 0.0777\n",
      "Epoch 902: train loss = 0.0002, val loss = 0.0789\n",
      "Epoch 903: train loss = 0.0002, val loss = 0.0777\n",
      "Epoch 904: train loss = 0.0002, val loss = 0.0783\n",
      "Epoch 905: train loss = 0.0002, val loss = 0.0785\n",
      "Epoch 906: train loss = 0.0003, val loss = 0.0789\n",
      "Epoch 907: train loss = 0.0003, val loss = 0.0765\n",
      "Epoch 908: train loss = 0.0004, val loss = 0.0798\n",
      "Epoch 909: train loss = 0.0004, val loss = 0.0787\n",
      "Epoch 910: train loss = 0.0003, val loss = 0.0768\n",
      "Epoch 911: train loss = 0.0003, val loss = 0.0788\n",
      "Epoch 912: train loss = 0.0003, val loss = 0.0785\n",
      "Epoch 913: train loss = 0.0004, val loss = 0.0767\n",
      "Epoch 914: train loss = 0.0004, val loss = 0.0788\n",
      "Epoch 915: train loss = 0.0004, val loss = 0.0772\n",
      "Epoch 916: train loss = 0.0004, val loss = 0.0787\n",
      "Epoch 917: train loss = 0.0003, val loss = 0.0786\n",
      "Epoch 918: train loss = 0.0003, val loss = 0.0771\n",
      "Epoch 919: train loss = 0.0002, val loss = 0.0769\n",
      "Epoch 920: train loss = 0.0002, val loss = 0.0784\n",
      "Epoch 921: train loss = 0.0002, val loss = 0.0770\n",
      "Epoch 922: train loss = 0.0002, val loss = 0.0769\n",
      "Epoch 923: train loss = 0.0002, val loss = 0.0785\n",
      "Epoch 924: train loss = 0.0002, val loss = 0.0779\n",
      "Epoch 925: train loss = 0.0002, val loss = 0.0780\n",
      "Epoch 926: train loss = 0.0002, val loss = 0.0784\n",
      "Epoch 927: train loss = 0.0002, val loss = 0.0782\n",
      "Epoch 928: train loss = 0.0002, val loss = 0.0780\n",
      "Epoch 929: train loss = 0.0001, val loss = 0.0777\n",
      "Epoch 930: train loss = 0.0001, val loss = 0.0776\n",
      "Epoch 931: train loss = 0.0001, val loss = 0.0773\n",
      "Epoch 932: train loss = 0.0001, val loss = 0.0773\n",
      "Epoch 933: train loss = 0.0001, val loss = 0.0775\n",
      "Epoch 934: train loss = 0.0001, val loss = 0.0765\n",
      "Epoch 935: train loss = 0.0001, val loss = 0.0774\n",
      "Epoch 936: train loss = 0.0002, val loss = 0.0775\n",
      "Epoch 937: train loss = 0.0002, val loss = 0.0787\n",
      "Epoch 938: train loss = 0.0002, val loss = 0.0769\n",
      "Epoch 939: train loss = 0.0002, val loss = 0.0771\n",
      "Epoch 940: train loss = 0.0002, val loss = 0.0771\n",
      "Epoch 941: train loss = 0.0002, val loss = 0.0772\n",
      "Epoch 942: train loss = 0.0002, val loss = 0.0778\n",
      "Epoch 943: train loss = 0.0001, val loss = 0.0764\n",
      "Epoch 944: train loss = 0.0002, val loss = 0.0766\n",
      "Epoch 945: train loss = 0.0002, val loss = 0.0778\n",
      "Epoch 946: train loss = 0.0002, val loss = 0.0761\n",
      "Epoch 947: train loss = 0.0002, val loss = 0.0776\n",
      "Epoch 948: train loss = 0.0002, val loss = 0.0776\n",
      "Epoch 949: train loss = 0.0002, val loss = 0.0773\n",
      "Epoch 950: train loss = 0.0002, val loss = 0.0770\n",
      "Epoch 951: train loss = 0.0001, val loss = 0.0776\n",
      "Epoch 952: train loss = 0.0002, val loss = 0.0772\n",
      "Epoch 953: train loss = 0.0002, val loss = 0.0776\n",
      "Epoch 954: train loss = 0.0002, val loss = 0.0777\n",
      "Epoch 955: train loss = 0.0002, val loss = 0.0768\n",
      "Epoch 956: train loss = 0.0002, val loss = 0.0772\n",
      "Epoch 957: train loss = 0.0002, val loss = 0.0770\n",
      "Epoch 958: train loss = 0.0002, val loss = 0.0771\n",
      "Epoch 959: train loss = 0.0002, val loss = 0.0781\n",
      "Epoch 960: train loss = 0.0002, val loss = 0.0770\n",
      "Epoch 961: train loss = 0.0002, val loss = 0.0779\n",
      "Epoch 962: train loss = 0.0003, val loss = 0.0763\n",
      "Epoch 963: train loss = 0.0002, val loss = 0.0785\n",
      "Epoch 964: train loss = 0.0002, val loss = 0.0764\n",
      "Epoch 965: train loss = 0.0002, val loss = 0.0764\n",
      "Epoch 966: train loss = 0.0002, val loss = 0.0768\n",
      "Epoch 967: train loss = 0.0002, val loss = 0.0767\n",
      "Epoch 968: train loss = 0.0002, val loss = 0.0765\n",
      "Epoch 969: train loss = 0.0002, val loss = 0.0774\n",
      "Epoch 970: train loss = 0.0002, val loss = 0.0774\n",
      "Epoch 971: train loss = 0.0001, val loss = 0.0760\n",
      "Epoch 972: train loss = 0.0001, val loss = 0.0775\n",
      "Epoch 973: train loss = 0.0001, val loss = 0.0774\n",
      "Epoch 974: train loss = 0.0001, val loss = 0.0766\n",
      "Epoch 975: train loss = 0.0002, val loss = 0.0767\n",
      "Epoch 976: train loss = 0.0002, val loss = 0.0776\n",
      "Epoch 977: train loss = 0.0002, val loss = 0.0766\n",
      "Epoch 978: train loss = 0.0001, val loss = 0.0766\n",
      "Epoch 979: train loss = 0.0001, val loss = 0.0771\n",
      "Epoch 980: train loss = 0.0001, val loss = 0.0772\n",
      "Epoch 981: train loss = 0.0001, val loss = 0.0764\n",
      "Epoch 982: train loss = 0.0001, val loss = 0.0769\n",
      "Epoch 983: train loss = 0.0001, val loss = 0.0770\n",
      "Epoch 984: train loss = 0.0001, val loss = 0.0768\n",
      "Epoch 985: train loss = 0.0001, val loss = 0.0769\n",
      "Epoch 986: train loss = 0.0001, val loss = 0.0765\n",
      "Epoch 987: train loss = 0.0001, val loss = 0.0765\n",
      "Epoch 988: train loss = 0.0001, val loss = 0.0767\n",
      "Epoch 989: train loss = 0.0001, val loss = 0.0767\n",
      "Epoch 990: train loss = 0.0001, val loss = 0.0768\n",
      "Epoch 991: train loss = 0.0001, val loss = 0.0769\n",
      "Epoch 992: train loss = 0.0001, val loss = 0.0770\n",
      "Epoch 993: train loss = 0.0001, val loss = 0.0766\n",
      "Epoch 994: train loss = 0.0001, val loss = 0.0765\n",
      "Epoch 995: train loss = 0.0001, val loss = 0.0764\n",
      "Epoch 996: train loss = 0.0001, val loss = 0.0765\n",
      "Epoch 997: train loss = 0.0001, val loss = 0.0775\n",
      "Epoch 998: train loss = 0.0001, val loss = 0.0769\n",
      "Epoch 999: train loss = 0.0001, val loss = 0.0771\n",
      "Epoch 1000: train loss = 0.0001, val loss = 0.0776\n"
     ]
    }
   ],
   "source": [
    "model = train_final_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0c8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mood_predictions(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83af6f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1562\n"
     ]
    }
   ],
   "source": [
    "from mood_RNN_classifier import get_accuracy_rate\n",
    "accuracy = get_accuracy_rate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a1aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  predicted_mood_class\n",
      "0   AS14.01                    24\n",
      "1   AS14.02                    27\n",
      "2   AS14.03                    25\n",
      "3   AS14.05                    33\n",
      "4   AS14.06                    36\n",
      "5   AS14.07                    27\n",
      "6   AS14.08                    20\n",
      "7   AS14.09                    23\n",
      "8   AS14.12                    25\n",
      "9   AS14.13                    27\n",
      "10  AS14.14                    32\n",
      "11  AS14.15                    25\n",
      "12  AS14.16                    20\n",
      "13  AS14.17                    25\n",
      "14  AS14.19                    25\n",
      "15  AS14.20                    23\n",
      "16  AS14.23                    26\n",
      "17  AS14.24                    27\n",
      "18  AS14.25                    27\n",
      "19  AS14.26                    27\n",
      "20  AS14.27                    25\n",
      "21  AS14.28                    26\n",
      "22  AS14.29                    25\n",
      "23  AS14.30                    23\n",
      "24  AS14.31                    32\n",
      "25  AS14.32                    23\n",
      "26  AS14.33                    27\n"
     ]
    }
   ],
   "source": [
    "# Run predictions on test_df\n",
    "test_predictions = predict(model, test_df, id_map, device)\n",
    "\n",
    "# Attach predictions to test_df\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted_mood_class'] = test_predictions\n",
    "\n",
    "# Optional: save to CSV or examine\n",
    "print(test_df_with_preds[['id', 'predicted_mood_class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3933caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n",
       "        32, 33, 34, 35, 36, 38], dtype=int32),\n",
       " mood\n",
       " 28    168\n",
       " 29     84\n",
       " 32     72\n",
       " 26     71\n",
       " 30     70\n",
       " 31     61\n",
       " 27     53\n",
       " 25     37\n",
       " 24     32\n",
       " 33     14\n",
       " 22      7\n",
       " 34      6\n",
       " 23      6\n",
       " 21      5\n",
       " 35      4\n",
       " 14      2\n",
       " 20      2\n",
       " 18      2\n",
       " 36      1\n",
       " 12      1\n",
       " 19      1\n",
       " 16      1\n",
       " 38      1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_split = train_df_split.sort_values('mood')\n",
    "train_df_split['mood'].unique(), train_df_split['mood'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
