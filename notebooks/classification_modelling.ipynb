{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60ca131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the path (/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1) to sys.path\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ./initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf5c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from data_loading import DataPreprocessor\n",
    "from mood_RNN_classifier import RNNClassifier, MoodDataset, OrdinalLabelSmoothingLoss, objective, train_epoch, train_final_model, evaluate, predict, plot_mood_predictions\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c42a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataPreprocessor()\n",
    "train_df_split, val_df_split, test_df = data_loader.load_and_preprocess_data(\"1d\", 0.25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6928e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume train_df and test_df are loaded and preprocessed\n",
    "id_map = {id_: idx for idx, id_ in enumerate(train_df_split['id'].unique())}\n",
    "input_dim = train_df_split.drop(columns=['id', 'mood', 'date']).shape[1]\n",
    "id_count = len(id_map)\n",
    "output_dim = train_df_split['mood'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c89292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 15:13:40,449] A new study created in memory with name: no-name-b7d588b1-9946-4866-b4b0-813bb96d0675\n",
      "[I 2025-04-18 15:13:45,376] Trial 0 finished with value: 0.4681664854288101 and parameters: {'hidden_dim': 113, 'id_embed_dim': 13, 'lr': 0.0018290305221790848, 'batch_size': 32, 'alpha': 0.11388129814345592}. Best is trial 0 with value: 0.4681664854288101.\n",
      "[I 2025-04-18 15:13:45,641] Trial 1 finished with value: 0.523064736276865 and parameters: {'hidden_dim': 123, 'id_embed_dim': 14, 'lr': 0.00015132697900167012, 'batch_size': 32, 'alpha': 0.045487882563767096}. Best is trial 0 with value: 0.4681664854288101.\n",
      "[I 2025-04-18 15:13:45,811] Trial 2 finished with value: 1.8916876912117004 and parameters: {'hidden_dim': 87, 'id_embed_dim': 4, 'lr': 0.00010031469977455742, 'batch_size': 64, 'alpha': 0.2815102541617919}. Best is trial 0 with value: 0.4681664854288101.\n",
      "[I 2025-04-18 15:13:46,047] Trial 3 finished with value: 0.7950034514069557 and parameters: {'hidden_dim': 46, 'id_embed_dim': 10, 'lr': 0.0003972792580456188, 'batch_size': 32, 'alpha': 0.16257131095486524}. Best is trial 0 with value: 0.4681664854288101.\n",
      "[I 2025-04-18 15:13:46,215] Trial 4 finished with value: 0.0838837530463934 and parameters: {'hidden_dim': 81, 'id_embed_dim': 4, 'lr': 0.0017374673065625088, 'batch_size': 64, 'alpha': 0.012024919159616232}. Best is trial 4 with value: 0.0838837530463934.\n",
      "[I 2025-04-18 15:13:46,378] Trial 5 finished with value: 0.2775767520070076 and parameters: {'hidden_dim': 54, 'id_embed_dim': 15, 'lr': 0.002506549382960651, 'batch_size': 64, 'alpha': 0.05290825563357353}. Best is trial 4 with value: 0.0838837530463934.\n",
      "[I 2025-04-18 15:13:46,635] Trial 6 finished with value: 1.2904304563999176 and parameters: {'hidden_dim': 109, 'id_embed_dim': 4, 'lr': 0.0001568312151119993, 'batch_size': 32, 'alpha': 0.20455894926557536}. Best is trial 4 with value: 0.0838837530463934.\n",
      "[I 2025-04-18 15:13:46,864] Trial 7 finished with value: 0.6994346305727959 and parameters: {'hidden_dim': 32, 'id_embed_dim': 13, 'lr': 0.0003088803437244455, 'batch_size': 32, 'alpha': 0.07035428466086847}. Best is trial 4 with value: 0.0838837530463934.\n",
      "[I 2025-04-18 15:13:46,999] Trial 8 finished with value: 1.5824967622756958 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.00012645020464343814, 'batch_size': 128, 'alpha': 0.15023485735146255}. Best is trial 4 with value: 0.0838837530463934.\n",
      "[I 2025-04-18 15:13:47,123] Trial 9 finished with value: 1.2182536721229553 and parameters: {'hidden_dim': 54, 'id_embed_dim': 13, 'lr': 0.000742377095102712, 'batch_size': 128, 'alpha': 0.2088465592434368}. Best is trial 4 with value: 0.0838837530463934.\n",
      "[I 2025-04-18 15:13:47,299] Trial 10 finished with value: 0.08513662591576576 and parameters: {'hidden_dim': 81, 'id_embed_dim': 7, 'lr': 0.00590842617289912, 'batch_size': 64, 'alpha': 0.012991344410208393}. Best is trial 4 with value: 0.0838837530463934.\n",
      "[I 2025-04-18 15:13:47,475] Trial 11 finished with value: 0.07277459558099508 and parameters: {'hidden_dim': 82, 'id_embed_dim': 7, 'lr': 0.009933685326770681, 'batch_size': 64, 'alpha': 0.011327869888570593}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:47,652] Trial 12 finished with value: 0.12737048044800758 and parameters: {'hidden_dim': 91, 'id_embed_dim': 7, 'lr': 0.008837759431408364, 'batch_size': 64, 'alpha': 0.021316545873473265}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:47,826] Trial 13 finished with value: 0.37315308302640915 and parameters: {'hidden_dim': 68, 'id_embed_dim': 7, 'lr': 0.004049228217786213, 'batch_size': 64, 'alpha': 0.07844598422637351}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:48,001] Trial 14 finished with value: 0.5270907133817673 and parameters: {'hidden_dim': 73, 'id_embed_dim': 6, 'lr': 0.0011101529062295212, 'batch_size': 64, 'alpha': 0.10512838458570953}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:48,190] Trial 15 finished with value: 0.0740227960050106 and parameters: {'hidden_dim': 100, 'id_embed_dim': 9, 'lr': 0.003190796321331472, 'batch_size': 64, 'alpha': 0.010912635833472403}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:48,368] Trial 16 finished with value: 0.5250110924243927 and parameters: {'hidden_dim': 98, 'id_embed_dim': 10, 'lr': 0.009350400987462742, 'batch_size': 64, 'alpha': 0.1076899838751517}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:48,505] Trial 17 finished with value: 0.8184731006622314 and parameters: {'hidden_dim': 100, 'id_embed_dim': 9, 'lr': 0.004489936987019035, 'batch_size': 128, 'alpha': 0.26963918377034435}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:48,692] Trial 18 finished with value: 0.23662160709500313 and parameters: {'hidden_dim': 126, 'id_embed_dim': 9, 'lr': 0.0031854088293186457, 'batch_size': 64, 'alpha': 0.044434060296293}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:48,865] Trial 19 finished with value: 0.5764445066452026 and parameters: {'hidden_dim': 67, 'id_embed_dim': 11, 'lr': 0.006882747831047334, 'batch_size': 64, 'alpha': 0.15022999376063026}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:49,005] Trial 20 finished with value: 0.34329238533973694 and parameters: {'hidden_dim': 98, 'id_embed_dim': 8, 'lr': 0.00532942014698121, 'batch_size': 128, 'alpha': 0.07144939982622305}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:49,180] Trial 21 finished with value: 0.08193711005151272 and parameters: {'hidden_dim': 78, 'id_embed_dim': 5, 'lr': 0.0015756755421229792, 'batch_size': 64, 'alpha': 0.011580485454204278}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:49,352] Trial 22 finished with value: 0.26728077977895737 and parameters: {'hidden_dim': 73, 'id_embed_dim': 6, 'lr': 0.0009453936157830476, 'batch_size': 64, 'alpha': 0.035088622850087195}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:49,527] Trial 23 finished with value: 0.07588023878633976 and parameters: {'hidden_dim': 88, 'id_embed_dim': 5, 'lr': 0.001785964121933123, 'batch_size': 64, 'alpha': 0.01228205894115518}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:49,705] Trial 24 finished with value: 0.3960404098033905 and parameters: {'hidden_dim': 93, 'id_embed_dim': 6, 'lr': 0.0030073640731541197, 'batch_size': 64, 'alpha': 0.07725307963072413}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:49,887] Trial 25 finished with value: 0.29298482462763786 and parameters: {'hidden_dim': 107, 'id_embed_dim': 8, 'lr': 0.0006172415835432244, 'batch_size': 64, 'alpha': 0.03431327079680463}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:50,066] Trial 26 finished with value: 0.31321588158607483 and parameters: {'hidden_dim': 87, 'id_embed_dim': 5, 'lr': 0.002634313640189778, 'batch_size': 64, 'alpha': 0.060365545221698194}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:50,247] Trial 27 finished with value: 0.577888548374176 and parameters: {'hidden_dim': 104, 'id_embed_dim': 8, 'lr': 0.001234609767603408, 'batch_size': 64, 'alpha': 0.12818453997216694}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:50,441] Trial 28 finished with value: 0.40262511372566223 and parameters: {'hidden_dim': 120, 'id_embed_dim': 9, 'lr': 0.0036195877605176606, 'batch_size': 64, 'alpha': 0.08698171045474973}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:50,638] Trial 29 finished with value: 0.2100990116596222 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.00216785643396368, 'batch_size': 128, 'alpha': 0.031113313744780453}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:50,899] Trial 30 finished with value: 0.7011072859168053 and parameters: {'hidden_dim': 92, 'id_embed_dim': 5, 'lr': 0.007338987282375094, 'batch_size': 32, 'alpha': 0.18008196804754084}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:51,074] Trial 31 finished with value: 0.09530497621744871 and parameters: {'hidden_dim': 77, 'id_embed_dim': 5, 'lr': 0.0017205191103337068, 'batch_size': 64, 'alpha': 0.013377322087674759}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:51,251] Trial 32 finished with value: 0.3028450198471546 and parameters: {'hidden_dim': 86, 'id_embed_dim': 6, 'lr': 0.0015640589323021204, 'batch_size': 64, 'alpha': 0.04802282318217406}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:51,421] Trial 33 finished with value: 0.2388521060347557 and parameters: {'hidden_dim': 64, 'id_embed_dim': 5, 'lr': 0.0012763801090126762, 'batch_size': 64, 'alpha': 0.03167009309619612}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:51,591] Trial 34 finished with value: 0.14553630724549294 and parameters: {'hidden_dim': 61, 'id_embed_dim': 7, 'lr': 0.0005712295741751686, 'batch_size': 64, 'alpha': 0.010686027725063342}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:51,845] Trial 35 finished with value: 0.28442294523119926 and parameters: {'hidden_dim': 85, 'id_embed_dim': 8, 'lr': 0.002232437584791516, 'batch_size': 32, 'alpha': 0.05043163867139305}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:52,020] Trial 36 finished with value: 0.17709745839238167 and parameters: {'hidden_dim': 76, 'id_embed_dim': 4, 'lr': 0.0048461115552508455, 'batch_size': 64, 'alpha': 0.02813343383202875}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:52,202] Trial 37 finished with value: 0.44091787189245224 and parameters: {'hidden_dim': 95, 'id_embed_dim': 16, 'lr': 0.0015410012873176796, 'batch_size': 64, 'alpha': 0.09651545587944627}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:52,378] Trial 38 finished with value: 0.37307586520910263 and parameters: {'hidden_dim': 82, 'id_embed_dim': 5, 'lr': 0.0008725390464475807, 'batch_size': 64, 'alpha': 0.05737968162307721}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:52,642] Trial 39 finished with value: 0.6564423963427544 and parameters: {'hidden_dim': 103, 'id_embed_dim': 4, 'lr': 0.00038319877572930757, 'batch_size': 32, 'alpha': 0.12617692216600615}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:52,816] Trial 40 finished with value: 0.8290920257568359 and parameters: {'hidden_dim': 57, 'id_embed_dim': 6, 'lr': 0.0019956455599431924, 'batch_size': 64, 'alpha': 0.26691215740278235}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:52,998] Trial 41 finished with value: 0.07928508892655373 and parameters: {'hidden_dim': 80, 'id_embed_dim': 4, 'lr': 0.001535811432638374, 'batch_size': 64, 'alpha': 0.010310213907764808}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:53,180] Trial 42 finished with value: 0.1586754359304905 and parameters: {'hidden_dim': 70, 'id_embed_dim': 4, 'lr': 0.0015186944652883568, 'batch_size': 64, 'alpha': 0.022337547721106216}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:53,367] Trial 43 finished with value: 0.23580515757203102 and parameters: {'hidden_dim': 78, 'id_embed_dim': 4, 'lr': 0.002964726012506606, 'batch_size': 64, 'alpha': 0.04102695219041705}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:53,556] Trial 44 finished with value: 0.08954713493585587 and parameters: {'hidden_dim': 89, 'id_embed_dim': 7, 'lr': 0.001318604357642588, 'batch_size': 64, 'alpha': 0.013186212304386598}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:53,732] Trial 45 finished with value: 0.3301607742905617 and parameters: {'hidden_dim': 83, 'id_embed_dim': 5, 'lr': 0.002444937706012674, 'batch_size': 64, 'alpha': 0.06284924455553617}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:53,862] Trial 46 finished with value: 0.3588092178106308 and parameters: {'hidden_dim': 73, 'id_embed_dim': 6, 'lr': 0.0008025940260701477, 'batch_size': 128, 'alpha': 0.022733330575949888}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:54,100] Trial 47 finished with value: 0.7409217804670334 and parameters: {'hidden_dim': 39, 'id_embed_dim': 11, 'lr': 0.0019026172292723527, 'batch_size': 32, 'alpha': 0.23770953790698307}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:54,281] Trial 48 finished with value: 0.2973298542201519 and parameters: {'hidden_dim': 110, 'id_embed_dim': 7, 'lr': 0.0010901018393235755, 'batch_size': 64, 'alpha': 0.043393792303976414}. Best is trial 11 with value: 0.07277459558099508.\n",
      "[I 2025-04-18 15:13:54,457] Trial 49 finished with value: 0.062070056796073914 and parameters: {'hidden_dim': 79, 'id_embed_dim': 4, 'lr': 0.003657801521133304, 'batch_size': 64, 'alpha': 0.01022974192365805}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:54,636] Trial 50 finished with value: 0.13384096510708332 and parameters: {'hidden_dim': 95, 'id_embed_dim': 4, 'lr': 0.006823587998592768, 'batch_size': 64, 'alpha': 0.022513523950143152}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:54,811] Trial 51 finished with value: 0.06471984926611185 and parameters: {'hidden_dim': 82, 'id_embed_dim': 5, 'lr': 0.004018570794159905, 'batch_size': 64, 'alpha': 0.010532696165238584}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:54,988] Trial 52 finished with value: 0.24038389697670937 and parameters: {'hidden_dim': 89, 'id_embed_dim': 4, 'lr': 0.003654022984555324, 'batch_size': 64, 'alpha': 0.038448446233111166}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:55,163] Trial 53 finished with value: 0.13403751514852047 and parameters: {'hidden_dim': 81, 'id_embed_dim': 5, 'lr': 0.005833833007726294, 'batch_size': 64, 'alpha': 0.020371859496813284}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:55,340] Trial 54 finished with value: 0.27200085669755936 and parameters: {'hidden_dim': 85, 'id_embed_dim': 14, 'lr': 0.0039044052129073347, 'batch_size': 64, 'alpha': 0.05239358580558251}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:55,471] Trial 55 finished with value: 0.1533421277999878 and parameters: {'hidden_dim': 71, 'id_embed_dim': 9, 'lr': 0.009496908725501378, 'batch_size': 128, 'alpha': 0.027723394283179617}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:55,653] Trial 56 finished with value: 0.3541419506072998 and parameters: {'hidden_dim': 100, 'id_embed_dim': 6, 'lr': 0.0048153730143507475, 'batch_size': 64, 'alpha': 0.0699029049948849}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:55,833] Trial 57 finished with value: 0.07318941690027714 and parameters: {'hidden_dim': 91, 'id_embed_dim': 4, 'lr': 0.007751318423159181, 'batch_size': 64, 'alpha': 0.011121611016811004}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:56,013] Trial 58 finished with value: 0.2387586049735546 and parameters: {'hidden_dim': 96, 'id_embed_dim': 7, 'lr': 0.006743568526899552, 'batch_size': 64, 'alpha': 0.04182584971107593}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:56,190] Trial 59 finished with value: 0.10850418731570244 and parameters: {'hidden_dim': 90, 'id_embed_dim': 10, 'lr': 0.00789542458828527, 'batch_size': 64, 'alpha': 0.018862742557759354}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:56,374] Trial 60 finished with value: 0.19466596841812134 and parameters: {'hidden_dim': 104, 'id_embed_dim': 6, 'lr': 0.005863499141011314, 'batch_size': 64, 'alpha': 0.03461354685954107}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:56,551] Trial 61 finished with value: 0.07875617127865553 and parameters: {'hidden_dim': 75, 'id_embed_dim': 4, 'lr': 0.008317929820241557, 'batch_size': 64, 'alpha': 0.012954187239796786}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:56,727] Trial 62 finished with value: 0.15718849934637547 and parameters: {'hidden_dim': 67, 'id_embed_dim': 5, 'lr': 0.00787418501204503, 'batch_size': 64, 'alpha': 0.02659938433493026}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:56,901] Trial 63 finished with value: 0.07031652703881264 and parameters: {'hidden_dim': 76, 'id_embed_dim': 4, 'lr': 0.004286232495197508, 'batch_size': 64, 'alpha': 0.010520719347664919}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:57,078] Trial 64 finished with value: 0.21090563386678696 and parameters: {'hidden_dim': 87, 'id_embed_dim': 5, 'lr': 0.004341962093039242, 'batch_size': 64, 'alpha': 0.034407764319104105}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:57,254] Trial 65 finished with value: 0.13842740841209888 and parameters: {'hidden_dim': 83, 'id_embed_dim': 4, 'lr': 0.0033821787678755323, 'batch_size': 64, 'alpha': 0.02071852739004978}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:57,391] Trial 66 finished with value: 0.7634177505970001 and parameters: {'hidden_dim': 91, 'id_embed_dim': 5, 'lr': 0.0027758016511771977, 'batch_size': 128, 'alpha': 0.19156141787840075}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:57,648] Trial 67 finished with value: 0.33896622993052006 and parameters: {'hidden_dim': 78, 'id_embed_dim': 6, 'lr': 0.005481987165061121, 'batch_size': 32, 'alpha': 0.0620468144698145}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:57,833] Trial 68 finished with value: 0.0626414455473423 and parameters: {'hidden_dim': 99, 'id_embed_dim': 5, 'lr': 0.009662708614407324, 'batch_size': 64, 'alpha': 0.010091450559473528}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:58,012] Trial 69 finished with value: 0.24527743086218834 and parameters: {'hidden_dim': 100, 'id_embed_dim': 8, 'lr': 0.009528991329543452, 'batch_size': 64, 'alpha': 0.04872990502768702}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:58,196] Trial 70 finished with value: 0.8173207342624664 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.00500584658930234, 'batch_size': 64, 'alpha': 0.29565305022572136}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:58,376] Trial 71 finished with value: 0.0727227870374918 and parameters: {'hidden_dim': 94, 'id_embed_dim': 5, 'lr': 0.006537299601806638, 'batch_size': 64, 'alpha': 0.011034414950301299}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:58,555] Trial 72 finished with value: 0.16396279633045197 and parameters: {'hidden_dim': 93, 'id_embed_dim': 4, 'lr': 0.006391234222162461, 'batch_size': 64, 'alpha': 0.02903825655416975}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:58,737] Trial 73 finished with value: 0.125601002946496 and parameters: {'hidden_dim': 107, 'id_embed_dim': 5, 'lr': 0.008397956086781474, 'batch_size': 64, 'alpha': 0.019676634723132913}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:58,919] Trial 74 finished with value: 0.22395707294344902 and parameters: {'hidden_dim': 102, 'id_embed_dim': 5, 'lr': 0.007274899691988518, 'batch_size': 64, 'alpha': 0.03667514135448521}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:59,100] Trial 75 finished with value: 0.09905735962092876 and parameters: {'hidden_dim': 98, 'id_embed_dim': 6, 'lr': 0.009918302960127818, 'batch_size': 64, 'alpha': 0.016850657869884596}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:59,275] Trial 76 finished with value: 0.3453768938779831 and parameters: {'hidden_dim': 96, 'id_embed_dim': 4, 'lr': 0.00020957336405619237, 'batch_size': 64, 'alpha': 0.010817816321034584}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:59,453] Trial 77 finished with value: 0.7119856029748917 and parameters: {'hidden_dim': 85, 'id_embed_dim': 4, 'lr': 0.00404310221951531, 'batch_size': 64, 'alpha': 0.16119896746924098}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:59,634] Trial 78 finished with value: 0.17091302201151848 and parameters: {'hidden_dim': 106, 'id_embed_dim': 7, 'lr': 0.006286186862951096, 'batch_size': 64, 'alpha': 0.028123019829155795}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:13:59,887] Trial 79 finished with value: 0.23412230983376503 and parameters: {'hidden_dim': 79, 'id_embed_dim': 5, 'lr': 0.004450793091040038, 'batch_size': 32, 'alpha': 0.04474174091054341}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:14:00,061] Trial 80 finished with value: 0.27172382548451424 and parameters: {'hidden_dim': 74, 'id_embed_dim': 10, 'lr': 0.00550880094192695, 'batch_size': 64, 'alpha': 0.05409517861646497}. Best is trial 49 with value: 0.062070056796073914.\n",
      "[I 2025-04-18 15:14:00,237] Trial 81 finished with value: 0.06021663825958967 and parameters: {'hidden_dim': 88, 'id_embed_dim': 5, 'lr': 0.0035264112706169647, 'batch_size': 64, 'alpha': 0.010151417438700731}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:00,415] Trial 82 finished with value: 0.11263645812869072 and parameters: {'hidden_dim': 92, 'id_embed_dim': 6, 'lr': 0.008724606103400694, 'batch_size': 64, 'alpha': 0.01822351379112864}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:00,592] Trial 83 finished with value: 0.06762806605547667 and parameters: {'hidden_dim': 88, 'id_embed_dim': 4, 'lr': 0.0031064240289405496, 'batch_size': 64, 'alpha': 0.010008053890561825}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:00,771] Trial 84 finished with value: 0.16873539984226227 and parameters: {'hidden_dim': 87, 'id_embed_dim': 4, 'lr': 0.003308528165732895, 'batch_size': 64, 'alpha': 0.026321961841218648}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:00,948] Trial 85 finished with value: 0.07156832329928875 and parameters: {'hidden_dim': 82, 'id_embed_dim': 4, 'lr': 0.0024659968184332025, 'batch_size': 64, 'alpha': 0.010270920336392726}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:01,124] Trial 86 finished with value: 0.19405557215213776 and parameters: {'hidden_dim': 84, 'id_embed_dim': 5, 'lr': 0.002718184194024365, 'batch_size': 64, 'alpha': 0.03327347308456942}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:01,258] Trial 87 finished with value: 0.12933211773633957 and parameters: {'hidden_dim': 81, 'id_embed_dim': 4, 'lr': 0.0024726679203562313, 'batch_size': 128, 'alpha': 0.01727045958947967}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:01,432] Trial 88 finished with value: 0.15941500291228294 and parameters: {'hidden_dim': 72, 'id_embed_dim': 5, 'lr': 0.0037584244996770695, 'batch_size': 64, 'alpha': 0.02622258292259401}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:01,607] Trial 89 finished with value: 0.20612267032265663 and parameters: {'hidden_dim': 76, 'id_embed_dim': 5, 'lr': 0.0043344593655513675, 'batch_size': 64, 'alpha': 0.03844887208475216}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:01,781] Trial 90 finished with value: 0.5558489412069321 and parameters: {'hidden_dim': 69, 'id_embed_dim': 4, 'lr': 0.004947550493198354, 'batch_size': 64, 'alpha': 0.13694947625565296}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:01,961] Trial 91 finished with value: 0.0742750084027648 and parameters: {'hidden_dim': 94, 'id_embed_dim': 4, 'lr': 0.0051829103705104615, 'batch_size': 64, 'alpha': 0.010927330697617156}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:02,141] Trial 92 finished with value: 0.06547915749251842 and parameters: {'hidden_dim': 89, 'id_embed_dim': 4, 'lr': 0.0029741281099014056, 'batch_size': 64, 'alpha': 0.010067683777993032}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:02,321] Trial 93 finished with value: 0.12032098695635796 and parameters: {'hidden_dim': 88, 'id_embed_dim': 4, 'lr': 0.0022615141918375763, 'batch_size': 64, 'alpha': 0.017329857024812805}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:02,498] Trial 94 finished with value: 0.14989588968455791 and parameters: {'hidden_dim': 80, 'id_embed_dim': 5, 'lr': 0.0030388211896073404, 'batch_size': 64, 'alpha': 0.024614286587165423}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:02,687] Trial 95 finished with value: 0.1186351329088211 and parameters: {'hidden_dim': 83, 'id_embed_dim': 4, 'lr': 0.003667849315354093, 'batch_size': 64, 'alpha': 0.017548156448156552}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:02,868] Trial 96 finished with value: 0.06971769221127033 and parameters: {'hidden_dim': 86, 'id_embed_dim': 6, 'lr': 0.002924608272237144, 'batch_size': 64, 'alpha': 0.010218437800222283}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:03,051] Trial 97 finished with value: 0.7414254546165466 and parameters: {'hidden_dim': 90, 'id_embed_dim': 6, 'lr': 0.003336147085553238, 'batch_size': 64, 'alpha': 0.2213029659000111}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:03,235] Trial 98 finished with value: 0.1868940219283104 and parameters: {'hidden_dim': 98, 'id_embed_dim': 5, 'lr': 0.002880211277616796, 'batch_size': 64, 'alpha': 0.03107071399616128}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:03,497] Trial 99 finished with value: 0.15449298918247223 and parameters: {'hidden_dim': 86, 'id_embed_dim': 5, 'lr': 0.0021566607155656626, 'batch_size': 32, 'alpha': 0.022797145396724644}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:03,671] Trial 100 finished with value: 0.2421407327055931 and parameters: {'hidden_dim': 65, 'id_embed_dim': 4, 'lr': 0.0040525716434357635, 'batch_size': 64, 'alpha': 0.040374784047539775}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:03,850] Trial 101 finished with value: 0.07773599214851856 and parameters: {'hidden_dim': 82, 'id_embed_dim': 6, 'lr': 0.0025076995213306505, 'batch_size': 64, 'alpha': 0.011022459549262027}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:04,030] Trial 102 finished with value: 0.10122990980744362 and parameters: {'hidden_dim': 78, 'id_embed_dim': 5, 'lr': 0.0031330420915552906, 'batch_size': 64, 'alpha': 0.01640845610621217}. Best is trial 81 with value: 0.06021663825958967.\n",
      "[I 2025-04-18 15:14:04,210] Trial 103 finished with value: 0.06010524183511734 and parameters: {'hidden_dim': 88, 'id_embed_dim': 6, 'lr': 0.0044784488650891074, 'batch_size': 64, 'alpha': 0.010146206103464608}. Best is trial 103 with value: 0.06010524183511734.\n",
      "[I 2025-04-18 15:14:04,391] Trial 104 finished with value: 0.15724772587418556 and parameters: {'hidden_dim': 89, 'id_embed_dim': 4, 'lr': 0.003456135734380842, 'batch_size': 64, 'alpha': 0.02311841082552707}. Best is trial 103 with value: 0.06010524183511734.\n",
      "[I 2025-04-18 15:14:04,579] Trial 105 finished with value: 0.05948306433856487 and parameters: {'hidden_dim': 93, 'id_embed_dim': 6, 'lr': 0.004345868468791349, 'batch_size': 64, 'alpha': 0.0100615855841976}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:04,759] Trial 106 finished with value: 0.09964122250676155 and parameters: {'hidden_dim': 85, 'id_embed_dim': 6, 'lr': 0.004138816136653732, 'batch_size': 64, 'alpha': 0.01670971342161563}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:04,899] Trial 107 finished with value: 0.18479077517986298 and parameters: {'hidden_dim': 88, 'id_embed_dim': 7, 'lr': 0.0045922593424063025, 'batch_size': 128, 'alpha': 0.03132226747575596}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:05,082] Trial 108 finished with value: 0.14775068871676922 and parameters: {'hidden_dim': 92, 'id_embed_dim': 6, 'lr': 0.0027453066855292474, 'batch_size': 64, 'alpha': 0.024594927401947245}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:05,260] Trial 109 finished with value: 0.27693911641836166 and parameters: {'hidden_dim': 76, 'id_embed_dim': 4, 'lr': 0.002322789278971399, 'batch_size': 64, 'alpha': 0.04632095005398405}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:05,437] Trial 110 finished with value: 0.21288979053497314 and parameters: {'hidden_dim': 80, 'id_embed_dim': 5, 'lr': 0.002001147436813616, 'batch_size': 64, 'alpha': 0.03505374586794964}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:05,617] Trial 111 finished with value: 0.09429601486772299 and parameters: {'hidden_dim': 96, 'id_embed_dim': 5, 'lr': 0.0036823440994044132, 'batch_size': 64, 'alpha': 0.01474983858411924}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:05,801] Trial 112 finished with value: 0.06112545728683472 and parameters: {'hidden_dim': 94, 'id_embed_dim': 6, 'lr': 0.006062007234889331, 'batch_size': 64, 'alpha': 0.010318687309158449}. Best is trial 105 with value: 0.05948306433856487.\n",
      "[I 2025-04-18 15:14:05,980] Trial 113 finished with value: 0.05637368932366371 and parameters: {'hidden_dim': 84, 'id_embed_dim': 6, 'lr': 0.004507834295211084, 'batch_size': 64, 'alpha': 0.010055510066362107}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:06,161] Trial 114 finished with value: 0.12533805146813393 and parameters: {'hidden_dim': 90, 'id_embed_dim': 6, 'lr': 0.005626721365075721, 'batch_size': 64, 'alpha': 0.021043803097329776}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:06,341] Trial 115 finished with value: 0.13694806583225727 and parameters: {'hidden_dim': 87, 'id_embed_dim': 7, 'lr': 0.00443974221459036, 'batch_size': 64, 'alpha': 0.025556261749523412}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:06,520] Trial 116 finished with value: 0.1215133797377348 and parameters: {'hidden_dim': 84, 'id_embed_dim': 6, 'lr': 0.004936753443469066, 'batch_size': 64, 'alpha': 0.018198336711294144}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:06,704] Trial 117 finished with value: 0.1831851489841938 and parameters: {'hidden_dim': 93, 'id_embed_dim': 6, 'lr': 0.005957424827267481, 'batch_size': 64, 'alpha': 0.031781505786996456}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:06,891] Trial 118 finished with value: 0.05914961080998182 and parameters: {'hidden_dim': 101, 'id_embed_dim': 7, 'lr': 0.004061615088338, 'batch_size': 64, 'alpha': 0.010349822380566797}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:07,077] Trial 119 finished with value: 0.10980921983718872 and parameters: {'hidden_dim': 102, 'id_embed_dim': 7, 'lr': 0.003964770016790916, 'batch_size': 64, 'alpha': 0.016738143903215645}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:07,261] Trial 120 finished with value: 0.1856811437755823 and parameters: {'hidden_dim': 97, 'id_embed_dim': 6, 'lr': 0.003208707047579059, 'batch_size': 64, 'alpha': 0.02904553771497121}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:07,444] Trial 121 finished with value: 0.11939665675163269 and parameters: {'hidden_dim': 100, 'id_embed_dim': 7, 'lr': 0.0035494059589319597, 'batch_size': 64, 'alpha': 0.021150591972580013}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:07,626] Trial 122 finished with value: 0.0739502627402544 and parameters: {'hidden_dim': 91, 'id_embed_dim': 6, 'lr': 0.004513508528402396, 'batch_size': 64, 'alpha': 0.011120511553879234}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:07,807] Trial 123 finished with value: 0.09996172972023487 and parameters: {'hidden_dim': 86, 'id_embed_dim': 8, 'lr': 0.005180635204565509, 'batch_size': 64, 'alpha': 0.015807358653366473}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:07,990] Trial 124 finished with value: 0.05938589107245207 and parameters: {'hidden_dim': 94, 'id_embed_dim': 5, 'lr': 0.004051626312304565, 'batch_size': 64, 'alpha': 0.01004572139146201}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:08,252] Trial 125 finished with value: 0.13646054174751043 and parameters: {'hidden_dim': 94, 'id_embed_dim': 6, 'lr': 0.0029548199395267563, 'batch_size': 32, 'alpha': 0.02358250254599236}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:08,436] Trial 126 finished with value: 0.05760482233017683 and parameters: {'hidden_dim': 109, 'id_embed_dim': 5, 'lr': 0.0038576153843227555, 'batch_size': 64, 'alpha': 0.010019717171239749}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:08,617] Trial 127 finished with value: 0.21929778158664703 and parameters: {'hidden_dim': 110, 'id_embed_dim': 5, 'lr': 0.0038912848468230046, 'batch_size': 64, 'alpha': 0.037198680971038484}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:08,803] Trial 128 finished with value: 0.4455567002296448 and parameters: {'hidden_dim': 102, 'id_embed_dim': 5, 'lr': 0.0048229604271378375, 'batch_size': 128, 'alpha': 0.09249172673280567}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:08,988] Trial 129 finished with value: 0.18431450799107552 and parameters: {'hidden_dim': 118, 'id_embed_dim': 5, 'lr': 0.007151588006051869, 'batch_size': 64, 'alpha': 0.029231219156123756}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:09,172] Trial 130 finished with value: 0.12955574318766594 and parameters: {'hidden_dim': 113, 'id_embed_dim': 5, 'lr': 0.003471492422781448, 'batch_size': 64, 'alpha': 0.019302543728964144}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:09,355] Trial 131 finished with value: 0.05763131659477949 and parameters: {'hidden_dim': 99, 'id_embed_dim': 6, 'lr': 0.0039695010677607955, 'batch_size': 64, 'alpha': 0.010537118163698095}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:09,540] Trial 132 finished with value: 0.13009753823280334 and parameters: {'hidden_dim': 104, 'id_embed_dim': 6, 'lr': 0.004000181285374831, 'batch_size': 64, 'alpha': 0.021975027631735377}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:09,726] Trial 133 finished with value: 0.06224840972572565 and parameters: {'hidden_dim': 98, 'id_embed_dim': 7, 'lr': 0.005305445236199117, 'batch_size': 64, 'alpha': 0.010317059507525256}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:09,912] Trial 134 finished with value: 0.1584041453897953 and parameters: {'hidden_dim': 100, 'id_embed_dim': 7, 'lr': 0.0059678666150021825, 'batch_size': 64, 'alpha': 0.026324384749616864}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:10,093] Trial 135 finished with value: 0.08726436831057072 and parameters: {'hidden_dim': 98, 'id_embed_dim': 7, 'lr': 0.005379372466798855, 'batch_size': 64, 'alpha': 0.015364028706784513}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:10,276] Trial 136 finished with value: 0.060280997306108475 and parameters: {'hidden_dim': 107, 'id_embed_dim': 8, 'lr': 0.004627819358806118, 'batch_size': 64, 'alpha': 0.010035661453957017}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:10,461] Trial 137 finished with value: 0.08820304460823536 and parameters: {'hidden_dim': 109, 'id_embed_dim': 8, 'lr': 0.004845883664772612, 'batch_size': 64, 'alpha': 0.015459613990905022}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:10,645] Trial 138 finished with value: 0.11534898169338703 and parameters: {'hidden_dim': 107, 'id_embed_dim': 7, 'lr': 0.006334806571135626, 'batch_size': 64, 'alpha': 0.019989456562095057}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:10,826] Trial 139 finished with value: 0.17245101928710938 and parameters: {'hidden_dim': 112, 'id_embed_dim': 8, 'lr': 0.004299121976948775, 'batch_size': 64, 'alpha': 0.03141826333571024}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:11,010] Trial 140 finished with value: 0.10688392259180546 and parameters: {'hidden_dim': 105, 'id_embed_dim': 6, 'lr': 0.005444023660947476, 'batch_size': 64, 'alpha': 0.01608358984310898}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:11,235] Trial 141 finished with value: 0.06933672446757555 and parameters: {'hidden_dim': 95, 'id_embed_dim': 6, 'lr': 0.004626419035076393, 'batch_size': 64, 'alpha': 0.011032634185158378}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:11,430] Trial 142 finished with value: 0.06839730218052864 and parameters: {'hidden_dim': 99, 'id_embed_dim': 7, 'lr': 0.0037262764551501786, 'batch_size': 64, 'alpha': 0.01058107750190083}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:11,619] Trial 143 finished with value: 0.15119598805904388 and parameters: {'hidden_dim': 102, 'id_embed_dim': 5, 'lr': 0.004214085314125216, 'batch_size': 64, 'alpha': 0.023239956283712927}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:11,803] Trial 144 finished with value: 0.10415535792708397 and parameters: {'hidden_dim': 109, 'id_embed_dim': 6, 'lr': 0.00343218818839205, 'batch_size': 64, 'alpha': 0.016738633160736427}. Best is trial 113 with value: 0.05637368932366371.\n",
      "[I 2025-04-18 15:14:11,979] Trial 145 finished with value: 0.05575660057365894 and parameters: {'hidden_dim': 96, 'id_embed_dim': 6, 'lr': 0.003816630655505969, 'batch_size': 64, 'alpha': 0.010107326787615903}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:12,159] Trial 146 finished with value: 0.1395486779510975 and parameters: {'hidden_dim': 97, 'id_embed_dim': 6, 'lr': 0.0038865860945538826, 'batch_size': 64, 'alpha': 0.026385465484950483}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:12,347] Trial 147 finished with value: 0.6308977603912354 and parameters: {'hidden_dim': 95, 'id_embed_dim': 8, 'lr': 0.004673731913382907, 'batch_size': 64, 'alpha': 0.1720109933443074}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:12,800] Trial 148 finished with value: 0.1229746975004673 and parameters: {'hidden_dim': 106, 'id_embed_dim': 7, 'lr': 0.005300534007940678, 'batch_size': 64, 'alpha': 0.02116774379697065}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:13,084] Trial 149 finished with value: 0.07460592151619494 and parameters: {'hidden_dim': 93, 'id_embed_dim': 6, 'lr': 0.0005314309374891666, 'batch_size': 32, 'alpha': 0.010022572235023593}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:13,268] Trial 150 finished with value: 0.46805672347545624 and parameters: {'hidden_dim': 100, 'id_embed_dim': 16, 'lr': 0.0060606451071630206, 'batch_size': 64, 'alpha': 0.11485206796809318}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:13,445] Trial 151 finished with value: 0.058687797747552395 and parameters: {'hidden_dim': 91, 'id_embed_dim': 5, 'lr': 0.004274790594965941, 'batch_size': 64, 'alpha': 0.010100948554105883}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:13,623] Trial 152 finished with value: 0.5889081805944443 and parameters: {'hidden_dim': 97, 'id_embed_dim': 9, 'lr': 0.00012120805211057818, 'batch_size': 64, 'alpha': 0.01648030761609524}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:13,802] Trial 153 finished with value: 0.1008982453495264 and parameters: {'hidden_dim': 92, 'id_embed_dim': 5, 'lr': 0.0042426071499003975, 'batch_size': 64, 'alpha': 0.016261397494534827}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:13,981] Trial 154 finished with value: 0.14085305109620094 and parameters: {'hidden_dim': 116, 'id_embed_dim': 5, 'lr': 0.003794324847340085, 'batch_size': 64, 'alpha': 0.02299984399387048}. Best is trial 145 with value: 0.05575660057365894.\n",
      "[I 2025-04-18 15:14:14,149] Trial 155 finished with value: 0.054521625861525536 and parameters: {'hidden_dim': 47, 'id_embed_dim': 6, 'lr': 0.005006320268060413, 'batch_size': 64, 'alpha': 0.010070315901064447}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:14,314] Trial 156 finished with value: 0.1717185527086258 and parameters: {'hidden_dim': 44, 'id_embed_dim': 6, 'lr': 0.005173698048179927, 'batch_size': 64, 'alpha': 0.02805785673932406}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:14,480] Trial 157 finished with value: 0.08737398497760296 and parameters: {'hidden_dim': 39, 'id_embed_dim': 6, 'lr': 0.0068722473101013645, 'batch_size': 64, 'alpha': 0.015571305133629932}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:14,649] Trial 158 finished with value: 0.2132485806941986 and parameters: {'hidden_dim': 55, 'id_embed_dim': 7, 'lr': 0.004360374061848208, 'batch_size': 64, 'alpha': 0.03678901023557135}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:14,818] Trial 159 finished with value: 0.057454246096313 and parameters: {'hidden_dim': 60, 'id_embed_dim': 6, 'lr': 0.004828698719772949, 'batch_size': 64, 'alpha': 0.010314535655493576}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:14,955] Trial 160 finished with value: 0.14281145110726357 and parameters: {'hidden_dim': 95, 'id_embed_dim': 6, 'lr': 0.004788093803722929, 'batch_size': 128, 'alpha': 0.02104717905286705}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:15,123] Trial 161 finished with value: 0.057367563247680664 and parameters: {'hidden_dim': 51, 'id_embed_dim': 6, 'lr': 0.005832607288182225, 'batch_size': 64, 'alpha': 0.010289517838758307}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:15,291] Trial 162 finished with value: 0.09410452470183372 and parameters: {'hidden_dim': 50, 'id_embed_dim': 6, 'lr': 0.005345091711073236, 'batch_size': 64, 'alpha': 0.01523064389216174}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:15,459] Trial 163 finished with value: 0.11231420747935772 and parameters: {'hidden_dim': 49, 'id_embed_dim': 6, 'lr': 0.005613791465759479, 'batch_size': 64, 'alpha': 0.021187308396209928}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:15,623] Trial 164 finished with value: 0.059809610247612 and parameters: {'hidden_dim': 43, 'id_embed_dim': 7, 'lr': 0.004766157562879854, 'batch_size': 64, 'alpha': 0.010415999842380248}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:15,790] Trial 165 finished with value: 0.8295840919017792 and parameters: {'hidden_dim': 45, 'id_embed_dim': 6, 'lr': 0.0046386280511040495, 'batch_size': 64, 'alpha': 0.24956356288564507}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:15,955] Trial 166 finished with value: 0.09300353843718767 and parameters: {'hidden_dim': 40, 'id_embed_dim': 6, 'lr': 0.0034754282082432777, 'batch_size': 64, 'alpha': 0.015835620463645735}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:16,122] Trial 167 finished with value: 0.05831987410783768 and parameters: {'hidden_dim': 49, 'id_embed_dim': 7, 'lr': 0.004126305255194449, 'batch_size': 64, 'alpha': 0.010148297673505028}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:16,290] Trial 168 finished with value: 0.13601714000105858 and parameters: {'hidden_dim': 51, 'id_embed_dim': 7, 'lr': 0.004082839207749004, 'batch_size': 64, 'alpha': 0.025460055989922615}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:16,458] Trial 169 finished with value: 0.12580928578972816 and parameters: {'hidden_dim': 41, 'id_embed_dim': 7, 'lr': 0.0046064615892393275, 'batch_size': 64, 'alpha': 0.01944563222570983}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:16,624] Trial 170 finished with value: 0.1823395937681198 and parameters: {'hidden_dim': 47, 'id_embed_dim': 7, 'lr': 0.006248953281822331, 'batch_size': 64, 'alpha': 0.031479388610536965}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:16,796] Trial 171 finished with value: 0.05783358868211508 and parameters: {'hidden_dim': 60, 'id_embed_dim': 6, 'lr': 0.003745523247473091, 'batch_size': 64, 'alpha': 0.01005111123853883}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:16,966] Trial 172 finished with value: 0.08413415495306253 and parameters: {'hidden_dim': 60, 'id_embed_dim': 6, 'lr': 0.004128068184460311, 'batch_size': 64, 'alpha': 0.015105163389779552}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:17,131] Trial 173 finished with value: 0.07313432078808546 and parameters: {'hidden_dim': 34, 'id_embed_dim': 6, 'lr': 0.0037115087023601077, 'batch_size': 64, 'alpha': 0.01116938467666189}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:17,298] Trial 174 finished with value: 0.1171121746301651 and parameters: {'hidden_dim': 53, 'id_embed_dim': 6, 'lr': 0.004971589508490396, 'batch_size': 64, 'alpha': 0.02031651055822805}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:17,466] Trial 175 finished with value: 0.08620170876383781 and parameters: {'hidden_dim': 48, 'id_embed_dim': 7, 'lr': 0.004375932485737988, 'batch_size': 64, 'alpha': 0.01559912129075491}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:17,634] Trial 176 finished with value: 0.3137691840529442 and parameters: {'hidden_dim': 57, 'id_embed_dim': 6, 'lr': 0.00027728945752777707, 'batch_size': 64, 'alpha': 0.010415420797922544}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:17,802] Trial 177 finished with value: 0.05984174972400069 and parameters: {'hidden_dim': 52, 'id_embed_dim': 8, 'lr': 0.0032234607336740933, 'batch_size': 64, 'alpha': 0.01005872064244086}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:17,969] Trial 178 finished with value: 0.15195956453680992 and parameters: {'hidden_dim': 43, 'id_embed_dim': 8, 'lr': 0.0031410067527657577, 'batch_size': 64, 'alpha': 0.02475837557363804}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:18,138] Trial 179 finished with value: 0.1212528683245182 and parameters: {'hidden_dim': 53, 'id_embed_dim': 8, 'lr': 0.003849380304198892, 'batch_size': 64, 'alpha': 0.01917715154528948}. Best is trial 155 with value: 0.054521625861525536.\n",
      "[I 2025-04-18 15:14:18,305] Trial 180 finished with value: 0.052933297120034695 and parameters: {'hidden_dim': 51, 'id_embed_dim': 9, 'lr': 0.0033181967938678844, 'batch_size': 64, 'alpha': 0.010225054303317455}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:18,472] Trial 181 finished with value: 0.056391652673482895 and parameters: {'hidden_dim': 51, 'id_embed_dim': 9, 'lr': 0.0034245205281232725, 'batch_size': 64, 'alpha': 0.010465428819804503}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:18,640] Trial 182 finished with value: 0.09027346037328243 and parameters: {'hidden_dim': 51, 'id_embed_dim': 9, 'lr': 0.003338750480833961, 'batch_size': 64, 'alpha': 0.016186148602323192}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:18,808] Trial 183 finished with value: 0.11093003675341606 and parameters: {'hidden_dim': 56, 'id_embed_dim': 9, 'lr': 0.003216092166157717, 'batch_size': 64, 'alpha': 0.020748190368780034}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:18,976] Trial 184 finished with value: 0.05564007814973593 and parameters: {'hidden_dim': 46, 'id_embed_dim': 8, 'lr': 0.0035994667538364953, 'batch_size': 64, 'alpha': 0.010225691470805794}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:19,144] Trial 185 finished with value: 0.15564073249697685 and parameters: {'hidden_dim': 46, 'id_embed_dim': 10, 'lr': 0.002722308066215108, 'batch_size': 64, 'alpha': 0.02713403187353794}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:19,309] Trial 186 finished with value: 0.07946198619902134 and parameters: {'hidden_dim': 43, 'id_embed_dim': 8, 'lr': 0.003814356715649519, 'batch_size': 64, 'alpha': 0.015371792962882958}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:19,555] Trial 187 finished with value: 0.08959376532584429 and parameters: {'hidden_dim': 60, 'id_embed_dim': 9, 'lr': 0.004121025514089459, 'batch_size': 32, 'alpha': 0.016060299460928535}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:19,723] Trial 188 finished with value: 0.13552160002291203 and parameters: {'hidden_dim': 52, 'id_embed_dim': 7, 'lr': 0.0035246061816703277, 'batch_size': 64, 'alpha': 0.023244880992439623}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:19,891] Trial 189 finished with value: 0.05542749539017677 and parameters: {'hidden_dim': 49, 'id_embed_dim': 10, 'lr': 0.0032283450538168174, 'batch_size': 64, 'alpha': 0.010278420825579317}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:20,056] Trial 190 finished with value: 0.09309242106974125 and parameters: {'hidden_dim': 45, 'id_embed_dim': 10, 'lr': 0.003202596422837457, 'batch_size': 64, 'alpha': 0.015559978921853138}. Best is trial 180 with value: 0.052933297120034695.\n",
      "[I 2025-04-18 15:14:20,222] Trial 191 finished with value: 0.05281495116651058 and parameters: {'hidden_dim': 48, 'id_embed_dim': 10, 'lr': 0.003587120013846666, 'batch_size': 64, 'alpha': 0.010239038262565874}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:20,387] Trial 192 finished with value: 0.060028133913874626 and parameters: {'hidden_dim': 48, 'id_embed_dim': 10, 'lr': 0.0029605309383665757, 'batch_size': 64, 'alpha': 0.01067872829683134}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:20,556] Trial 193 finished with value: 0.11491445451974869 and parameters: {'hidden_dim': 49, 'id_embed_dim': 11, 'lr': 0.0036316069434739, 'batch_size': 64, 'alpha': 0.020533462631548014}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:20,722] Trial 194 finished with value: 0.08569162804633379 and parameters: {'hidden_dim': 47, 'id_embed_dim': 10, 'lr': 0.004019451315005948, 'batch_size': 64, 'alpha': 0.01516145454828492}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:20,891] Trial 195 finished with value: 0.1561558712273836 and parameters: {'hidden_dim': 54, 'id_embed_dim': 10, 'lr': 0.0033115556286922015, 'batch_size': 64, 'alpha': 0.02687697745402417}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:21,060] Trial 196 finished with value: 0.0928086619824171 and parameters: {'hidden_dim': 50, 'id_embed_dim': 9, 'lr': 0.003672984050878827, 'batch_size': 64, 'alpha': 0.015412741795112338}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:21,200] Trial 197 finished with value: 0.11841563880443573 and parameters: {'hidden_dim': 124, 'id_embed_dim': 11, 'lr': 0.004015710722521931, 'batch_size': 128, 'alpha': 0.02011554454219036}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:21,366] Trial 198 finished with value: 0.08922139182686806 and parameters: {'hidden_dim': 42, 'id_embed_dim': 9, 'lr': 0.0026277850904043833, 'batch_size': 64, 'alpha': 0.014810665193794842}. Best is trial 191 with value: 0.05281495116651058.\n",
      "[I 2025-04-18 15:14:21,534] Trial 199 finished with value: 0.06342181377112865 and parameters: {'hidden_dim': 37, 'id_embed_dim': 9, 'lr': 0.0033846426169939275, 'batch_size': 64, 'alpha': 0.010230548232747261}. Best is trial 191 with value: 0.05281495116651058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'hidden_dim': 48, 'id_embed_dim': 10, 'lr': 0.003587120013846666, 'batch_size': 64, 'alpha': 0.010239038262565874}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, train_df_split, val_df_split, id_map, input_dim, id_count, output_dim, device), n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparams:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c89fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "model = RNNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    id_count=id_count,\n",
    "    id_embed_dim=best_params['id_embed_dim'],\n",
    "    output_dim=output_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = OrdinalLabelSmoothingLoss(num_classes=output_dim, alpha=best_params['alpha'])\n",
    "\n",
    "train_loader = DataLoader(MoodDataset(train_df_split, id_map), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(MoodDataset(val_df_split, id_map), batch_size=best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0bfbe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.4274, val loss = 0.2782\n",
      "Epoch 2: train loss = 0.1921, val loss = 0.1144\n",
      "Epoch 3: train loss = 0.0939, val loss = 0.0780\n",
      "Epoch 4: train loss = 0.0692, val loss = 0.0640\n",
      "Epoch 5: train loss = 0.0602, val loss = 0.0569\n",
      "Epoch 6: train loss = 0.0553, val loss = 0.0538\n",
      "Epoch 7: train loss = 0.0519, val loss = 0.0535\n",
      "Epoch 8: train loss = 0.0494, val loss = 0.0533\n",
      "Epoch 9: train loss = 0.0470, val loss = 0.0525\n",
      "Epoch 10: train loss = 0.0450, val loss = 0.0535\n",
      "Epoch 11: train loss = 0.0429, val loss = 0.0530\n",
      "Epoch 12: train loss = 0.0410, val loss = 0.0530\n",
      "Epoch 13: train loss = 0.0399, val loss = 0.0534\n",
      "Epoch 14: train loss = 0.0387, val loss = 0.0572\n",
      "Epoch 15: train loss = 0.0379, val loss = 0.0562\n",
      "Epoch 16: train loss = 0.0367, val loss = 0.0551\n",
      "Epoch 17: train loss = 0.0358, val loss = 0.0561\n",
      "Epoch 18: train loss = 0.0348, val loss = 0.0578\n",
      "Epoch 19: train loss = 0.0340, val loss = 0.0574\n",
      "Epoch 20: train loss = 0.0330, val loss = 0.0571\n",
      "Epoch 21: train loss = 0.0319, val loss = 0.0600\n",
      "Epoch 22: train loss = 0.0309, val loss = 0.0598\n",
      "Epoch 23: train loss = 0.0305, val loss = 0.0582\n",
      "Epoch 24: train loss = 0.0296, val loss = 0.0626\n",
      "Epoch 25: train loss = 0.0294, val loss = 0.0625\n",
      "Epoch 26: train loss = 0.0282, val loss = 0.0601\n",
      "Epoch 27: train loss = 0.0272, val loss = 0.0622\n",
      "Epoch 28: train loss = 0.0261, val loss = 0.0610\n",
      "Epoch 29: train loss = 0.0254, val loss = 0.0624\n",
      "Epoch 30: train loss = 0.0250, val loss = 0.0654\n",
      "Epoch 31: train loss = 0.0237, val loss = 0.0643\n",
      "Epoch 32: train loss = 0.0230, val loss = 0.0652\n",
      "Epoch 33: train loss = 0.0226, val loss = 0.0670\n",
      "Epoch 34: train loss = 0.0218, val loss = 0.0665\n",
      "Epoch 35: train loss = 0.0210, val loss = 0.0663\n",
      "Epoch 36: train loss = 0.0213, val loss = 0.0680\n",
      "Epoch 37: train loss = 0.0212, val loss = 0.0714\n",
      "Epoch 38: train loss = 0.0196, val loss = 0.0700\n",
      "Epoch 39: train loss = 0.0194, val loss = 0.0775\n",
      "Epoch 40: train loss = 0.0183, val loss = 0.0741\n",
      "Epoch 41: train loss = 0.0178, val loss = 0.0727\n",
      "Epoch 42: train loss = 0.0171, val loss = 0.0737\n",
      "Epoch 43: train loss = 0.0167, val loss = 0.0729\n",
      "Epoch 44: train loss = 0.0164, val loss = 0.0757\n",
      "Epoch 45: train loss = 0.0157, val loss = 0.0748\n",
      "Epoch 46: train loss = 0.0152, val loss = 0.0764\n",
      "Epoch 47: train loss = 0.0148, val loss = 0.0765\n",
      "Epoch 48: train loss = 0.0141, val loss = 0.0774\n",
      "Epoch 49: train loss = 0.0140, val loss = 0.0778\n",
      "Epoch 50: train loss = 0.0138, val loss = 0.0789\n",
      "Epoch 51: train loss = 0.0131, val loss = 0.0789\n",
      "Epoch 52: train loss = 0.0127, val loss = 0.0806\n",
      "Epoch 53: train loss = 0.0125, val loss = 0.0816\n",
      "Epoch 54: train loss = 0.0122, val loss = 0.0818\n",
      "Epoch 55: train loss = 0.0116, val loss = 0.0831\n",
      "Epoch 56: train loss = 0.0112, val loss = 0.0830\n",
      "Epoch 57: train loss = 0.0108, val loss = 0.0864\n",
      "Epoch 58: train loss = 0.0109, val loss = 0.0838\n",
      "Epoch 59: train loss = 0.0101, val loss = 0.0844\n",
      "Epoch 60: train loss = 0.0105, val loss = 0.0882\n",
      "Epoch 61: train loss = 0.0100, val loss = 0.0879\n",
      "Epoch 62: train loss = 0.0093, val loss = 0.0884\n",
      "Epoch 63: train loss = 0.0088, val loss = 0.0881\n",
      "Epoch 64: train loss = 0.0087, val loss = 0.0890\n",
      "Epoch 65: train loss = 0.0087, val loss = 0.0907\n",
      "Epoch 66: train loss = 0.0082, val loss = 0.0906\n",
      "Epoch 67: train loss = 0.0079, val loss = 0.0916\n",
      "Epoch 68: train loss = 0.0078, val loss = 0.0930\n",
      "Epoch 69: train loss = 0.0075, val loss = 0.0913\n",
      "Epoch 70: train loss = 0.0073, val loss = 0.0932\n",
      "Epoch 71: train loss = 0.0071, val loss = 0.0931\n",
      "Epoch 72: train loss = 0.0070, val loss = 0.0944\n",
      "Epoch 73: train loss = 0.0068, val loss = 0.0956\n",
      "Epoch 74: train loss = 0.0068, val loss = 0.0978\n",
      "Epoch 75: train loss = 0.0062, val loss = 0.0936\n",
      "Epoch 76: train loss = 0.0058, val loss = 0.0968\n",
      "Epoch 77: train loss = 0.0055, val loss = 0.0953\n",
      "Epoch 78: train loss = 0.0056, val loss = 0.0966\n",
      "Epoch 79: train loss = 0.0054, val loss = 0.0973\n",
      "Epoch 80: train loss = 0.0050, val loss = 0.0967\n",
      "Epoch 81: train loss = 0.0053, val loss = 0.0979\n",
      "Epoch 82: train loss = 0.0050, val loss = 0.0984\n",
      "Epoch 83: train loss = 0.0048, val loss = 0.0989\n",
      "Epoch 84: train loss = 0.0046, val loss = 0.0985\n",
      "Epoch 85: train loss = 0.0043, val loss = 0.1001\n",
      "Epoch 86: train loss = 0.0043, val loss = 0.1011\n",
      "Epoch 87: train loss = 0.0045, val loss = 0.1013\n",
      "Epoch 88: train loss = 0.0043, val loss = 0.0989\n",
      "Epoch 89: train loss = 0.0039, val loss = 0.1029\n",
      "Epoch 90: train loss = 0.0036, val loss = 0.1048\n",
      "Epoch 91: train loss = 0.0035, val loss = 0.1049\n",
      "Epoch 92: train loss = 0.0034, val loss = 0.1028\n",
      "Epoch 93: train loss = 0.0035, val loss = 0.1064\n",
      "Epoch 94: train loss = 0.0039, val loss = 0.1070\n",
      "Epoch 95: train loss = 0.0037, val loss = 0.1045\n",
      "Epoch 96: train loss = 0.0034, val loss = 0.1048\n",
      "Epoch 97: train loss = 0.0033, val loss = 0.1088\n",
      "Epoch 98: train loss = 0.0030, val loss = 0.1057\n",
      "Epoch 99: train loss = 0.0032, val loss = 0.1075\n",
      "Epoch 100: train loss = 0.0029, val loss = 0.1056\n",
      "Epoch 101: train loss = 0.0029, val loss = 0.1076\n",
      "Epoch 102: train loss = 0.0027, val loss = 0.1067\n",
      "Epoch 103: train loss = 0.0025, val loss = 0.1073\n",
      "Epoch 104: train loss = 0.0026, val loss = 0.1084\n",
      "Epoch 105: train loss = 0.0024, val loss = 0.1067\n",
      "Epoch 106: train loss = 0.0022, val loss = 0.1085\n",
      "Epoch 107: train loss = 0.0024, val loss = 0.1103\n",
      "Epoch 108: train loss = 0.0022, val loss = 0.1060\n",
      "Epoch 109: train loss = 0.0022, val loss = 0.1083\n",
      "Epoch 110: train loss = 0.0021, val loss = 0.1112\n",
      "Epoch 111: train loss = 0.0020, val loss = 0.1098\n",
      "Epoch 112: train loss = 0.0022, val loss = 0.1107\n",
      "Epoch 113: train loss = 0.0019, val loss = 0.1096\n",
      "Epoch 114: train loss = 0.0018, val loss = 0.1087\n",
      "Epoch 115: train loss = 0.0017, val loss = 0.1119\n",
      "Epoch 116: train loss = 0.0017, val loss = 0.1098\n",
      "Epoch 117: train loss = 0.0018, val loss = 0.1070\n",
      "Epoch 118: train loss = 0.0025, val loss = 0.1107\n",
      "Epoch 119: train loss = 0.0020, val loss = 0.1053\n",
      "Epoch 120: train loss = 0.0017, val loss = 0.1115\n",
      "Epoch 121: train loss = 0.0015, val loss = 0.1055\n",
      "Epoch 122: train loss = 0.0014, val loss = 0.1081\n",
      "Epoch 123: train loss = 0.0014, val loss = 0.1115\n",
      "Epoch 124: train loss = 0.0016, val loss = 0.1076\n",
      "Epoch 125: train loss = 0.0018, val loss = 0.1084\n",
      "Epoch 126: train loss = 0.0016, val loss = 0.1079\n",
      "Epoch 127: train loss = 0.0014, val loss = 0.1128\n",
      "Epoch 128: train loss = 0.0014, val loss = 0.1081\n",
      "Epoch 129: train loss = 0.0013, val loss = 0.1096\n",
      "Epoch 130: train loss = 0.0012, val loss = 0.1085\n",
      "Epoch 131: train loss = 0.0013, val loss = 0.1109\n",
      "Epoch 132: train loss = 0.0013, val loss = 0.1112\n",
      "Epoch 133: train loss = 0.0011, val loss = 0.1114\n",
      "Epoch 134: train loss = 0.0011, val loss = 0.1130\n",
      "Epoch 135: train loss = 0.0010, val loss = 0.1108\n",
      "Epoch 136: train loss = 0.0010, val loss = 0.1138\n",
      "Epoch 137: train loss = 0.0012, val loss = 0.1134\n",
      "Epoch 138: train loss = 0.0011, val loss = 0.1125\n",
      "Epoch 139: train loss = 0.0011, val loss = 0.1131\n",
      "Epoch 140: train loss = 0.0010, val loss = 0.1154\n",
      "Epoch 141: train loss = 0.0010, val loss = 0.1164\n",
      "Epoch 142: train loss = 0.0010, val loss = 0.1148\n",
      "Epoch 143: train loss = 0.0009, val loss = 0.1148\n",
      "Epoch 144: train loss = 0.0008, val loss = 0.1142\n",
      "Epoch 145: train loss = 0.0008, val loss = 0.1164\n",
      "Epoch 146: train loss = 0.0008, val loss = 0.1158\n",
      "Epoch 147: train loss = 0.0007, val loss = 0.1160\n",
      "Epoch 148: train loss = 0.0007, val loss = 0.1163\n",
      "Epoch 149: train loss = 0.0007, val loss = 0.1169\n",
      "Epoch 150: train loss = 0.0007, val loss = 0.1162\n",
      "Epoch 151: train loss = 0.0007, val loss = 0.1153\n",
      "Epoch 152: train loss = 0.0007, val loss = 0.1175\n",
      "Epoch 153: train loss = 0.0006, val loss = 0.1169\n",
      "Epoch 154: train loss = 0.0006, val loss = 0.1177\n",
      "Epoch 155: train loss = 0.0007, val loss = 0.1188\n",
      "Epoch 156: train loss = 0.0006, val loss = 0.1173\n",
      "Epoch 157: train loss = 0.0007, val loss = 0.1185\n",
      "Epoch 158: train loss = 0.0007, val loss = 0.1184\n",
      "Epoch 159: train loss = 0.0006, val loss = 0.1189\n",
      "Epoch 160: train loss = 0.0007, val loss = 0.1185\n",
      "Epoch 161: train loss = 0.0006, val loss = 0.1161\n",
      "Epoch 162: train loss = 0.0006, val loss = 0.1187\n",
      "Epoch 163: train loss = 0.0005, val loss = 0.1179\n",
      "Epoch 164: train loss = 0.0005, val loss = 0.1195\n",
      "Epoch 165: train loss = 0.0005, val loss = 0.1179\n",
      "Epoch 166: train loss = 0.0005, val loss = 0.1192\n",
      "Epoch 167: train loss = 0.0005, val loss = 0.1206\n",
      "Epoch 168: train loss = 0.0005, val loss = 0.1196\n",
      "Epoch 169: train loss = 0.0007, val loss = 0.1202\n",
      "Epoch 170: train loss = 0.0006, val loss = 0.1181\n",
      "Epoch 171: train loss = 0.0005, val loss = 0.1180\n",
      "Epoch 172: train loss = 0.0006, val loss = 0.1194\n",
      "Epoch 173: train loss = 0.0006, val loss = 0.1199\n",
      "Epoch 174: train loss = 0.0005, val loss = 0.1183\n",
      "Epoch 175: train loss = 0.0006, val loss = 0.1210\n",
      "Epoch 176: train loss = 0.0005, val loss = 0.1170\n",
      "Epoch 177: train loss = 0.0006, val loss = 0.1200\n",
      "Epoch 178: train loss = 0.0007, val loss = 0.1195\n",
      "Epoch 179: train loss = 0.0006, val loss = 0.1183\n",
      "Epoch 180: train loss = 0.0006, val loss = 0.1190\n",
      "Epoch 181: train loss = 0.0005, val loss = 0.1207\n",
      "Epoch 182: train loss = 0.0004, val loss = 0.1178\n",
      "Epoch 183: train loss = 0.0004, val loss = 0.1188\n",
      "Epoch 184: train loss = 0.0005, val loss = 0.1199\n",
      "Epoch 185: train loss = 0.0006, val loss = 0.1197\n",
      "Epoch 186: train loss = 0.0005, val loss = 0.1199\n",
      "Epoch 187: train loss = 0.0005, val loss = 0.1213\n",
      "Epoch 188: train loss = 0.0004, val loss = 0.1197\n",
      "Epoch 189: train loss = 0.0004, val loss = 0.1229\n",
      "Epoch 190: train loss = 0.0004, val loss = 0.1193\n",
      "Epoch 191: train loss = 0.0004, val loss = 0.1202\n",
      "Epoch 192: train loss = 0.0003, val loss = 0.1206\n",
      "Epoch 193: train loss = 0.0003, val loss = 0.1197\n",
      "Epoch 194: train loss = 0.0003, val loss = 0.1205\n",
      "Epoch 195: train loss = 0.0004, val loss = 0.1200\n",
      "Epoch 196: train loss = 0.0003, val loss = 0.1183\n",
      "Epoch 197: train loss = 0.0004, val loss = 0.1192\n",
      "Epoch 198: train loss = 0.0003, val loss = 0.1179\n",
      "Epoch 199: train loss = 0.0003, val loss = 0.1226\n",
      "Epoch 200: train loss = 0.0003, val loss = 0.1212\n",
      "Epoch 201: train loss = 0.0003, val loss = 0.1226\n",
      "Epoch 202: train loss = 0.0003, val loss = 0.1187\n",
      "Epoch 203: train loss = 0.0003, val loss = 0.1207\n",
      "Epoch 204: train loss = 0.0004, val loss = 0.1213\n",
      "Epoch 205: train loss = 0.0004, val loss = 0.1208\n",
      "Epoch 206: train loss = 0.0004, val loss = 0.1191\n",
      "Epoch 207: train loss = 0.0003, val loss = 0.1216\n",
      "Epoch 208: train loss = 0.0004, val loss = 0.1217\n",
      "Epoch 209: train loss = 0.0003, val loss = 0.1216\n",
      "Epoch 210: train loss = 0.0003, val loss = 0.1180\n",
      "Epoch 211: train loss = 0.0004, val loss = 0.1232\n",
      "Epoch 212: train loss = 0.0004, val loss = 0.1189\n",
      "Epoch 213: train loss = 0.0003, val loss = 0.1198\n",
      "Epoch 214: train loss = 0.0003, val loss = 0.1198\n",
      "Epoch 215: train loss = 0.0003, val loss = 0.1194\n",
      "Epoch 216: train loss = 0.0003, val loss = 0.1206\n",
      "Epoch 217: train loss = 0.0003, val loss = 0.1185\n",
      "Epoch 218: train loss = 0.0003, val loss = 0.1206\n",
      "Epoch 219: train loss = 0.0005, val loss = 0.1220\n",
      "Epoch 220: train loss = 0.0005, val loss = 0.1201\n",
      "Epoch 221: train loss = 0.0006, val loss = 0.1197\n",
      "Epoch 222: train loss = 0.0004, val loss = 0.1202\n",
      "Epoch 223: train loss = 0.0004, val loss = 0.1197\n",
      "Epoch 224: train loss = 0.0004, val loss = 0.1213\n",
      "Epoch 225: train loss = 0.0003, val loss = 0.1209\n",
      "Epoch 226: train loss = 0.0003, val loss = 0.1207\n",
      "Epoch 227: train loss = 0.0004, val loss = 0.1203\n",
      "Epoch 228: train loss = 0.0004, val loss = 0.1201\n",
      "Epoch 229: train loss = 0.0003, val loss = 0.1212\n",
      "Epoch 230: train loss = 0.0004, val loss = 0.1215\n",
      "Epoch 231: train loss = 0.0007, val loss = 0.1205\n",
      "Epoch 232: train loss = 0.0007, val loss = 0.1190\n",
      "Epoch 233: train loss = 0.0008, val loss = 0.1191\n",
      "Epoch 234: train loss = 0.0010, val loss = 0.1198\n",
      "Epoch 235: train loss = 0.0008, val loss = 0.1186\n",
      "Epoch 236: train loss = 0.0005, val loss = 0.1216\n",
      "Epoch 237: train loss = 0.0005, val loss = 0.1194\n",
      "Epoch 238: train loss = 0.0004, val loss = 0.1185\n",
      "Epoch 239: train loss = 0.0004, val loss = 0.1203\n",
      "Epoch 240: train loss = 0.0003, val loss = 0.1185\n",
      "Epoch 241: train loss = 0.0004, val loss = 0.1195\n",
      "Epoch 242: train loss = 0.0003, val loss = 0.1176\n",
      "Epoch 243: train loss = 0.0003, val loss = 0.1185\n",
      "Epoch 244: train loss = 0.0003, val loss = 0.1187\n",
      "Epoch 245: train loss = 0.0003, val loss = 0.1185\n",
      "Epoch 246: train loss = 0.0002, val loss = 0.1183\n",
      "Epoch 247: train loss = 0.0003, val loss = 0.1212\n",
      "Epoch 248: train loss = 0.0004, val loss = 0.1213\n",
      "Epoch 249: train loss = 0.0004, val loss = 0.1177\n",
      "Epoch 250: train loss = 0.0004, val loss = 0.1210\n",
      "Epoch 251: train loss = 0.0004, val loss = 0.1183\n",
      "Epoch 252: train loss = 0.0005, val loss = 0.1193\n",
      "Epoch 253: train loss = 0.0004, val loss = 0.1189\n",
      "Epoch 254: train loss = 0.0005, val loss = 0.1217\n",
      "Epoch 255: train loss = 0.0005, val loss = 0.1202\n",
      "Epoch 256: train loss = 0.0006, val loss = 0.1179\n",
      "Epoch 257: train loss = 0.0006, val loss = 0.1204\n",
      "Epoch 258: train loss = 0.0006, val loss = 0.1156\n",
      "Epoch 259: train loss = 0.0005, val loss = 0.1189\n",
      "Epoch 260: train loss = 0.0005, val loss = 0.1173\n",
      "Epoch 261: train loss = 0.0004, val loss = 0.1179\n",
      "Epoch 262: train loss = 0.0004, val loss = 0.1203\n",
      "Epoch 263: train loss = 0.0004, val loss = 0.1217\n",
      "Epoch 264: train loss = 0.0006, val loss = 0.1202\n",
      "Epoch 265: train loss = 0.0004, val loss = 0.1223\n",
      "Epoch 266: train loss = 0.0003, val loss = 0.1195\n",
      "Epoch 267: train loss = 0.0004, val loss = 0.1195\n",
      "Epoch 268: train loss = 0.0003, val loss = 0.1212\n",
      "Epoch 269: train loss = 0.0003, val loss = 0.1211\n",
      "Epoch 270: train loss = 0.0003, val loss = 0.1210\n",
      "Epoch 271: train loss = 0.0002, val loss = 0.1196\n",
      "Epoch 272: train loss = 0.0003, val loss = 0.1203\n",
      "Epoch 273: train loss = 0.0003, val loss = 0.1199\n",
      "Epoch 274: train loss = 0.0002, val loss = 0.1203\n",
      "Epoch 275: train loss = 0.0005, val loss = 0.1232\n",
      "Epoch 276: train loss = 0.0005, val loss = 0.1224\n",
      "Epoch 277: train loss = 0.0005, val loss = 0.1201\n",
      "Epoch 278: train loss = 0.0005, val loss = 0.1241\n",
      "Epoch 279: train loss = 0.0006, val loss = 0.1222\n",
      "Epoch 280: train loss = 0.0006, val loss = 0.1230\n",
      "Epoch 281: train loss = 0.0007, val loss = 0.1248\n",
      "Epoch 282: train loss = 0.0007, val loss = 0.1211\n",
      "Epoch 283: train loss = 0.0006, val loss = 0.1223\n",
      "Epoch 284: train loss = 0.0007, val loss = 0.1223\n",
      "Epoch 285: train loss = 0.0006, val loss = 0.1203\n",
      "Epoch 286: train loss = 0.0004, val loss = 0.1207\n",
      "Epoch 287: train loss = 0.0005, val loss = 0.1219\n",
      "Epoch 288: train loss = 0.0004, val loss = 0.1207\n",
      "Epoch 289: train loss = 0.0004, val loss = 0.1217\n",
      "Epoch 290: train loss = 0.0004, val loss = 0.1225\n",
      "Epoch 291: train loss = 0.0003, val loss = 0.1225\n",
      "Epoch 292: train loss = 0.0002, val loss = 0.1215\n",
      "Epoch 293: train loss = 0.0002, val loss = 0.1225\n",
      "Epoch 294: train loss = 0.0002, val loss = 0.1236\n",
      "Epoch 295: train loss = 0.0002, val loss = 0.1229\n",
      "Epoch 296: train loss = 0.0002, val loss = 0.1224\n",
      "Epoch 297: train loss = 0.0002, val loss = 0.1235\n",
      "Epoch 298: train loss = 0.0002, val loss = 0.1235\n",
      "Epoch 299: train loss = 0.0003, val loss = 0.1229\n",
      "Epoch 300: train loss = 0.0002, val loss = 0.1230\n",
      "Epoch 301: train loss = 0.0002, val loss = 0.1219\n",
      "Epoch 302: train loss = 0.0003, val loss = 0.1220\n",
      "Epoch 303: train loss = 0.0002, val loss = 0.1223\n",
      "Epoch 304: train loss = 0.0002, val loss = 0.1234\n",
      "Epoch 305: train loss = 0.0002, val loss = 0.1235\n",
      "Epoch 306: train loss = 0.0002, val loss = 0.1243\n",
      "Epoch 307: train loss = 0.0002, val loss = 0.1238\n",
      "Epoch 308: train loss = 0.0001, val loss = 0.1244\n",
      "Epoch 309: train loss = 0.0001, val loss = 0.1238\n",
      "Epoch 310: train loss = 0.0001, val loss = 0.1225\n",
      "Epoch 311: train loss = 0.0001, val loss = 0.1237\n",
      "Epoch 312: train loss = 0.0001, val loss = 0.1234\n",
      "Epoch 313: train loss = 0.0001, val loss = 0.1248\n",
      "Epoch 314: train loss = 0.0002, val loss = 0.1229\n",
      "Epoch 315: train loss = 0.0002, val loss = 0.1247\n",
      "Epoch 316: train loss = 0.0002, val loss = 0.1248\n",
      "Epoch 317: train loss = 0.0002, val loss = 0.1246\n",
      "Epoch 318: train loss = 0.0003, val loss = 0.1234\n",
      "Epoch 319: train loss = 0.0003, val loss = 0.1220\n",
      "Epoch 320: train loss = 0.0004, val loss = 0.1228\n",
      "Epoch 321: train loss = 0.0005, val loss = 0.1231\n",
      "Epoch 322: train loss = 0.0005, val loss = 0.1224\n",
      "Epoch 323: train loss = 0.0004, val loss = 0.1226\n",
      "Epoch 324: train loss = 0.0005, val loss = 0.1207\n",
      "Epoch 325: train loss = 0.0005, val loss = 0.1225\n",
      "Epoch 326: train loss = 0.0004, val loss = 0.1244\n",
      "Epoch 327: train loss = 0.0003, val loss = 0.1218\n",
      "Epoch 328: train loss = 0.0003, val loss = 0.1243\n",
      "Epoch 329: train loss = 0.0004, val loss = 0.1252\n",
      "Epoch 330: train loss = 0.0006, val loss = 0.1225\n",
      "Epoch 331: train loss = 0.0011, val loss = 0.1231\n",
      "Epoch 332: train loss = 0.0010, val loss = 0.1209\n",
      "Epoch 333: train loss = 0.0009, val loss = 0.1197\n",
      "Epoch 334: train loss = 0.0007, val loss = 0.1214\n",
      "Epoch 335: train loss = 0.0007, val loss = 0.1208\n",
      "Epoch 336: train loss = 0.0005, val loss = 0.1184\n",
      "Epoch 337: train loss = 0.0005, val loss = 0.1173\n",
      "Epoch 338: train loss = 0.0004, val loss = 0.1205\n",
      "Epoch 339: train loss = 0.0004, val loss = 0.1213\n",
      "Epoch 340: train loss = 0.0003, val loss = 0.1187\n",
      "Epoch 341: train loss = 0.0003, val loss = 0.1203\n",
      "Epoch 342: train loss = 0.0003, val loss = 0.1210\n",
      "Epoch 343: train loss = 0.0003, val loss = 0.1198\n",
      "Epoch 344: train loss = 0.0002, val loss = 0.1205\n",
      "Epoch 345: train loss = 0.0002, val loss = 0.1190\n",
      "Epoch 346: train loss = 0.0002, val loss = 0.1205\n",
      "Epoch 347: train loss = 0.0001, val loss = 0.1200\n",
      "Epoch 348: train loss = 0.0001, val loss = 0.1204\n",
      "Epoch 349: train loss = 0.0001, val loss = 0.1212\n",
      "Epoch 350: train loss = 0.0001, val loss = 0.1209\n",
      "Epoch 351: train loss = 0.0001, val loss = 0.1199\n",
      "Epoch 352: train loss = 0.0001, val loss = 0.1216\n",
      "Epoch 353: train loss = 0.0001, val loss = 0.1208\n",
      "Epoch 354: train loss = 0.0001, val loss = 0.1212\n",
      "Epoch 355: train loss = 0.0002, val loss = 0.1208\n",
      "Epoch 356: train loss = 0.0004, val loss = 0.1222\n",
      "Epoch 357: train loss = 0.0004, val loss = 0.1226\n",
      "Epoch 358: train loss = 0.0004, val loss = 0.1209\n",
      "Epoch 359: train loss = 0.0004, val loss = 0.1188\n",
      "Epoch 360: train loss = 0.0003, val loss = 0.1212\n",
      "Epoch 361: train loss = 0.0003, val loss = 0.1216\n",
      "Epoch 362: train loss = 0.0003, val loss = 0.1211\n",
      "Epoch 363: train loss = 0.0003, val loss = 0.1211\n",
      "Epoch 364: train loss = 0.0002, val loss = 0.1205\n",
      "Epoch 365: train loss = 0.0003, val loss = 0.1210\n",
      "Epoch 366: train loss = 0.0003, val loss = 0.1215\n",
      "Epoch 367: train loss = 0.0003, val loss = 0.1209\n",
      "Epoch 368: train loss = 0.0003, val loss = 0.1209\n",
      "Epoch 369: train loss = 0.0004, val loss = 0.1204\n",
      "Epoch 370: train loss = 0.0003, val loss = 0.1200\n",
      "Epoch 371: train loss = 0.0002, val loss = 0.1217\n",
      "Epoch 372: train loss = 0.0002, val loss = 0.1227\n",
      "Epoch 373: train loss = 0.0002, val loss = 0.1202\n",
      "Epoch 374: train loss = 0.0002, val loss = 0.1211\n",
      "Epoch 375: train loss = 0.0002, val loss = 0.1226\n",
      "Epoch 376: train loss = 0.0002, val loss = 0.1208\n",
      "Epoch 377: train loss = 0.0002, val loss = 0.1209\n",
      "Epoch 378: train loss = 0.0002, val loss = 0.1214\n",
      "Epoch 379: train loss = 0.0002, val loss = 0.1217\n",
      "Epoch 380: train loss = 0.0002, val loss = 0.1202\n",
      "Epoch 381: train loss = 0.0002, val loss = 0.1217\n",
      "Epoch 382: train loss = 0.0002, val loss = 0.1228\n",
      "Epoch 383: train loss = 0.0005, val loss = 0.1207\n",
      "Epoch 384: train loss = 0.0005, val loss = 0.1218\n",
      "Epoch 385: train loss = 0.0006, val loss = 0.1201\n",
      "Epoch 386: train loss = 0.0007, val loss = 0.1212\n",
      "Epoch 387: train loss = 0.0006, val loss = 0.1180\n",
      "Epoch 388: train loss = 0.0006, val loss = 0.1169\n",
      "Epoch 389: train loss = 0.0006, val loss = 0.1185\n",
      "Epoch 390: train loss = 0.0005, val loss = 0.1176\n",
      "Epoch 391: train loss = 0.0005, val loss = 0.1194\n",
      "Epoch 392: train loss = 0.0006, val loss = 0.1210\n",
      "Epoch 393: train loss = 0.0006, val loss = 0.1192\n",
      "Epoch 394: train loss = 0.0006, val loss = 0.1182\n",
      "Epoch 395: train loss = 0.0005, val loss = 0.1168\n",
      "Epoch 396: train loss = 0.0004, val loss = 0.1173\n",
      "Epoch 397: train loss = 0.0003, val loss = 0.1197\n",
      "Epoch 398: train loss = 0.0003, val loss = 0.1209\n",
      "Epoch 399: train loss = 0.0003, val loss = 0.1199\n",
      "Epoch 400: train loss = 0.0004, val loss = 0.1197\n",
      "Epoch 401: train loss = 0.0004, val loss = 0.1170\n",
      "Epoch 402: train loss = 0.0003, val loss = 0.1180\n",
      "Epoch 403: train loss = 0.0003, val loss = 0.1177\n",
      "Epoch 404: train loss = 0.0004, val loss = 0.1171\n",
      "Epoch 405: train loss = 0.0003, val loss = 0.1166\n",
      "Epoch 406: train loss = 0.0004, val loss = 0.1195\n",
      "Epoch 407: train loss = 0.0003, val loss = 0.1179\n",
      "Epoch 408: train loss = 0.0003, val loss = 0.1186\n",
      "Epoch 409: train loss = 0.0003, val loss = 0.1183\n",
      "Epoch 410: train loss = 0.0003, val loss = 0.1176\n",
      "Epoch 411: train loss = 0.0002, val loss = 0.1186\n",
      "Epoch 412: train loss = 0.0002, val loss = 0.1168\n",
      "Epoch 413: train loss = 0.0002, val loss = 0.1170\n",
      "Epoch 414: train loss = 0.0002, val loss = 0.1171\n",
      "Epoch 415: train loss = 0.0002, val loss = 0.1183\n",
      "Epoch 416: train loss = 0.0002, val loss = 0.1173\n",
      "Epoch 417: train loss = 0.0002, val loss = 0.1171\n",
      "Epoch 418: train loss = 0.0002, val loss = 0.1173\n",
      "Epoch 419: train loss = 0.0002, val loss = 0.1166\n",
      "Epoch 420: train loss = 0.0002, val loss = 0.1161\n",
      "Epoch 421: train loss = 0.0002, val loss = 0.1170\n",
      "Epoch 422: train loss = 0.0002, val loss = 0.1161\n",
      "Epoch 423: train loss = 0.0002, val loss = 0.1169\n",
      "Epoch 424: train loss = 0.0001, val loss = 0.1165\n",
      "Epoch 425: train loss = 0.0001, val loss = 0.1176\n",
      "Epoch 426: train loss = 0.0003, val loss = 0.1178\n",
      "Epoch 427: train loss = 0.0002, val loss = 0.1172\n",
      "Epoch 428: train loss = 0.0002, val loss = 0.1165\n",
      "Epoch 429: train loss = 0.0002, val loss = 0.1175\n",
      "Epoch 430: train loss = 0.0002, val loss = 0.1172\n",
      "Epoch 431: train loss = 0.0002, val loss = 0.1178\n",
      "Epoch 432: train loss = 0.0002, val loss = 0.1181\n",
      "Epoch 433: train loss = 0.0002, val loss = 0.1168\n",
      "Epoch 434: train loss = 0.0002, val loss = 0.1177\n",
      "Epoch 435: train loss = 0.0001, val loss = 0.1158\n",
      "Epoch 436: train loss = 0.0001, val loss = 0.1176\n",
      "Epoch 437: train loss = 0.0006, val loss = 0.1156\n",
      "Epoch 438: train loss = 0.0003, val loss = 0.1183\n",
      "Epoch 439: train loss = 0.0002, val loss = 0.1160\n",
      "Epoch 440: train loss = 0.0002, val loss = 0.1183\n",
      "Epoch 441: train loss = 0.0002, val loss = 0.1170\n",
      "Epoch 442: train loss = 0.0002, val loss = 0.1178\n",
      "Epoch 443: train loss = 0.0001, val loss = 0.1166\n",
      "Epoch 444: train loss = 0.0001, val loss = 0.1177\n",
      "Epoch 445: train loss = 0.0001, val loss = 0.1156\n",
      "Epoch 446: train loss = 0.0003, val loss = 0.1198\n",
      "Epoch 447: train loss = 0.0003, val loss = 0.1177\n",
      "Epoch 448: train loss = 0.0003, val loss = 0.1176\n",
      "Epoch 449: train loss = 0.0003, val loss = 0.1167\n",
      "Epoch 450: train loss = 0.0002, val loss = 0.1155\n",
      "Epoch 451: train loss = 0.0002, val loss = 0.1160\n",
      "Epoch 452: train loss = 0.0002, val loss = 0.1156\n",
      "Epoch 453: train loss = 0.0002, val loss = 0.1153\n",
      "Epoch 454: train loss = 0.0002, val loss = 0.1166\n",
      "Epoch 455: train loss = 0.0002, val loss = 0.1154\n",
      "Epoch 456: train loss = 0.0002, val loss = 0.1162\n",
      "Epoch 457: train loss = 0.0001, val loss = 0.1162\n",
      "Epoch 458: train loss = 0.0002, val loss = 0.1158\n",
      "Epoch 459: train loss = 0.0002, val loss = 0.1167\n",
      "Epoch 460: train loss = 0.0002, val loss = 0.1151\n",
      "Epoch 461: train loss = 0.0002, val loss = 0.1167\n",
      "Epoch 462: train loss = 0.0002, val loss = 0.1154\n",
      "Epoch 463: train loss = 0.0002, val loss = 0.1164\n",
      "Epoch 464: train loss = 0.0002, val loss = 0.1153\n",
      "Epoch 465: train loss = 0.0002, val loss = 0.1158\n",
      "Epoch 466: train loss = 0.0002, val loss = 0.1172\n",
      "Epoch 467: train loss = 0.0002, val loss = 0.1148\n",
      "Epoch 468: train loss = 0.0003, val loss = 0.1178\n",
      "Epoch 469: train loss = 0.0004, val loss = 0.1150\n",
      "Epoch 470: train loss = 0.0005, val loss = 0.1123\n",
      "Epoch 471: train loss = 0.0006, val loss = 0.1122\n",
      "Epoch 472: train loss = 0.0007, val loss = 0.1144\n",
      "Epoch 473: train loss = 0.0005, val loss = 0.1137\n",
      "Epoch 474: train loss = 0.0005, val loss = 0.1138\n",
      "Epoch 475: train loss = 0.0007, val loss = 0.1133\n",
      "Epoch 476: train loss = 0.0006, val loss = 0.1116\n",
      "Epoch 477: train loss = 0.0007, val loss = 0.1133\n",
      "Epoch 478: train loss = 0.0006, val loss = 0.1134\n",
      "Epoch 479: train loss = 0.0006, val loss = 0.1112\n",
      "Epoch 480: train loss = 0.0005, val loss = 0.1136\n",
      "Epoch 481: train loss = 0.0004, val loss = 0.1115\n",
      "Epoch 482: train loss = 0.0004, val loss = 0.1141\n",
      "Epoch 483: train loss = 0.0003, val loss = 0.1140\n",
      "Epoch 484: train loss = 0.0004, val loss = 0.1134\n",
      "Epoch 485: train loss = 0.0004, val loss = 0.1132\n",
      "Epoch 486: train loss = 0.0004, val loss = 0.1114\n",
      "Epoch 487: train loss = 0.0004, val loss = 0.1112\n",
      "Epoch 488: train loss = 0.0003, val loss = 0.1101\n",
      "Epoch 489: train loss = 0.0003, val loss = 0.1123\n",
      "Epoch 490: train loss = 0.0002, val loss = 0.1123\n",
      "Epoch 491: train loss = 0.0003, val loss = 0.1132\n",
      "Epoch 492: train loss = 0.0003, val loss = 0.1112\n",
      "Epoch 493: train loss = 0.0002, val loss = 0.1120\n",
      "Epoch 494: train loss = 0.0002, val loss = 0.1117\n",
      "Epoch 495: train loss = 0.0002, val loss = 0.1133\n",
      "Epoch 496: train loss = 0.0001, val loss = 0.1114\n",
      "Epoch 497: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 498: train loss = 0.0001, val loss = 0.1114\n",
      "Epoch 499: train loss = 0.0001, val loss = 0.1109\n",
      "Epoch 500: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 501: train loss = 0.0001, val loss = 0.1121\n",
      "Epoch 502: train loss = 0.0001, val loss = 0.1112\n",
      "Epoch 503: train loss = 0.0001, val loss = 0.1125\n",
      "Epoch 504: train loss = 0.0001, val loss = 0.1110\n",
      "Epoch 505: train loss = 0.0001, val loss = 0.1115\n",
      "Epoch 506: train loss = 0.0002, val loss = 0.1111\n",
      "Epoch 507: train loss = 0.0001, val loss = 0.1120\n",
      "Epoch 508: train loss = 0.0001, val loss = 0.1110\n",
      "Epoch 509: train loss = 0.0001, val loss = 0.1115\n",
      "Epoch 510: train loss = 0.0001, val loss = 0.1115\n",
      "Epoch 511: train loss = 0.0001, val loss = 0.1118\n",
      "Epoch 512: train loss = 0.0001, val loss = 0.1116\n",
      "Epoch 513: train loss = 0.0002, val loss = 0.1117\n",
      "Epoch 514: train loss = 0.0002, val loss = 0.1128\n",
      "Epoch 515: train loss = 0.0004, val loss = 0.1104\n",
      "Epoch 516: train loss = 0.0004, val loss = 0.1115\n",
      "Epoch 517: train loss = 0.0007, val loss = 0.1107\n",
      "Epoch 518: train loss = 0.0006, val loss = 0.1132\n",
      "Epoch 519: train loss = 0.0006, val loss = 0.1103\n",
      "Epoch 520: train loss = 0.0005, val loss = 0.1101\n",
      "Epoch 521: train loss = 0.0005, val loss = 0.1093\n",
      "Epoch 522: train loss = 0.0006, val loss = 0.1107\n",
      "Epoch 523: train loss = 0.0004, val loss = 0.1092\n",
      "Epoch 524: train loss = 0.0002, val loss = 0.1109\n",
      "Epoch 525: train loss = 0.0003, val loss = 0.1118\n",
      "Epoch 526: train loss = 0.0005, val loss = 0.1115\n",
      "Epoch 527: train loss = 0.0004, val loss = 0.1106\n",
      "Epoch 528: train loss = 0.0003, val loss = 0.1114\n",
      "Epoch 529: train loss = 0.0004, val loss = 0.1086\n",
      "Epoch 530: train loss = 0.0004, val loss = 0.1097\n",
      "Epoch 531: train loss = 0.0003, val loss = 0.1081\n",
      "Epoch 532: train loss = 0.0004, val loss = 0.1114\n",
      "Epoch 533: train loss = 0.0003, val loss = 0.1101\n",
      "Epoch 534: train loss = 0.0002, val loss = 0.1102\n",
      "Epoch 535: train loss = 0.0002, val loss = 0.1085\n",
      "Epoch 536: train loss = 0.0001, val loss = 0.1085\n",
      "Epoch 537: train loss = 0.0001, val loss = 0.1088\n",
      "Epoch 538: train loss = 0.0002, val loss = 0.1101\n",
      "Epoch 539: train loss = 0.0002, val loss = 0.1076\n",
      "Epoch 540: train loss = 0.0002, val loss = 0.1096\n",
      "Epoch 541: train loss = 0.0002, val loss = 0.1101\n",
      "Epoch 542: train loss = 0.0001, val loss = 0.1088\n",
      "Epoch 543: train loss = 0.0001, val loss = 0.1106\n",
      "Epoch 544: train loss = 0.0001, val loss = 0.1088\n",
      "Epoch 545: train loss = 0.0002, val loss = 0.1108\n",
      "Epoch 546: train loss = 0.0003, val loss = 0.1095\n",
      "Epoch 547: train loss = 0.0003, val loss = 0.1105\n",
      "Epoch 548: train loss = 0.0002, val loss = 0.1096\n",
      "Epoch 549: train loss = 0.0002, val loss = 0.1095\n",
      "Epoch 550: train loss = 0.0002, val loss = 0.1100\n",
      "Epoch 551: train loss = 0.0002, val loss = 0.1110\n",
      "Epoch 552: train loss = 0.0002, val loss = 0.1106\n",
      "Epoch 553: train loss = 0.0001, val loss = 0.1118\n",
      "Epoch 554: train loss = 0.0001, val loss = 0.1095\n",
      "Epoch 555: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 556: train loss = 0.0001, val loss = 0.1101\n",
      "Epoch 557: train loss = 0.0001, val loss = 0.1118\n",
      "Epoch 558: train loss = 0.0001, val loss = 0.1103\n",
      "Epoch 559: train loss = 0.0001, val loss = 0.1108\n",
      "Epoch 560: train loss = 0.0001, val loss = 0.1104\n",
      "Epoch 561: train loss = 0.0001, val loss = 0.1108\n",
      "Epoch 562: train loss = 0.0001, val loss = 0.1107\n",
      "Epoch 563: train loss = 0.0001, val loss = 0.1106\n",
      "Epoch 564: train loss = 0.0001, val loss = 0.1107\n",
      "Epoch 565: train loss = 0.0001, val loss = 0.1112\n",
      "Epoch 566: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 567: train loss = 0.0001, val loss = 0.1101\n",
      "Epoch 568: train loss = 0.0001, val loss = 0.1107\n",
      "Epoch 569: train loss = 0.0001, val loss = 0.1108\n",
      "Epoch 570: train loss = 0.0001, val loss = 0.1109\n",
      "Epoch 571: train loss = 0.0001, val loss = 0.1101\n",
      "Epoch 572: train loss = 0.0001, val loss = 0.1096\n",
      "Epoch 573: train loss = 0.0001, val loss = 0.1108\n",
      "Epoch 574: train loss = 0.0001, val loss = 0.1116\n",
      "Epoch 575: train loss = 0.0001, val loss = 0.1108\n",
      "Epoch 576: train loss = 0.0001, val loss = 0.1114\n",
      "Epoch 577: train loss = 0.0002, val loss = 0.1111\n",
      "Epoch 578: train loss = 0.0001, val loss = 0.1112\n",
      "Epoch 579: train loss = 0.0002, val loss = 0.1096\n",
      "Epoch 580: train loss = 0.0001, val loss = 0.1108\n",
      "Epoch 581: train loss = 0.0002, val loss = 0.1099\n",
      "Epoch 582: train loss = 0.0002, val loss = 0.1125\n",
      "Epoch 583: train loss = 0.0002, val loss = 0.1104\n",
      "Epoch 584: train loss = 0.0004, val loss = 0.1158\n",
      "Epoch 585: train loss = 0.0005, val loss = 0.1144\n",
      "Epoch 586: train loss = 0.0005, val loss = 0.1104\n",
      "Epoch 587: train loss = 0.0006, val loss = 0.1090\n",
      "Epoch 588: train loss = 0.0006, val loss = 0.1100\n",
      "Epoch 589: train loss = 0.0006, val loss = 0.1112\n",
      "Epoch 590: train loss = 0.0005, val loss = 0.1093\n",
      "Epoch 591: train loss = 0.0006, val loss = 0.1106\n",
      "Epoch 592: train loss = 0.0006, val loss = 0.1049\n",
      "Epoch 593: train loss = 0.0007, val loss = 0.1068\n",
      "Epoch 594: train loss = 0.0006, val loss = 0.1060\n",
      "Epoch 595: train loss = 0.0003, val loss = 0.1094\n",
      "Epoch 596: train loss = 0.0003, val loss = 0.1088\n",
      "Epoch 597: train loss = 0.0002, val loss = 0.1078\n",
      "Epoch 598: train loss = 0.0002, val loss = 0.1083\n",
      "Epoch 599: train loss = 0.0003, val loss = 0.1093\n",
      "Epoch 600: train loss = 0.0003, val loss = 0.1089\n",
      "Epoch 601: train loss = 0.0002, val loss = 0.1087\n",
      "Epoch 602: train loss = 0.0002, val loss = 0.1093\n",
      "Epoch 603: train loss = 0.0002, val loss = 0.1081\n",
      "Epoch 604: train loss = 0.0001, val loss = 0.1089\n",
      "Epoch 605: train loss = 0.0001, val loss = 0.1086\n",
      "Epoch 606: train loss = 0.0001, val loss = 0.1083\n",
      "Epoch 607: train loss = 0.0002, val loss = 0.1075\n",
      "Epoch 608: train loss = 0.0001, val loss = 0.1089\n",
      "Epoch 609: train loss = 0.0002, val loss = 0.1061\n",
      "Epoch 610: train loss = 0.0002, val loss = 0.1106\n",
      "Epoch 611: train loss = 0.0002, val loss = 0.1083\n",
      "Epoch 612: train loss = 0.0002, val loss = 0.1087\n",
      "Epoch 613: train loss = 0.0003, val loss = 0.1111\n",
      "Epoch 614: train loss = 0.0003, val loss = 0.1061\n",
      "Epoch 615: train loss = 0.0003, val loss = 0.1083\n",
      "Epoch 616: train loss = 0.0002, val loss = 0.1085\n",
      "Epoch 617: train loss = 0.0002, val loss = 0.1077\n",
      "Epoch 618: train loss = 0.0004, val loss = 0.1084\n",
      "Epoch 619: train loss = 0.0003, val loss = 0.1079\n",
      "Epoch 620: train loss = 0.0003, val loss = 0.1076\n",
      "Epoch 621: train loss = 0.0002, val loss = 0.1078\n",
      "Epoch 622: train loss = 0.0001, val loss = 0.1069\n",
      "Epoch 623: train loss = 0.0001, val loss = 0.1103\n",
      "Epoch 624: train loss = 0.0001, val loss = 0.1085\n",
      "Epoch 625: train loss = 0.0001, val loss = 0.1093\n",
      "Epoch 626: train loss = 0.0001, val loss = 0.1077\n",
      "Epoch 627: train loss = 0.0001, val loss = 0.1074\n",
      "Epoch 628: train loss = 0.0001, val loss = 0.1086\n",
      "Epoch 629: train loss = 0.0001, val loss = 0.1075\n",
      "Epoch 630: train loss = 0.0001, val loss = 0.1078\n",
      "Epoch 631: train loss = 0.0001, val loss = 0.1088\n",
      "Epoch 632: train loss = 0.0001, val loss = 0.1078\n",
      "Epoch 633: train loss = 0.0001, val loss = 0.1093\n",
      "Epoch 634: train loss = 0.0001, val loss = 0.1082\n",
      "Epoch 635: train loss = 0.0001, val loss = 0.1092\n",
      "Epoch 636: train loss = 0.0001, val loss = 0.1083\n",
      "Epoch 637: train loss = 0.0001, val loss = 0.1096\n",
      "Epoch 638: train loss = 0.0001, val loss = 0.1092\n",
      "Epoch 639: train loss = 0.0001, val loss = 0.1090\n",
      "Epoch 640: train loss = 0.0001, val loss = 0.1077\n",
      "Epoch 641: train loss = 0.0001, val loss = 0.1089\n",
      "Epoch 642: train loss = 0.0001, val loss = 0.1074\n",
      "Epoch 643: train loss = 0.0001, val loss = 0.1097\n",
      "Epoch 644: train loss = 0.0001, val loss = 0.1088\n",
      "Epoch 645: train loss = 0.0001, val loss = 0.1079\n",
      "Epoch 646: train loss = 0.0001, val loss = 0.1073\n",
      "Epoch 647: train loss = 0.0001, val loss = 0.1082\n",
      "Epoch 648: train loss = 0.0001, val loss = 0.1085\n",
      "Epoch 649: train loss = 0.0001, val loss = 0.1080\n",
      "Epoch 650: train loss = 0.0001, val loss = 0.1090\n",
      "Epoch 651: train loss = 0.0001, val loss = 0.1090\n",
      "Epoch 652: train loss = 0.0001, val loss = 0.1078\n",
      "Epoch 653: train loss = 0.0001, val loss = 0.1077\n",
      "Epoch 654: train loss = 0.0002, val loss = 0.1084\n",
      "Epoch 655: train loss = 0.0002, val loss = 0.1076\n",
      "Epoch 656: train loss = 0.0002, val loss = 0.1105\n",
      "Epoch 657: train loss = 0.0002, val loss = 0.1058\n",
      "Epoch 658: train loss = 0.0002, val loss = 0.1097\n",
      "Epoch 659: train loss = 0.0002, val loss = 0.1072\n",
      "Epoch 660: train loss = 0.0002, val loss = 0.1073\n",
      "Epoch 661: train loss = 0.0002, val loss = 0.1074\n",
      "Epoch 662: train loss = 0.0002, val loss = 0.1084\n",
      "Epoch 663: train loss = 0.0002, val loss = 0.1080\n",
      "Epoch 664: train loss = 0.0002, val loss = 0.1089\n",
      "Epoch 665: train loss = 0.0003, val loss = 0.1072\n",
      "Epoch 666: train loss = 0.0002, val loss = 0.1067\n",
      "Epoch 667: train loss = 0.0002, val loss = 0.1072\n",
      "Epoch 668: train loss = 0.0002, val loss = 0.1080\n",
      "Epoch 669: train loss = 0.0001, val loss = 0.1080\n",
      "Epoch 670: train loss = 0.0002, val loss = 0.1072\n",
      "Epoch 671: train loss = 0.0002, val loss = 0.1074\n",
      "Epoch 672: train loss = 0.0002, val loss = 0.1070\n",
      "Epoch 673: train loss = 0.0003, val loss = 0.1047\n",
      "Epoch 674: train loss = 0.0004, val loss = 0.1052\n",
      "Epoch 675: train loss = 0.0003, val loss = 0.1032\n",
      "Epoch 676: train loss = 0.0003, val loss = 0.1050\n",
      "Epoch 677: train loss = 0.0003, val loss = 0.1054\n",
      "Epoch 678: train loss = 0.0004, val loss = 0.1073\n",
      "Epoch 679: train loss = 0.0005, val loss = 0.1049\n",
      "Epoch 680: train loss = 0.0004, val loss = 0.1043\n",
      "Epoch 681: train loss = 0.0004, val loss = 0.1077\n",
      "Epoch 682: train loss = 0.0008, val loss = 0.1053\n",
      "Epoch 683: train loss = 0.0007, val loss = 0.1046\n",
      "Epoch 684: train loss = 0.0006, val loss = 0.1032\n",
      "Epoch 685: train loss = 0.0005, val loss = 0.1017\n",
      "Epoch 686: train loss = 0.0005, val loss = 0.1033\n",
      "Epoch 687: train loss = 0.0004, val loss = 0.1042\n",
      "Epoch 688: train loss = 0.0006, val loss = 0.1033\n",
      "Epoch 689: train loss = 0.0004, val loss = 0.1027\n",
      "Epoch 690: train loss = 0.0004, val loss = 0.1057\n",
      "Epoch 691: train loss = 0.0004, val loss = 0.1026\n",
      "Epoch 692: train loss = 0.0003, val loss = 0.1057\n",
      "Epoch 693: train loss = 0.0003, val loss = 0.1022\n",
      "Epoch 694: train loss = 0.0002, val loss = 0.1062\n",
      "Epoch 695: train loss = 0.0002, val loss = 0.1043\n",
      "Epoch 696: train loss = 0.0002, val loss = 0.1051\n",
      "Epoch 697: train loss = 0.0002, val loss = 0.1051\n",
      "Epoch 698: train loss = 0.0002, val loss = 0.1048\n",
      "Epoch 699: train loss = 0.0002, val loss = 0.1057\n",
      "Epoch 700: train loss = 0.0001, val loss = 0.1039\n",
      "Epoch 701: train loss = 0.0001, val loss = 0.1052\n",
      "Epoch 702: train loss = 0.0001, val loss = 0.1044\n",
      "Epoch 703: train loss = 0.0001, val loss = 0.1047\n",
      "Epoch 704: train loss = 0.0001, val loss = 0.1039\n",
      "Epoch 705: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 706: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 707: train loss = 0.0001, val loss = 0.1053\n",
      "Epoch 708: train loss = 0.0001, val loss = 0.1049\n",
      "Epoch 709: train loss = 0.0001, val loss = 0.1044\n",
      "Epoch 710: train loss = 0.0001, val loss = 0.1046\n",
      "Epoch 711: train loss = 0.0001, val loss = 0.1056\n",
      "Epoch 712: train loss = 0.0001, val loss = 0.1043\n",
      "Epoch 713: train loss = 0.0001, val loss = 0.1038\n",
      "Epoch 714: train loss = 0.0001, val loss = 0.1047\n",
      "Epoch 715: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 716: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 717: train loss = 0.0001, val loss = 0.1044\n",
      "Epoch 718: train loss = 0.0001, val loss = 0.1051\n",
      "Epoch 719: train loss = 0.0000, val loss = 0.1043\n",
      "Epoch 720: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 721: train loss = 0.0000, val loss = 0.1045\n",
      "Epoch 722: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 723: train loss = 0.0001, val loss = 0.1045\n",
      "Epoch 724: train loss = 0.0001, val loss = 0.1045\n",
      "Epoch 725: train loss = 0.0001, val loss = 0.1048\n",
      "Epoch 726: train loss = 0.0001, val loss = 0.1046\n",
      "Epoch 727: train loss = 0.0001, val loss = 0.1053\n",
      "Epoch 728: train loss = 0.0001, val loss = 0.1048\n",
      "Epoch 729: train loss = 0.0001, val loss = 0.1047\n",
      "Epoch 730: train loss = 0.0001, val loss = 0.1046\n",
      "Epoch 731: train loss = 0.0001, val loss = 0.1049\n",
      "Epoch 732: train loss = 0.0001, val loss = 0.1048\n",
      "Epoch 733: train loss = 0.0001, val loss = 0.1039\n",
      "Epoch 734: train loss = 0.0002, val loss = 0.1048\n",
      "Epoch 735: train loss = 0.0002, val loss = 0.1044\n",
      "Epoch 736: train loss = 0.0002, val loss = 0.1041\n",
      "Epoch 737: train loss = 0.0002, val loss = 0.1047\n",
      "Epoch 738: train loss = 0.0002, val loss = 0.1045\n",
      "Epoch 739: train loss = 0.0002, val loss = 0.1043\n",
      "Epoch 740: train loss = 0.0002, val loss = 0.1040\n",
      "Epoch 741: train loss = 0.0002, val loss = 0.1031\n",
      "Epoch 742: train loss = 0.0004, val loss = 0.1040\n",
      "Epoch 743: train loss = 0.0007, val loss = 0.1031\n",
      "Epoch 744: train loss = 0.0006, val loss = 0.1020\n",
      "Epoch 745: train loss = 0.0005, val loss = 0.1038\n",
      "Epoch 746: train loss = 0.0004, val loss = 0.1037\n",
      "Epoch 747: train loss = 0.0004, val loss = 0.1011\n",
      "Epoch 748: train loss = 0.0003, val loss = 0.0991\n",
      "Epoch 749: train loss = 0.0003, val loss = 0.1020\n",
      "Epoch 750: train loss = 0.0006, val loss = 0.1024\n",
      "Epoch 751: train loss = 0.0005, val loss = 0.1037\n",
      "Epoch 752: train loss = 0.0006, val loss = 0.1018\n",
      "Epoch 753: train loss = 0.0004, val loss = 0.1020\n",
      "Epoch 754: train loss = 0.0003, val loss = 0.1021\n",
      "Epoch 755: train loss = 0.0003, val loss = 0.1029\n",
      "Epoch 756: train loss = 0.0004, val loss = 0.1050\n",
      "Epoch 757: train loss = 0.0003, val loss = 0.1043\n",
      "Epoch 758: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 759: train loss = 0.0002, val loss = 0.1034\n",
      "Epoch 760: train loss = 0.0002, val loss = 0.1031\n",
      "Epoch 761: train loss = 0.0001, val loss = 0.1020\n",
      "Epoch 762: train loss = 0.0001, val loss = 0.1046\n",
      "Epoch 763: train loss = 0.0001, val loss = 0.1036\n",
      "Epoch 764: train loss = 0.0001, val loss = 0.1043\n",
      "Epoch 765: train loss = 0.0001, val loss = 0.1036\n",
      "Epoch 766: train loss = 0.0001, val loss = 0.1038\n",
      "Epoch 767: train loss = 0.0001, val loss = 0.1033\n",
      "Epoch 768: train loss = 0.0001, val loss = 0.1034\n",
      "Epoch 769: train loss = 0.0001, val loss = 0.1035\n",
      "Epoch 770: train loss = 0.0001, val loss = 0.1052\n",
      "Epoch 771: train loss = 0.0001, val loss = 0.1028\n",
      "Epoch 772: train loss = 0.0001, val loss = 0.1043\n",
      "Epoch 773: train loss = 0.0002, val loss = 0.1039\n",
      "Epoch 774: train loss = 0.0002, val loss = 0.1045\n",
      "Epoch 775: train loss = 0.0001, val loss = 0.1035\n",
      "Epoch 776: train loss = 0.0001, val loss = 0.1035\n",
      "Epoch 777: train loss = 0.0001, val loss = 0.1045\n",
      "Epoch 778: train loss = 0.0001, val loss = 0.1051\n",
      "Epoch 779: train loss = 0.0001, val loss = 0.1036\n",
      "Epoch 780: train loss = 0.0001, val loss = 0.1045\n",
      "Epoch 781: train loss = 0.0000, val loss = 0.1051\n",
      "Epoch 782: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 783: train loss = 0.0001, val loss = 0.1057\n",
      "Epoch 784: train loss = 0.0001, val loss = 0.1040\n",
      "Epoch 785: train loss = 0.0001, val loss = 0.1037\n",
      "Epoch 786: train loss = 0.0001, val loss = 0.1036\n",
      "Epoch 787: train loss = 0.0003, val loss = 0.1043\n",
      "Epoch 788: train loss = 0.0002, val loss = 0.1043\n",
      "Epoch 789: train loss = 0.0003, val loss = 0.1028\n",
      "Epoch 790: train loss = 0.0002, val loss = 0.1028\n",
      "Epoch 791: train loss = 0.0002, val loss = 0.1039\n",
      "Epoch 792: train loss = 0.0002, val loss = 0.1029\n",
      "Epoch 793: train loss = 0.0002, val loss = 0.1024\n",
      "Epoch 794: train loss = 0.0002, val loss = 0.1029\n",
      "Epoch 795: train loss = 0.0001, val loss = 0.1040\n",
      "Epoch 796: train loss = 0.0001, val loss = 0.1035\n",
      "Epoch 797: train loss = 0.0001, val loss = 0.1013\n",
      "Epoch 798: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 799: train loss = 0.0001, val loss = 0.1029\n",
      "Epoch 800: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 801: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 802: train loss = 0.0000, val loss = 0.1033\n",
      "Epoch 803: train loss = 0.0001, val loss = 0.1044\n",
      "Epoch 804: train loss = 0.0001, val loss = 0.1028\n",
      "Epoch 805: train loss = 0.0000, val loss = 0.1038\n",
      "Epoch 806: train loss = 0.0000, val loss = 0.1026\n",
      "Epoch 807: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 808: train loss = 0.0001, val loss = 0.1037\n",
      "Epoch 809: train loss = 0.0001, val loss = 0.1033\n",
      "Epoch 810: train loss = 0.0001, val loss = 0.1044\n",
      "Epoch 811: train loss = 0.0001, val loss = 0.1034\n",
      "Epoch 812: train loss = 0.0001, val loss = 0.1054\n",
      "Epoch 813: train loss = 0.0001, val loss = 0.1025\n",
      "Epoch 814: train loss = 0.0001, val loss = 0.1040\n",
      "Epoch 815: train loss = 0.0001, val loss = 0.1014\n",
      "Epoch 816: train loss = 0.0001, val loss = 0.1039\n",
      "Epoch 817: train loss = 0.0001, val loss = 0.1032\n",
      "Epoch 818: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 819: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 820: train loss = 0.0001, val loss = 0.1033\n",
      "Epoch 821: train loss = 0.0002, val loss = 0.1043\n",
      "Epoch 822: train loss = 0.0002, val loss = 0.1035\n",
      "Epoch 823: train loss = 0.0004, val loss = 0.1017\n",
      "Epoch 824: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 825: train loss = 0.0004, val loss = 0.1027\n",
      "Epoch 826: train loss = 0.0003, val loss = 0.1022\n",
      "Epoch 827: train loss = 0.0003, val loss = 0.1021\n",
      "Epoch 828: train loss = 0.0003, val loss = 0.1001\n",
      "Epoch 829: train loss = 0.0002, val loss = 0.1046\n",
      "Epoch 830: train loss = 0.0004, val loss = 0.0994\n",
      "Epoch 831: train loss = 0.0003, val loss = 0.1023\n",
      "Epoch 832: train loss = 0.0003, val loss = 0.1002\n",
      "Epoch 833: train loss = 0.0002, val loss = 0.1014\n",
      "Epoch 834: train loss = 0.0002, val loss = 0.1005\n",
      "Epoch 835: train loss = 0.0002, val loss = 0.1032\n",
      "Epoch 836: train loss = 0.0002, val loss = 0.1005\n",
      "Epoch 837: train loss = 0.0002, val loss = 0.1028\n",
      "Epoch 838: train loss = 0.0001, val loss = 0.1005\n",
      "Epoch 839: train loss = 0.0001, val loss = 0.1006\n",
      "Epoch 840: train loss = 0.0001, val loss = 0.1020\n",
      "Epoch 841: train loss = 0.0001, val loss = 0.1012\n",
      "Epoch 842: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 843: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 844: train loss = 0.0002, val loss = 0.1015\n",
      "Epoch 845: train loss = 0.0001, val loss = 0.1021\n",
      "Epoch 846: train loss = 0.0001, val loss = 0.1013\n",
      "Epoch 847: train loss = 0.0001, val loss = 0.1017\n",
      "Epoch 848: train loss = 0.0001, val loss = 0.1015\n",
      "Epoch 849: train loss = 0.0001, val loss = 0.1017\n",
      "Epoch 850: train loss = 0.0001, val loss = 0.1015\n",
      "Epoch 851: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 852: train loss = 0.0001, val loss = 0.1020\n",
      "Epoch 853: train loss = 0.0001, val loss = 0.1013\n",
      "Epoch 854: train loss = 0.0001, val loss = 0.1012\n",
      "Epoch 855: train loss = 0.0001, val loss = 0.1024\n",
      "Epoch 856: train loss = 0.0001, val loss = 0.1014\n",
      "Epoch 857: train loss = 0.0001, val loss = 0.1010\n",
      "Epoch 858: train loss = 0.0001, val loss = 0.1014\n",
      "Epoch 859: train loss = 0.0001, val loss = 0.1017\n",
      "Epoch 860: train loss = 0.0002, val loss = 0.1014\n",
      "Epoch 861: train loss = 0.0003, val loss = 0.1020\n",
      "Epoch 862: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 863: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 864: train loss = 0.0002, val loss = 0.1013\n",
      "Epoch 865: train loss = 0.0002, val loss = 0.1036\n",
      "Epoch 866: train loss = 0.0010, val loss = 0.0977\n",
      "Epoch 867: train loss = 0.0013, val loss = 0.1004\n",
      "Epoch 868: train loss = 0.0010, val loss = 0.0982\n",
      "Epoch 869: train loss = 0.0006, val loss = 0.0980\n",
      "Epoch 870: train loss = 0.0005, val loss = 0.0989\n",
      "Epoch 871: train loss = 0.0004, val loss = 0.1000\n",
      "Epoch 872: train loss = 0.0002, val loss = 0.0989\n",
      "Epoch 873: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 874: train loss = 0.0002, val loss = 0.0988\n",
      "Epoch 875: train loss = 0.0002, val loss = 0.0988\n",
      "Epoch 876: train loss = 0.0001, val loss = 0.0985\n",
      "Epoch 877: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 878: train loss = 0.0001, val loss = 0.0995\n",
      "Epoch 879: train loss = 0.0001, val loss = 0.0990\n",
      "Epoch 880: train loss = 0.0001, val loss = 0.0999\n",
      "Epoch 881: train loss = 0.0001, val loss = 0.0984\n",
      "Epoch 882: train loss = 0.0001, val loss = 0.0993\n",
      "Epoch 883: train loss = 0.0000, val loss = 0.0991\n",
      "Epoch 884: train loss = 0.0000, val loss = 0.0992\n",
      "Epoch 885: train loss = 0.0000, val loss = 0.0996\n",
      "Epoch 886: train loss = 0.0000, val loss = 0.0992\n",
      "Epoch 887: train loss = 0.0000, val loss = 0.0996\n",
      "Epoch 888: train loss = 0.0000, val loss = 0.0993\n",
      "Epoch 889: train loss = 0.0000, val loss = 0.0996\n",
      "Epoch 890: train loss = 0.0000, val loss = 0.0996\n",
      "Epoch 891: train loss = 0.0000, val loss = 0.0997\n",
      "Epoch 892: train loss = 0.0000, val loss = 0.0993\n",
      "Epoch 893: train loss = 0.0000, val loss = 0.0997\n",
      "Epoch 894: train loss = 0.0000, val loss = 0.0997\n",
      "Epoch 895: train loss = 0.0000, val loss = 0.0996\n",
      "Epoch 896: train loss = 0.0007, val loss = 0.1000\n",
      "Epoch 897: train loss = 0.0005, val loss = 0.0993\n",
      "Epoch 898: train loss = 0.0003, val loss = 0.0999\n",
      "Epoch 899: train loss = 0.0003, val loss = 0.0985\n",
      "Epoch 900: train loss = 0.0003, val loss = 0.0986\n",
      "Epoch 901: train loss = 0.0002, val loss = 0.0994\n",
      "Epoch 902: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 903: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 904: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 905: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 906: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 907: train loss = 0.0002, val loss = 0.1019\n",
      "Epoch 908: train loss = 0.0002, val loss = 0.1000\n",
      "Epoch 909: train loss = 0.0002, val loss = 0.0989\n",
      "Epoch 910: train loss = 0.0001, val loss = 0.0990\n",
      "Epoch 911: train loss = 0.0001, val loss = 0.1000\n",
      "Epoch 912: train loss = 0.0001, val loss = 0.0998\n",
      "Epoch 913: train loss = 0.0001, val loss = 0.0999\n",
      "Epoch 914: train loss = 0.0001, val loss = 0.0997\n",
      "Epoch 915: train loss = 0.0001, val loss = 0.1006\n",
      "Epoch 916: train loss = 0.0001, val loss = 0.0992\n",
      "Epoch 917: train loss = 0.0001, val loss = 0.1013\n",
      "Epoch 918: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 919: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 920: train loss = 0.0001, val loss = 0.1006\n",
      "Epoch 921: train loss = 0.0001, val loss = 0.1005\n",
      "Epoch 922: train loss = 0.0001, val loss = 0.1000\n",
      "Epoch 923: train loss = 0.0001, val loss = 0.1012\n",
      "Epoch 924: train loss = 0.0001, val loss = 0.0996\n",
      "Epoch 925: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 926: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 927: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 928: train loss = 0.0000, val loss = 0.1006\n",
      "Epoch 929: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 930: train loss = 0.0000, val loss = 0.1013\n",
      "Epoch 931: train loss = 0.0000, val loss = 0.1001\n",
      "Epoch 932: train loss = 0.0000, val loss = 0.1006\n",
      "Epoch 933: train loss = 0.0000, val loss = 0.0998\n",
      "Epoch 934: train loss = 0.0000, val loss = 0.0997\n",
      "Epoch 935: train loss = 0.0000, val loss = 0.1006\n",
      "Epoch 936: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 937: train loss = 0.0000, val loss = 0.1000\n",
      "Epoch 938: train loss = 0.0001, val loss = 0.0994\n",
      "Epoch 939: train loss = 0.0001, val loss = 0.0991\n",
      "Epoch 940: train loss = 0.0001, val loss = 0.0999\n",
      "Epoch 941: train loss = 0.0001, val loss = 0.1003\n",
      "Epoch 942: train loss = 0.0001, val loss = 0.0996\n",
      "Epoch 943: train loss = 0.0001, val loss = 0.1003\n",
      "Epoch 944: train loss = 0.0001, val loss = 0.1003\n",
      "Epoch 945: train loss = 0.0001, val loss = 0.1003\n",
      "Epoch 946: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 947: train loss = 0.0002, val loss = 0.1014\n",
      "Epoch 948: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 949: train loss = 0.0002, val loss = 0.1005\n",
      "Epoch 950: train loss = 0.0002, val loss = 0.0987\n",
      "Epoch 951: train loss = 0.0002, val loss = 0.1006\n",
      "Epoch 952: train loss = 0.0002, val loss = 0.0994\n",
      "Epoch 953: train loss = 0.0002, val loss = 0.1018\n",
      "Epoch 954: train loss = 0.0002, val loss = 0.0994\n",
      "Epoch 955: train loss = 0.0002, val loss = 0.0999\n",
      "Epoch 956: train loss = 0.0002, val loss = 0.0981\n",
      "Epoch 957: train loss = 0.0003, val loss = 0.0993\n",
      "Epoch 958: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 959: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 960: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 961: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 962: train loss = 0.0001, val loss = 0.0987\n",
      "Epoch 963: train loss = 0.0002, val loss = 0.1000\n",
      "Epoch 964: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 965: train loss = 0.0002, val loss = 0.0981\n",
      "Epoch 966: train loss = 0.0004, val loss = 0.0985\n",
      "Epoch 967: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 968: train loss = 0.0003, val loss = 0.1006\n",
      "Epoch 969: train loss = 0.0003, val loss = 0.1027\n",
      "Epoch 970: train loss = 0.0003, val loss = 0.1006\n",
      "Epoch 971: train loss = 0.0003, val loss = 0.1032\n",
      "Epoch 972: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 973: train loss = 0.0003, val loss = 0.0978\n",
      "Epoch 974: train loss = 0.0003, val loss = 0.0970\n",
      "Epoch 975: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 976: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 977: train loss = 0.0003, val loss = 0.0987\n",
      "Epoch 978: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 979: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 980: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 981: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 982: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 983: train loss = 0.0001, val loss = 0.0996\n",
      "Epoch 984: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 985: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 986: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 987: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 988: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 989: train loss = 0.0001, val loss = 0.0988\n",
      "Epoch 990: train loss = 0.0001, val loss = 0.0991\n",
      "Epoch 991: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 992: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 993: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 994: train loss = 0.0001, val loss = 0.0985\n",
      "Epoch 995: train loss = 0.0000, val loss = 0.0982\n",
      "Epoch 996: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 997: train loss = 0.0000, val loss = 0.0980\n",
      "Epoch 998: train loss = 0.0000, val loss = 0.0983\n",
      "Epoch 999: train loss = 0.0000, val loss = 0.0974\n",
      "Epoch 1000: train loss = 0.0000, val loss = 0.0979\n"
     ]
    }
   ],
   "source": [
    "model = train_final_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0c8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mood_predictions(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83af6f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1914\n"
     ]
    }
   ],
   "source": [
    "from mood_RNN_classifier import get_accuracy_rate\n",
    "accuracy = get_accuracy_rate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a1aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  predicted_mood_class\n",
      "0   AS14.01                    15\n",
      "1   AS14.02                    28\n",
      "2   AS14.03                    33\n",
      "3   AS14.05                     0\n",
      "4   AS14.06                     2\n",
      "5   AS14.07                    34\n",
      "6   AS14.08                    24\n",
      "7   AS14.09                    33\n",
      "8   AS14.12                    22\n",
      "9   AS14.13                    33\n",
      "10  AS14.14                    34\n",
      "11  AS14.15                    33\n",
      "12  AS14.16                    23\n",
      "13  AS14.17                    22\n",
      "14  AS14.19                    34\n",
      "15  AS14.20                    33\n",
      "16  AS14.23                     2\n",
      "17  AS14.24                    34\n",
      "18  AS14.25                    22\n",
      "19  AS14.26                    31\n",
      "20  AS14.27                    33\n",
      "21  AS14.28                    28\n",
      "22  AS14.29                    31\n",
      "23  AS14.30                    33\n",
      "24  AS14.31                    15\n",
      "25  AS14.32                    19\n",
      "26  AS14.33                    33\n"
     ]
    }
   ],
   "source": [
    "# Run predictions on test_df\n",
    "test_predictions = predict(model, test_df, id_map, device)\n",
    "\n",
    "# Attach predictions to test_df\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted_mood_class'] = test_predictions\n",
    "\n",
    "# Optional: save to CSV or examine\n",
    "print(test_df_with_preds[['id', 'predicted_mood_class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3933caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "        31, 32, 33, 34, 35, 36, 38], dtype=int32),\n",
       " mood\n",
       " 28    236\n",
       " 29    108\n",
       " 30    101\n",
       " 32     97\n",
       " 26     92\n",
       " 31     78\n",
       " 27     77\n",
       " 25     54\n",
       " 24     52\n",
       " 33     17\n",
       " 34     11\n",
       " 22     10\n",
       " 23      8\n",
       " 35      5\n",
       " 21      5\n",
       " 20      3\n",
       " 18      3\n",
       " 14      2\n",
       " 19      2\n",
       " 36      2\n",
       " 15      2\n",
       " 12      1\n",
       " 16      1\n",
       " 38      1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_split = train_df_split.sort_values('mood')\n",
    "train_df_split['mood'].unique(), train_df_split['mood'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
