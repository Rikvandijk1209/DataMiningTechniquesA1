{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60ca131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Path (/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1) already exists in sys.path\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ./initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf5c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from data_loading import DataPreprocessor\n",
    "from mood_RNN_classifier import RNNClassifier, MoodDataset, OrdinalLabelSmoothingLoss, objective, train_epoch, train_final_model, evaluate, predict, plot_mood_predictions\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c42a51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split_train_val() got multiple values for argument 'fraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data_loader = DataPreprocessor()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df_split, val_df_split, test_df = \u001b[43mdata_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:26\u001b[39m, in \u001b[36mload_and_preprocess_data\u001b[39m\u001b[34m(self, interval, bucket_step, technique)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: split_train_val() got multiple values for argument 'fraction'"
     ]
    }
   ],
   "source": [
    "data_loader = DataPreprocessor()\n",
    "train_df_split, val_df_split, test_df = data_loader.load_and_preprocess_data(\"1d\", 0.25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6928e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume train_df and test_df are loaded and preprocessed\n",
    "id_map = {id_: idx for idx, id_ in enumerate(train_df_split['id'].unique())}\n",
    "input_dim = train_df_split.drop(columns=['id', 'mood', 'date']).shape[1]\n",
    "id_count = len(id_map)\n",
    "output_dim = train_df_split['mood'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c89292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 20:37:04,691] A new study created in memory with name: no-name-85b43180-378f-4c0e-b075-dc9d4924e46a\n",
      "[I 2025-04-17 20:37:09,868] Trial 0 finished with value: 0.22316743433475494 and parameters: {'hidden_dim': 63, 'id_embed_dim': 5, 'lr': 0.0049639865353721425, 'batch_size': 64, 'alpha': 0.04456595327275025}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:09,994] Trial 1 finished with value: 0.5566116571426392 and parameters: {'hidden_dim': 57, 'id_embed_dim': 16, 'lr': 0.0016419874840177464, 'batch_size': 128, 'alpha': 0.10412491869201401}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,227] Trial 2 finished with value: 0.4580661691725254 and parameters: {'hidden_dim': 36, 'id_embed_dim': 8, 'lr': 0.004214208282860192, 'batch_size': 32, 'alpha': 0.1144483469299451}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,460] Trial 3 finished with value: 1.4468481838703156 and parameters: {'hidden_dim': 48, 'id_embed_dim': 15, 'lr': 0.00012847477898188172, 'batch_size': 32, 'alpha': 0.15952204213141782}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,639] Trial 4 finished with value: 0.559240072965622 and parameters: {'hidden_dim': 109, 'id_embed_dim': 11, 'lr': 0.00285847636000973, 'batch_size': 64, 'alpha': 0.14133919374307705}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,900] Trial 5 finished with value: 0.3959162849932909 and parameters: {'hidden_dim': 94, 'id_embed_dim': 9, 'lr': 0.000959004775064577, 'batch_size': 32, 'alpha': 0.07691987547508385}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,154] Trial 6 finished with value: 0.5475961491465569 and parameters: {'hidden_dim': 90, 'id_embed_dim': 10, 'lr': 0.0009305727342693942, 'batch_size': 32, 'alpha': 0.1294617525351189}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,278] Trial 7 finished with value: 1.0174967050552368 and parameters: {'hidden_dim': 54, 'id_embed_dim': 12, 'lr': 0.0015246137802307094, 'batch_size': 128, 'alpha': 0.27467109750960694}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,518] Trial 8 finished with value: 0.3998469691723585 and parameters: {'hidden_dim': 57, 'id_embed_dim': 13, 'lr': 0.008393230184686289, 'batch_size': 32, 'alpha': 0.07733352254597115}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,768] Trial 9 finished with value: 1.049925409257412 and parameters: {'hidden_dim': 96, 'id_embed_dim': 15, 'lr': 0.0002470537425886334, 'batch_size': 32, 'alpha': 0.19342570626978045}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,952] Trial 10 finished with value: 0.1018451415002346 and parameters: {'hidden_dim': 125, 'id_embed_dim': 4, 'lr': 0.009116117543999267, 'batch_size': 64, 'alpha': 0.017935477351516377}. Best is trial 10 with value: 0.1018451415002346.\n",
      "[I 2025-04-17 20:37:12,134] Trial 11 finished with value: 0.06147644203156233 and parameters: {'hidden_dim': 128, 'id_embed_dim': 4, 'lr': 0.008759792413482445, 'batch_size': 64, 'alpha': 0.01070053856604862}. Best is trial 11 with value: 0.06147644203156233.\n",
      "[I 2025-04-17 20:37:12,323] Trial 12 finished with value: 0.05555207468569279 and parameters: {'hidden_dim': 126, 'id_embed_dim': 4, 'lr': 0.009723243485482763, 'batch_size': 64, 'alpha': 0.01039165986937}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:12,512] Trial 13 finished with value: 0.3668849319219589 and parameters: {'hidden_dim': 126, 'id_embed_dim': 6, 'lr': 0.00038319311099752816, 'batch_size': 64, 'alpha': 0.01803616878395828}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:12,690] Trial 14 finished with value: 0.6974070072174072 and parameters: {'hidden_dim': 112, 'id_embed_dim': 7, 'lr': 0.005700538186922354, 'batch_size': 64, 'alpha': 0.2341684716924466}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:12,870] Trial 15 finished with value: 0.324625700712204 and parameters: {'hidden_dim': 112, 'id_embed_dim': 4, 'lr': 0.0025488009305596, 'batch_size': 64, 'alpha': 0.06251628951415193}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:13,050] Trial 16 finished with value: 0.05536685138940811 and parameters: {'hidden_dim': 79, 'id_embed_dim': 6, 'lr': 0.009930284719147366, 'batch_size': 64, 'alpha': 0.010955559852850221}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,226] Trial 17 finished with value: 0.4988250583410263 and parameters: {'hidden_dim': 74, 'id_embed_dim': 6, 'lr': 0.0006252286941811653, 'batch_size': 64, 'alpha': 0.04388138393101973}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,356] Trial 18 finished with value: 0.6931120455265045 and parameters: {'hidden_dim': 76, 'id_embed_dim': 7, 'lr': 0.0029314477938574843, 'batch_size': 128, 'alpha': 0.1921392448052608}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,534] Trial 19 finished with value: 0.420554481446743 and parameters: {'hidden_dim': 84, 'id_embed_dim': 6, 'lr': 0.0057387334553696875, 'batch_size': 64, 'alpha': 0.09427768177308873}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,714] Trial 20 finished with value: 0.26087402552366257 and parameters: {'hidden_dim': 102, 'id_embed_dim': 8, 'lr': 0.001988994116096351, 'batch_size': 64, 'alpha': 0.044878590958239456}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,898] Trial 21 finished with value: 0.06413324177265167 and parameters: {'hidden_dim': 119, 'id_embed_dim': 4, 'lr': 0.0073368607440338724, 'batch_size': 64, 'alpha': 0.011846065487222753}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,070] Trial 22 finished with value: 0.05565113667398691 and parameters: {'hidden_dim': 70, 'id_embed_dim': 5, 'lr': 0.00997520346590688, 'batch_size': 64, 'alpha': 0.010158583398398013}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,243] Trial 23 finished with value: 0.2660074643790722 and parameters: {'hidden_dim': 68, 'id_embed_dim': 5, 'lr': 0.004197223125338242, 'batch_size': 64, 'alpha': 0.04342911575304767}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,423] Trial 24 finished with value: 0.3306473195552826 and parameters: {'hidden_dim': 85, 'id_embed_dim': 5, 'lr': 0.0097167946819779, 'batch_size': 64, 'alpha': 0.07071447202375256}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,560] Trial 25 finished with value: 0.21617744117975235 and parameters: {'hidden_dim': 68, 'id_embed_dim': 7, 'lr': 0.003912400210109566, 'batch_size': 128, 'alpha': 0.03317413910782839}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,749] Trial 26 finished with value: 0.3042144775390625 and parameters: {'hidden_dim': 80, 'id_embed_dim': 5, 'lr': 0.006227090453544527, 'batch_size': 64, 'alpha': 0.06127471442641197}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,923] Trial 27 finished with value: 0.16927653178572655 and parameters: {'hidden_dim': 39, 'id_embed_dim': 9, 'lr': 0.006649160220888407, 'batch_size': 64, 'alpha': 0.032283342237739834}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,116] Trial 28 finished with value: 0.6607734262943268 and parameters: {'hidden_dim': 102, 'id_embed_dim': 6, 'lr': 0.0035005476099699917, 'batch_size': 64, 'alpha': 0.17658555527897957}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,300] Trial 29 finished with value: 0.8906346559524536 and parameters: {'hidden_dim': 67, 'id_embed_dim': 8, 'lr': 0.005008206668819342, 'batch_size': 64, 'alpha': 0.2976309885174498}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,435] Trial 30 finished with value: 1.3775851726531982 and parameters: {'hidden_dim': 75, 'id_embed_dim': 5, 'lr': 0.00011131643757060093, 'batch_size': 128, 'alpha': 0.09441402428081767}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,625] Trial 31 finished with value: 0.08706812746822834 and parameters: {'hidden_dim': 118, 'id_embed_dim': 4, 'lr': 0.008309291417593059, 'batch_size': 64, 'alpha': 0.0151719934274589}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,809] Trial 32 finished with value: 0.06259273085743189 and parameters: {'hidden_dim': 128, 'id_embed_dim': 4, 'lr': 0.009978012670993654, 'batch_size': 64, 'alpha': 0.010061884513249313}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,981] Trial 33 finished with value: 0.19907968491315842 and parameters: {'hidden_dim': 61, 'id_embed_dim': 5, 'lr': 0.00657413070160299, 'batch_size': 64, 'alpha': 0.034048527533920625}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,166] Trial 34 finished with value: 0.3062892146408558 and parameters: {'hidden_dim': 119, 'id_embed_dim': 6, 'lr': 0.004761498591740868, 'batch_size': 64, 'alpha': 0.05789074837651728}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,355] Trial 35 finished with value: 0.16827015951275826 and parameters: {'hidden_dim': 104, 'id_embed_dim': 4, 'lr': 0.007583168662560991, 'batch_size': 64, 'alpha': 0.030106047205182066}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,529] Trial 36 finished with value: 0.28724484518170357 and parameters: {'hidden_dim': 42, 'id_embed_dim': 7, 'lr': 0.009914170249512907, 'batch_size': 64, 'alpha': 0.05347435066686643}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,676] Trial 37 finished with value: 0.6882201731204987 and parameters: {'hidden_dim': 88, 'id_embed_dim': 5, 'lr': 0.00140295320176409, 'batch_size': 128, 'alpha': 0.08584669716052898}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,941] Trial 38 finished with value: 0.5062135756015778 and parameters: {'hidden_dim': 95, 'id_embed_dim': 6, 'lr': 0.002294469321109287, 'batch_size': 32, 'alpha': 0.12959847542515063}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,106] Trial 39 finished with value: 0.44425083696842194 and parameters: {'hidden_dim': 48, 'id_embed_dim': 9, 'lr': 0.003467994104801525, 'batch_size': 64, 'alpha': 0.10673677939340484}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,357] Trial 40 finished with value: 0.153709358535707 and parameters: {'hidden_dim': 63, 'id_embed_dim': 8, 'lr': 0.00481805492642966, 'batch_size': 32, 'alpha': 0.026767100387599257}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,548] Trial 41 finished with value: 0.11302952654659748 and parameters: {'hidden_dim': 128, 'id_embed_dim': 4, 'lr': 0.007769729117487667, 'batch_size': 64, 'alpha': 0.019684021861826186}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,737] Trial 42 finished with value: 0.05648484732955694 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.009825678354526446, 'batch_size': 64, 'alpha': 0.010266545326245108}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,925] Trial 43 finished with value: 0.26384858787059784 and parameters: {'hidden_dim': 122, 'id_embed_dim': 4, 'lr': 0.007283738338370547, 'batch_size': 64, 'alpha': 0.044273368798980015}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,110] Trial 44 finished with value: 0.762955829501152 and parameters: {'hidden_dim': 110, 'id_embed_dim': 5, 'lr': 0.005789970478190222, 'batch_size': 64, 'alpha': 0.23885823001968826}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,339] Trial 45 finished with value: 0.05621304828673601 and parameters: {'hidden_dim': 122, 'id_embed_dim': 10, 'lr': 0.008756333164861916, 'batch_size': 64, 'alpha': 0.010432801168808375}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,524] Trial 46 finished with value: 0.624946340918541 and parameters: {'hidden_dim': 115, 'id_embed_dim': 11, 'lr': 0.00021645533144171747, 'batch_size': 64, 'alpha': 0.026340910199197754}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,788] Trial 47 finished with value: 0.38894139789044857 and parameters: {'hidden_dim': 107, 'id_embed_dim': 14, 'lr': 0.0006342763983333198, 'batch_size': 32, 'alpha': 0.0755028804209961}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,973] Trial 48 finished with value: 0.2500622980296612 and parameters: {'hidden_dim': 122, 'id_embed_dim': 12, 'lr': 0.005252859479777643, 'batch_size': 64, 'alpha': 0.051626824840157404}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,116] Trial 49 finished with value: 0.21327482163906097 and parameters: {'hidden_dim': 115, 'id_embed_dim': 10, 'lr': 0.008344913381255753, 'batch_size': 128, 'alpha': 0.039471777443053066}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,293] Trial 50 finished with value: 0.12452213652431965 and parameters: {'hidden_dim': 81, 'id_embed_dim': 16, 'lr': 0.0030605415292296323, 'batch_size': 64, 'alpha': 0.023841023224252807}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,480] Trial 51 finished with value: 0.05639892816543579 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.008400546263323459, 'batch_size': 64, 'alpha': 0.010238484605193793}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,665] Trial 52 finished with value: 0.13186048530042171 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.006730970434684805, 'batch_size': 64, 'alpha': 0.021132081437540414}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,850] Trial 53 finished with value: 0.06888443324714899 and parameters: {'hidden_dim': 115, 'id_embed_dim': 5, 'lr': 0.009949593966263989, 'batch_size': 64, 'alpha': 0.010991476280675823}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,039] Trial 54 finished with value: 0.341814287006855 and parameters: {'hidden_dim': 123, 'id_embed_dim': 11, 'lr': 0.008256304127431849, 'batch_size': 64, 'alpha': 0.06597479619770283}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,212] Trial 55 finished with value: 0.5782202258706093 and parameters: {'hidden_dim': 70, 'id_embed_dim': 7, 'lr': 0.006106626497902465, 'batch_size': 64, 'alpha': 0.15107362069574082}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,393] Trial 56 finished with value: 0.20185798406600952 and parameters: {'hidden_dim': 91, 'id_embed_dim': 13, 'lr': 0.004263599714508849, 'batch_size': 64, 'alpha': 0.039453808304277985}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,576] Trial 57 finished with value: 0.2933184653520584 and parameters: {'hidden_dim': 99, 'id_embed_dim': 6, 'lr': 0.008891271080099787, 'batch_size': 64, 'alpha': 0.05297863712779189}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,844] Trial 58 finished with value: 0.1428525522351265 and parameters: {'hidden_dim': 107, 'id_embed_dim': 5, 'lr': 0.00708308341936109, 'batch_size': 32, 'alpha': 0.022943602027970193}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,014] Trial 59 finished with value: 0.10879471898078918 and parameters: {'hidden_dim': 55, 'id_embed_dim': 6, 'lr': 0.0012664202061740074, 'batch_size': 64, 'alpha': 0.010653877727459182}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,203] Trial 60 finished with value: 0.21048878505825996 and parameters: {'hidden_dim': 125, 'id_embed_dim': 4, 'lr': 0.005697975628653981, 'batch_size': 64, 'alpha': 0.033385118728693736}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,390] Trial 61 finished with value: 0.12178081646561623 and parameters: {'hidden_dim': 118, 'id_embed_dim': 4, 'lr': 0.008177931093611559, 'batch_size': 64, 'alpha': 0.019190171841690157}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,573] Trial 62 finished with value: 0.21307244524359703 and parameters: {'hidden_dim': 128, 'id_embed_dim': 5, 'lr': 0.008745844394833559, 'batch_size': 64, 'alpha': 0.03948552845607741}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,747] Trial 63 finished with value: 0.17184165865182877 and parameters: {'hidden_dim': 78, 'id_embed_dim': 4, 'lr': 0.009927815475980843, 'batch_size': 64, 'alpha': 0.026455628570633438}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,934] Trial 64 finished with value: 0.05993795022368431 and parameters: {'hidden_dim': 121, 'id_embed_dim': 5, 'lr': 0.00689935907692606, 'batch_size': 64, 'alpha': 0.011671484969369799}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,115] Trial 65 finished with value: 0.25783900171518326 and parameters: {'hidden_dim': 112, 'id_embed_dim': 15, 'lr': 0.006718610343814702, 'batch_size': 64, 'alpha': 0.04732047971930044}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,248] Trial 66 finished with value: 0.7708775997161865 and parameters: {'hidden_dim': 71, 'id_embed_dim': 5, 'lr': 0.007435536296321056, 'batch_size': 128, 'alpha': 0.23711177218629545}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,432] Trial 67 finished with value: 0.10629190690815449 and parameters: {'hidden_dim': 116, 'id_embed_dim': 5, 'lr': 0.0054610981155523005, 'batch_size': 64, 'alpha': 0.017154105150882042}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,619] Trial 68 finished with value: 0.207490012049675 and parameters: {'hidden_dim': 121, 'id_embed_dim': 6, 'lr': 0.004644731081792112, 'batch_size': 64, 'alpha': 0.033816862322997986}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,783] Trial 69 finished with value: 1.1484081447124481 and parameters: {'hidden_dim': 32, 'id_embed_dim': 7, 'lr': 0.0007772477786342009, 'batch_size': 64, 'alpha': 0.21425401916947281}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,971] Trial 70 finished with value: 0.051440260373055935 and parameters: {'hidden_dim': 125, 'id_embed_dim': 10, 'lr': 0.008844073033562001, 'batch_size': 64, 'alpha': 0.011230085926425425}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,158] Trial 71 finished with value: 0.06483070272952318 and parameters: {'hidden_dim': 125, 'id_embed_dim': 10, 'lr': 0.008957022628983636, 'batch_size': 64, 'alpha': 0.01043840051283027}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,348] Trial 72 finished with value: 0.12975355237722397 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.0062400218468134025, 'batch_size': 64, 'alpha': 0.021546278165679715}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,534] Trial 73 finished with value: 0.5211273729801178 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.00037706860760636777, 'batch_size': 64, 'alpha': 0.031169231894628783}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,717] Trial 74 finished with value: 0.09577189013361931 and parameters: {'hidden_dim': 120, 'id_embed_dim': 4, 'lr': 0.007389538177617447, 'batch_size': 64, 'alpha': 0.016653697470470682}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,902] Trial 75 finished with value: 0.059726348146796227 and parameters: {'hidden_dim': 113, 'id_embed_dim': 10, 'lr': 0.008835495910413384, 'batch_size': 64, 'alpha': 0.010200116210348583}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,084] Trial 76 finished with value: 0.2095451056957245 and parameters: {'hidden_dim': 112, 'id_embed_dim': 12, 'lr': 0.008670812326750373, 'batch_size': 64, 'alpha': 0.03750654112316294}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,340] Trial 77 finished with value: 0.1761326091364026 and parameters: {'hidden_dim': 83, 'id_embed_dim': 9, 'lr': 0.0099259477162702, 'batch_size': 32, 'alpha': 0.02781674807233452}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,469] Trial 78 finished with value: 0.26632437109947205 and parameters: {'hidden_dim': 64, 'id_embed_dim': 10, 'lr': 0.0036825758157731513, 'batch_size': 128, 'alpha': 0.049487753210005814}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,640] Trial 79 finished with value: 0.09831168316304684 and parameters: {'hidden_dim': 72, 'id_embed_dim': 10, 'lr': 0.008114936005826617, 'batch_size': 64, 'alpha': 0.017775012248496234}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,830] Trial 80 finished with value: 0.3058706223964691 and parameters: {'hidden_dim': 124, 'id_embed_dim': 11, 'lr': 0.006192622930155039, 'batch_size': 64, 'alpha': 0.06010433847869577}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,015] Trial 81 finished with value: 0.052613225765526295 and parameters: {'hidden_dim': 120, 'id_embed_dim': 8, 'lr': 0.007231094930573221, 'batch_size': 64, 'alpha': 0.010098681772663948}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,204] Trial 82 finished with value: 0.1442696377635002 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.009063713055381846, 'batch_size': 64, 'alpha': 0.026682832175969957}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,386] Trial 83 finished with value: 0.051656538620591164 and parameters: {'hidden_dim': 128, 'id_embed_dim': 8, 'lr': 0.0075294991585114525, 'batch_size': 64, 'alpha': 0.010206491488676736}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,569] Trial 84 finished with value: 0.10502895899116993 and parameters: {'hidden_dim': 128, 'id_embed_dim': 8, 'lr': 0.005190314765674963, 'batch_size': 64, 'alpha': 0.01912367060665684}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,752] Trial 85 finished with value: 0.2412664331495762 and parameters: {'hidden_dim': 120, 'id_embed_dim': 7, 'lr': 0.007712178002742582, 'batch_size': 64, 'alpha': 0.04263924977221345}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,935] Trial 86 finished with value: 0.15927043929696083 and parameters: {'hidden_dim': 77, 'id_embed_dim': 9, 'lr': 0.007452558192094029, 'batch_size': 64, 'alpha': 0.03091918091885756}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,123] Trial 87 finished with value: 0.5280983224511147 and parameters: {'hidden_dim': 126, 'id_embed_dim': 8, 'lr': 0.0018194647722065806, 'batch_size': 64, 'alpha': 0.12286002021792271}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,309] Trial 88 finished with value: 0.1690085120499134 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.004284617031979871, 'batch_size': 64, 'alpha': 0.024138404640477014}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,486] Trial 89 finished with value: 0.7620307803153992 and parameters: {'hidden_dim': 86, 'id_embed_dim': 6, 'lr': 0.005954173975885699, 'batch_size': 64, 'alpha': 0.2622175259959613}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,758] Trial 90 finished with value: 0.10942597966641188 and parameters: {'hidden_dim': 118, 'id_embed_dim': 8, 'lr': 0.0066950732815876865, 'batch_size': 32, 'alpha': 0.016784287378305146}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,946] Trial 91 finished with value: 0.059697492979466915 and parameters: {'hidden_dim': 115, 'id_embed_dim': 10, 'lr': 0.008951044064300204, 'batch_size': 64, 'alpha': 0.011699831204134377}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,216] Trial 92 finished with value: 0.6090877801179886 and parameters: {'hidden_dim': 109, 'id_embed_dim': 9, 'lr': 0.007993397965357371, 'batch_size': 64, 'alpha': 0.17045157450177437}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,401] Trial 93 finished with value: 0.09811156801879406 and parameters: {'hidden_dim': 128, 'id_embed_dim': 9, 'lr': 0.009279680616981596, 'batch_size': 64, 'alpha': 0.015263240419761584}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,587] Trial 94 finished with value: 0.8092063665390015 and parameters: {'hidden_dim': 120, 'id_embed_dim': 10, 'lr': 0.0001529354390673257, 'batch_size': 64, 'alpha': 0.037473656733363746}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,773] Trial 95 finished with value: 0.14140361547470093 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.009308926062824751, 'batch_size': 64, 'alpha': 0.024014081201971044}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,959] Trial 96 finished with value: 0.05683023761957884 and parameters: {'hidden_dim': 118, 'id_embed_dim': 10, 'lr': 0.007847668518056568, 'batch_size': 64, 'alpha': 0.010016271198901961}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,103] Trial 97 finished with value: 0.15159650146961212 and parameters: {'hidden_dim': 118, 'id_embed_dim': 12, 'lr': 0.007064437776123548, 'batch_size': 128, 'alpha': 0.03044670787209806}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,276] Trial 98 finished with value: 0.11919774487614632 and parameters: {'hidden_dim': 73, 'id_embed_dim': 5, 'lr': 0.00811994303179023, 'batch_size': 64, 'alpha': 0.02185217800457917}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,464] Trial 99 finished with value: 0.08479415997862816 and parameters: {'hidden_dim': 126, 'id_embed_dim': 7, 'lr': 0.009979199085238206, 'batch_size': 64, 'alpha': 0.015631973197691273}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,650] Trial 100 finished with value: 0.19536328688263893 and parameters: {'hidden_dim': 122, 'id_embed_dim': 4, 'lr': 0.005485115263021394, 'batch_size': 64, 'alpha': 0.033788518550011894}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,821] Trial 101 finished with value: 0.0623770859092474 and parameters: {'hidden_dim': 60, 'id_embed_dim': 10, 'lr': 0.00636480376742213, 'batch_size': 64, 'alpha': 0.011970151921424213}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,005] Trial 102 finished with value: 0.11838086135685444 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.00778910209524276, 'batch_size': 64, 'alpha': 0.021575857611973854}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,188] Trial 103 finished with value: 0.05408777203410864 and parameters: {'hidden_dim': 116, 'id_embed_dim': 9, 'lr': 0.008785813487703143, 'batch_size': 64, 'alpha': 0.010387851449657561}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,361] Trial 104 finished with value: 0.15692763403058052 and parameters: {'hidden_dim': 66, 'id_embed_dim': 8, 'lr': 0.006748068112492149, 'batch_size': 64, 'alpha': 0.028457977749848425}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,543] Trial 105 finished with value: 0.09015006572008133 and parameters: {'hidden_dim': 92, 'id_embed_dim': 9, 'lr': 0.008418457325466071, 'batch_size': 64, 'alpha': 0.015415489977318389}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,730] Trial 106 finished with value: 0.24438275396823883 and parameters: {'hidden_dim': 117, 'id_embed_dim': 9, 'lr': 0.007374934165076103, 'batch_size': 64, 'alpha': 0.04500515599563171}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,914] Trial 107 finished with value: 0.055488359183073044 and parameters: {'hidden_dim': 120, 'id_embed_dim': 11, 'lr': 0.009259071446640246, 'batch_size': 64, 'alpha': 0.01012406757861981}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,103] Trial 108 finished with value: 0.19923456385731697 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.009089225991716689, 'batch_size': 64, 'alpha': 0.03620764771802616}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,288] Trial 109 finished with value: 0.1425265371799469 and parameters: {'hidden_dim': 124, 'id_embed_dim': 5, 'lr': 0.00996389199700516, 'batch_size': 64, 'alpha': 0.02399780152618844}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,542] Trial 110 finished with value: 0.1049658334814012 and parameters: {'hidden_dim': 79, 'id_embed_dim': 8, 'lr': 0.004683682065754066, 'batch_size': 32, 'alpha': 0.018721914166454363}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,734] Trial 111 finished with value: 0.0617807749658823 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.008147056945449161, 'batch_size': 64, 'alpha': 0.011673795581766202}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,917] Trial 112 finished with value: 0.05365587119013071 and parameters: {'hidden_dim': 111, 'id_embed_dim': 10, 'lr': 0.007094695071862744, 'batch_size': 64, 'alpha': 0.010407544700953399}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,104] Trial 113 finished with value: 0.13983620330691338 and parameters: {'hidden_dim': 120, 'id_embed_dim': 9, 'lr': 0.006952315102169736, 'batch_size': 64, 'alpha': 0.027489525471284602}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,287] Trial 114 finished with value: 0.10455440171062946 and parameters: {'hidden_dim': 111, 'id_embed_dim': 11, 'lr': 0.006020846146492399, 'batch_size': 64, 'alpha': 0.019888082402370784}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,475] Trial 115 finished with value: 0.09726104140281677 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.008703971060801608, 'batch_size': 64, 'alpha': 0.01668663923054104}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,659] Trial 116 finished with value: 0.4586992785334587 and parameters: {'hidden_dim': 116, 'id_embed_dim': 4, 'lr': 0.0004165303276020622, 'batch_size': 64, 'alpha': 0.025365045217407498}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,844] Trial 117 finished with value: 0.21301452815532684 and parameters: {'hidden_dim': 108, 'id_embed_dim': 12, 'lr': 0.009349589016335341, 'batch_size': 64, 'alpha': 0.04112794933316675}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,984] Trial 118 finished with value: 0.05161529406905174 and parameters: {'hidden_dim': 128, 'id_embed_dim': 10, 'lr': 0.006474238289639191, 'batch_size': 128, 'alpha': 0.010273284449224871}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,124] Trial 119 finished with value: 0.16531460732221603 and parameters: {'hidden_dim': 105, 'id_embed_dim': 10, 'lr': 0.005336260515213286, 'batch_size': 128, 'alpha': 0.03205181559752514}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,268] Trial 120 finished with value: 0.08465771749615669 and parameters: {'hidden_dim': 126, 'id_embed_dim': 10, 'lr': 0.007227592659310704, 'batch_size': 128, 'alpha': 0.016266937490808518}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,412] Trial 121 finished with value: 0.1255403310060501 and parameters: {'hidden_dim': 127, 'id_embed_dim': 9, 'lr': 0.00656310377446087, 'batch_size': 128, 'alpha': 0.02200772966283972}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,555] Trial 122 finished with value: 0.05349251069128513 and parameters: {'hidden_dim': 122, 'id_embed_dim': 11, 'lr': 0.008489682790677747, 'batch_size': 128, 'alpha': 0.010458047215296962}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,699] Trial 123 finished with value: 0.054100871086120605 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.008475492188014515, 'batch_size': 128, 'alpha': 0.010351592456744576}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,843] Trial 124 finished with value: 0.0869884192943573 and parameters: {'hidden_dim': 119, 'id_embed_dim': 11, 'lr': 0.007644926532639134, 'batch_size': 128, 'alpha': 0.0154538167344406}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,978] Trial 125 finished with value: 0.14334271848201752 and parameters: {'hidden_dim': 75, 'id_embed_dim': 11, 'lr': 0.0058175357549638184, 'batch_size': 128, 'alpha': 0.02718655752781548}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,121] Trial 126 finished with value: 0.11283138021826744 and parameters: {'hidden_dim': 121, 'id_embed_dim': 12, 'lr': 0.008330059617605285, 'batch_size': 128, 'alpha': 0.02067049223131956}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,261] Trial 127 finished with value: 0.11557397991418839 and parameters: {'hidden_dim': 128, 'id_embed_dim': 11, 'lr': 0.0011961001373963847, 'batch_size': 128, 'alpha': 0.0108750578022667}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,405] Trial 128 finished with value: 0.19316331297159195 and parameters: {'hidden_dim': 124, 'id_embed_dim': 10, 'lr': 0.008969530793784392, 'batch_size': 128, 'alpha': 0.03507438240927843}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,550] Trial 129 finished with value: 0.05220457911491394 and parameters: {'hidden_dim': 114, 'id_embed_dim': 11, 'lr': 0.006481188122078644, 'batch_size': 128, 'alpha': 0.010168246495813047}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,691] Trial 130 finished with value: 0.09440067037940025 and parameters: {'hidden_dim': 97, 'id_embed_dim': 11, 'lr': 0.005052910753362683, 'batch_size': 128, 'alpha': 0.017771118585101223}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,826] Trial 131 finished with value: 0.054690705612301826 and parameters: {'hidden_dim': 82, 'id_embed_dim': 11, 'lr': 0.006388565106139582, 'batch_size': 128, 'alpha': 0.01043470832306722}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,955] Trial 132 finished with value: 0.1266445480287075 and parameters: {'hidden_dim': 48, 'id_embed_dim': 12, 'lr': 0.006631209861241481, 'batch_size': 128, 'alpha': 0.02353417917236858}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:34,090] Trial 133 finished with value: 0.05090113542973995 and parameters: {'hidden_dim': 81, 'id_embed_dim': 11, 'lr': 0.007275992778172948, 'batch_size': 128, 'alpha': 0.010078136576243972}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,225] Trial 134 finished with value: 0.08768726512789726 and parameters: {'hidden_dim': 82, 'id_embed_dim': 11, 'lr': 0.006070573569936727, 'batch_size': 128, 'alpha': 0.015648231844048878}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,365] Trial 135 finished with value: 0.16624315828084946 and parameters: {'hidden_dim': 87, 'id_embed_dim': 13, 'lr': 0.0076536517667785955, 'batch_size': 128, 'alpha': 0.03030065302704323}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,508] Trial 136 finished with value: 0.06472321227192879 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.002415266659827341, 'batch_size': 128, 'alpha': 0.010482263499157143}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,645] Trial 137 finished with value: 0.12118079885840416 and parameters: {'hidden_dim': 84, 'id_embed_dim': 11, 'lr': 0.0055315415197884415, 'batch_size': 128, 'alpha': 0.021299009790294286}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,788] Trial 138 finished with value: 0.08092004060745239 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.007108110671844618, 'batch_size': 128, 'alpha': 0.015981044594742908}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,925] Trial 139 finished with value: 0.15860337018966675 and parameters: {'hidden_dim': 81, 'id_embed_dim': 12, 'lr': 0.004481902814131419, 'batch_size': 128, 'alpha': 0.028002046011718625}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,069] Trial 140 finished with value: 0.11584538966417313 and parameters: {'hidden_dim': 125, 'id_embed_dim': 10, 'lr': 0.0064741675869571785, 'batch_size': 128, 'alpha': 0.021086958976777266}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,209] Trial 141 finished with value: 0.05720410495996475 and parameters: {'hidden_dim': 70, 'id_embed_dim': 11, 'lr': 0.00991054133775717, 'batch_size': 128, 'alpha': 0.010548050151411328}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,347] Trial 142 finished with value: 0.6459639668464661 and parameters: {'hidden_dim': 89, 'id_embed_dim': 11, 'lr': 0.00800214695491973, 'batch_size': 128, 'alpha': 0.1972589442861588}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,479] Trial 143 finished with value: 0.07903076708316803 and parameters: {'hidden_dim': 76, 'id_embed_dim': 10, 'lr': 0.009179360814912604, 'batch_size': 128, 'alpha': 0.015203730247797215}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,622] Trial 144 finished with value: 0.13921266794204712 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.007369967348885671, 'batch_size': 128, 'alpha': 0.024810926538758694}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,754] Trial 145 finished with value: 0.08689390495419502 and parameters: {'hidden_dim': 58, 'id_embed_dim': 10, 'lr': 0.00847486687044206, 'batch_size': 128, 'alpha': 0.01642601322269499}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,898] Trial 146 finished with value: 0.10970665886998177 and parameters: {'hidden_dim': 115, 'id_embed_dim': 11, 'lr': 0.0069876369936395415, 'batch_size': 128, 'alpha': 0.020724731206204695}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,039] Trial 147 finished with value: 0.15838222950696945 and parameters: {'hidden_dim': 128, 'id_embed_dim': 11, 'lr': 0.006171605240084891, 'batch_size': 128, 'alpha': 0.030539199305943045}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,169] Trial 148 finished with value: 0.058270152658224106 and parameters: {'hidden_dim': 53, 'id_embed_dim': 12, 'lr': 0.008319594306217117, 'batch_size': 128, 'alpha': 0.011841416892421113}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,423] Trial 149 finished with value: 0.42758868262171745 and parameters: {'hidden_dim': 79, 'id_embed_dim': 10, 'lr': 0.009237679486967692, 'batch_size': 32, 'alpha': 0.08942445497423067}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,568] Trial 150 finished with value: 0.48056089878082275 and parameters: {'hidden_dim': 122, 'id_embed_dim': 6, 'lr': 0.007642463576190737, 'batch_size': 128, 'alpha': 0.10497598511803088}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,799] Trial 151 finished with value: 0.07247111853212118 and parameters: {'hidden_dim': 124, 'id_embed_dim': 10, 'lr': 0.008589530545470798, 'batch_size': 64, 'alpha': 0.011701147136901376}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,943] Trial 152 finished with value: 0.05259040556848049 and parameters: {'hidden_dim': 121, 'id_embed_dim': 9, 'lr': 0.009414109318818378, 'batch_size': 128, 'alpha': 0.010322931985936036}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,087] Trial 153 finished with value: 0.09702982380986214 and parameters: {'hidden_dim': 121, 'id_embed_dim': 9, 'lr': 0.009886653688139924, 'batch_size': 128, 'alpha': 0.017702812468014452}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,232] Trial 154 finished with value: 0.05513021908700466 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.007968598842272042, 'batch_size': 128, 'alpha': 0.010449140854516595}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,377] Trial 155 finished with value: 0.05857452191412449 and parameters: {'hidden_dim': 114, 'id_embed_dim': 8, 'lr': 0.006672539417249449, 'batch_size': 128, 'alpha': 0.010011778383335598}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,519] Trial 156 finished with value: 0.14669062942266464 and parameters: {'hidden_dim': 110, 'id_embed_dim': 7, 'lr': 0.007596131527995706, 'batch_size': 128, 'alpha': 0.024860203843522596}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,663] Trial 157 finished with value: 0.09143709391355515 and parameters: {'hidden_dim': 119, 'id_embed_dim': 9, 'lr': 0.007101067170284066, 'batch_size': 128, 'alpha': 0.01692644177688149}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,806] Trial 158 finished with value: 0.1308250054717064 and parameters: {'hidden_dim': 116, 'id_embed_dim': 8, 'lr': 0.005666301064924639, 'batch_size': 128, 'alpha': 0.023341014040730745}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,951] Trial 159 finished with value: 0.05205344595015049 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.008037387972004074, 'batch_size': 128, 'alpha': 0.01026022767799177}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,095] Trial 160 finished with value: 0.19356537610292435 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.006268427253045301, 'batch_size': 128, 'alpha': 0.03708208476323883}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,239] Trial 161 finished with value: 0.08927670121192932 and parameters: {'hidden_dim': 122, 'id_embed_dim': 8, 'lr': 0.008134299363326745, 'batch_size': 128, 'alpha': 0.015934113990517862}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,384] Trial 162 finished with value: 0.06054350174963474 and parameters: {'hidden_dim': 126, 'id_embed_dim': 9, 'lr': 0.00907468810631862, 'batch_size': 128, 'alpha': 0.010783176504340339}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,527] Trial 163 finished with value: 0.05360429920256138 and parameters: {'hidden_dim': 120, 'id_embed_dim': 8, 'lr': 0.007777035865963799, 'batch_size': 128, 'alpha': 0.010188637406594446}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,670] Trial 164 finished with value: 0.54863041639328 and parameters: {'hidden_dim': 120, 'id_embed_dim': 8, 'lr': 0.007705892630040006, 'batch_size': 128, 'alpha': 0.14191405310713326}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,814] Trial 165 finished with value: 0.11340060830116272 and parameters: {'hidden_dim': 118, 'id_embed_dim': 7, 'lr': 0.007196721861867506, 'batch_size': 128, 'alpha': 0.01945293459301741}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,959] Trial 166 finished with value: 0.05008469149470329 and parameters: {'hidden_dim': 123, 'id_embed_dim': 8, 'lr': 0.008485165951808546, 'batch_size': 128, 'alpha': 0.010445818879173832}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,104] Trial 167 finished with value: 0.8175391852855682 and parameters: {'hidden_dim': 123, 'id_embed_dim': 8, 'lr': 0.006541273374203244, 'batch_size': 128, 'alpha': 0.2924565112787347}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,247] Trial 168 finished with value: 0.13165225833654404 and parameters: {'hidden_dim': 117, 'id_embed_dim': 8, 'lr': 0.008276270195782755, 'batch_size': 128, 'alpha': 0.02005062227645945}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,391] Trial 169 finished with value: 0.1525122970342636 and parameters: {'hidden_dim': 124, 'id_embed_dim': 9, 'lr': 0.007042323225318343, 'batch_size': 128, 'alpha': 0.0288565693655648}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,537] Trial 170 finished with value: 0.05784558691084385 and parameters: {'hidden_dim': 127, 'id_embed_dim': 7, 'lr': 0.005790593354439077, 'batch_size': 128, 'alpha': 0.01017858938101399}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,683] Trial 171 finished with value: 0.054975297302007675 and parameters: {'hidden_dim': 121, 'id_embed_dim': 8, 'lr': 0.009008282233692133, 'batch_size': 128, 'alpha': 0.010176324583181316}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,829] Trial 172 finished with value: 0.0862882174551487 and parameters: {'hidden_dim': 121, 'id_embed_dim': 8, 'lr': 0.008663415120651998, 'batch_size': 128, 'alpha': 0.016167876731178074}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,973] Trial 173 finished with value: 0.05297590792179108 and parameters: {'hidden_dim': 122, 'id_embed_dim': 8, 'lr': 0.007990600382312066, 'batch_size': 128, 'alpha': 0.010076454207975764}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,119] Trial 174 finished with value: 0.06127040646970272 and parameters: {'hidden_dim': 122, 'id_embed_dim': 8, 'lr': 0.007799090731871563, 'batch_size': 128, 'alpha': 0.01005383603739881}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,263] Trial 175 finished with value: 0.08967378363013268 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.0068753487127741495, 'batch_size': 128, 'alpha': 0.016240364386087697}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,407] Trial 176 finished with value: 0.1335640847682953 and parameters: {'hidden_dim': 124, 'id_embed_dim': 8, 'lr': 0.00799381773916382, 'batch_size': 128, 'alpha': 0.024887454422873757}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,547] Trial 177 finished with value: 0.10670970007777214 and parameters: {'hidden_dim': 112, 'id_embed_dim': 9, 'lr': 0.005035526809767247, 'batch_size': 128, 'alpha': 0.02081229612395726}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,689] Trial 178 finished with value: 0.07823453471064568 and parameters: {'hidden_dim': 116, 'id_embed_dim': 8, 'lr': 0.008736218762670778, 'batch_size': 128, 'alpha': 0.015435621895675159}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,836] Trial 179 finished with value: 0.15515702962875366 and parameters: {'hidden_dim': 126, 'id_embed_dim': 7, 'lr': 0.0061704060140859406, 'batch_size': 128, 'alpha': 0.02485808187484984}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,981] Trial 180 finished with value: 0.08294307813048363 and parameters: {'hidden_dim': 121, 'id_embed_dim': 9, 'lr': 0.007412327123500602, 'batch_size': 128, 'alpha': 0.015934990705048578}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,132] Trial 181 finished with value: 0.059299347922205925 and parameters: {'hidden_dim': 128, 'id_embed_dim': 8, 'lr': 0.009276107678972929, 'batch_size': 128, 'alpha': 0.010337629836183644}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,277] Trial 182 finished with value: 0.10835566371679306 and parameters: {'hidden_dim': 118, 'id_embed_dim': 8, 'lr': 0.009961905893953418, 'batch_size': 128, 'alpha': 0.02023191537233988}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,424] Trial 183 finished with value: 0.09070723876357079 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.008386878051114807, 'batch_size': 128, 'alpha': 0.015499096760508299}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,568] Trial 184 finished with value: 0.08224388211965561 and parameters: {'hidden_dim': 123, 'id_embed_dim': 7, 'lr': 0.007757101913755014, 'batch_size': 128, 'alpha': 0.014866272065112231}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,711] Trial 185 finished with value: 0.05335717648267746 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.008940569539033673, 'batch_size': 128, 'alpha': 0.010023882581139017}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,857] Trial 186 finished with value: 0.052620502188801765 and parameters: {'hidden_dim': 116, 'id_embed_dim': 10, 'lr': 0.006828229023600156, 'batch_size': 128, 'alpha': 0.010273130270962182}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,010] Trial 187 finished with value: 0.11930907517671585 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.006765151075034555, 'batch_size': 128, 'alpha': 0.021091023428463442}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,156] Trial 188 finished with value: 0.1543382927775383 and parameters: {'hidden_dim': 113, 'id_embed_dim': 10, 'lr': 0.007254472660357189, 'batch_size': 128, 'alpha': 0.028588699621335197}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,300] Trial 189 finished with value: 0.10691815987229347 and parameters: {'hidden_dim': 116, 'id_embed_dim': 10, 'lr': 0.006259978849764696, 'batch_size': 128, 'alpha': 0.019517368565244456}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,440] Trial 190 finished with value: 0.055666208267211914 and parameters: {'hidden_dim': 110, 'id_embed_dim': 10, 'lr': 0.008780384573511792, 'batch_size': 128, 'alpha': 0.01036029627030003}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,586] Trial 191 finished with value: 0.09126255661249161 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.008028710027483014, 'batch_size': 128, 'alpha': 0.015117503773939141}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,729] Trial 192 finished with value: 0.05440042167901993 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.007466120712074496, 'batch_size': 128, 'alpha': 0.010838113872028935}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,877] Trial 193 finished with value: 0.08801053464412689 and parameters: {'hidden_dim': 123, 'id_embed_dim': 11, 'lr': 0.007134935735652707, 'batch_size': 128, 'alpha': 0.01621754134346199}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:43,027] Trial 194 finished with value: 0.04822372645139694 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.006456160517681038, 'batch_size': 128, 'alpha': 0.010000905656281955}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,173] Trial 195 finished with value: 0.1511356681585312 and parameters: {'hidden_dim': 101, 'id_embed_dim': 11, 'lr': 0.00584231340781724, 'batch_size': 128, 'alpha': 0.02601138894289126}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,321] Trial 196 finished with value: 0.11274676769971848 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.006405997200660003, 'batch_size': 128, 'alpha': 0.02118117394596252}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,471] Trial 197 finished with value: 0.08045604452490807 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.005156847399428343, 'batch_size': 128, 'alpha': 0.015550574982799995}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,617] Trial 198 finished with value: 0.12036525458097458 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.006719099199685368, 'batch_size': 128, 'alpha': 0.022338880647603927}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,892] Trial 199 finished with value: 0.0624529286287725 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.00744227393327017, 'batch_size': 32, 'alpha': 0.010039802584321517}. Best is trial 194 with value: 0.04822372645139694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.006456160517681038, 'batch_size': 128, 'alpha': 0.010000905656281955}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, train_df_split, val_df_split, id_map, input_dim, id_count, output_dim, device), n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparams:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c89fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "model = RNNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    id_count=id_count,\n",
    "    id_embed_dim=best_params['id_embed_dim'],\n",
    "    output_dim=output_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = OrdinalLabelSmoothingLoss(num_classes=output_dim, alpha=best_params['alpha'])\n",
    "\n",
    "train_loader = DataLoader(MoodDataset(train_df_split, id_map), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(MoodDataset(val_df_split, id_map), batch_size=best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0bfbe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.3811, val loss = 0.2817\n",
      "Epoch 2: train loss = 0.1403, val loss = 0.1171\n",
      "Epoch 3: train loss = 0.0815, val loss = 0.0823\n",
      "Epoch 4: train loss = 0.0624, val loss = 0.0661\n",
      "Epoch 5: train loss = 0.0534, val loss = 0.0604\n",
      "Epoch 6: train loss = 0.0485, val loss = 0.0571\n",
      "Epoch 7: train loss = 0.0457, val loss = 0.0565\n",
      "Epoch 8: train loss = 0.0432, val loss = 0.0549\n",
      "Epoch 9: train loss = 0.0411, val loss = 0.0524\n",
      "Epoch 10: train loss = 0.0389, val loss = 0.0546\n",
      "Epoch 11: train loss = 0.0374, val loss = 0.0531\n",
      "Epoch 12: train loss = 0.0355, val loss = 0.0529\n",
      "Epoch 13: train loss = 0.0340, val loss = 0.0546\n",
      "Epoch 14: train loss = 0.0319, val loss = 0.0536\n",
      "Epoch 15: train loss = 0.0309, val loss = 0.0544\n",
      "Epoch 16: train loss = 0.0297, val loss = 0.0570\n",
      "Epoch 17: train loss = 0.0278, val loss = 0.0579\n",
      "Epoch 18: train loss = 0.0268, val loss = 0.0587\n",
      "Epoch 19: train loss = 0.0256, val loss = 0.0587\n",
      "Epoch 20: train loss = 0.0243, val loss = 0.0611\n",
      "Epoch 21: train loss = 0.0232, val loss = 0.0631\n",
      "Epoch 22: train loss = 0.0227, val loss = 0.0635\n",
      "Epoch 23: train loss = 0.0211, val loss = 0.0648\n",
      "Epoch 24: train loss = 0.0206, val loss = 0.0637\n",
      "Epoch 25: train loss = 0.0192, val loss = 0.0666\n",
      "Epoch 26: train loss = 0.0183, val loss = 0.0674\n",
      "Epoch 27: train loss = 0.0174, val loss = 0.0683\n",
      "Epoch 28: train loss = 0.0169, val loss = 0.0691\n",
      "Epoch 29: train loss = 0.0157, val loss = 0.0714\n",
      "Epoch 30: train loss = 0.0152, val loss = 0.0707\n",
      "Epoch 31: train loss = 0.0143, val loss = 0.0756\n",
      "Epoch 32: train loss = 0.0136, val loss = 0.0738\n",
      "Epoch 33: train loss = 0.0131, val loss = 0.0769\n",
      "Epoch 34: train loss = 0.0127, val loss = 0.0755\n",
      "Epoch 35: train loss = 0.0120, val loss = 0.0769\n",
      "Epoch 36: train loss = 0.0115, val loss = 0.0781\n",
      "Epoch 37: train loss = 0.0107, val loss = 0.0767\n",
      "Epoch 38: train loss = 0.0102, val loss = 0.0802\n",
      "Epoch 39: train loss = 0.0099, val loss = 0.0792\n",
      "Epoch 40: train loss = 0.0098, val loss = 0.0778\n",
      "Epoch 41: train loss = 0.0093, val loss = 0.0804\n",
      "Epoch 42: train loss = 0.0087, val loss = 0.0810\n",
      "Epoch 43: train loss = 0.0083, val loss = 0.0792\n",
      "Epoch 44: train loss = 0.0078, val loss = 0.0811\n",
      "Epoch 45: train loss = 0.0075, val loss = 0.0834\n",
      "Epoch 46: train loss = 0.0070, val loss = 0.0824\n",
      "Epoch 47: train loss = 0.0069, val loss = 0.0820\n",
      "Epoch 48: train loss = 0.0063, val loss = 0.0808\n",
      "Epoch 49: train loss = 0.0059, val loss = 0.0832\n",
      "Epoch 50: train loss = 0.0056, val loss = 0.0841\n",
      "Epoch 51: train loss = 0.0054, val loss = 0.0838\n",
      "Epoch 52: train loss = 0.0052, val loss = 0.0846\n",
      "Epoch 53: train loss = 0.0047, val loss = 0.0831\n",
      "Epoch 54: train loss = 0.0046, val loss = 0.0839\n",
      "Epoch 55: train loss = 0.0046, val loss = 0.0849\n",
      "Epoch 56: train loss = 0.0042, val loss = 0.0858\n",
      "Epoch 57: train loss = 0.0040, val loss = 0.0855\n",
      "Epoch 58: train loss = 0.0039, val loss = 0.0869\n",
      "Epoch 59: train loss = 0.0040, val loss = 0.0860\n",
      "Epoch 60: train loss = 0.0036, val loss = 0.0875\n",
      "Epoch 61: train loss = 0.0036, val loss = 0.0887\n",
      "Epoch 62: train loss = 0.0037, val loss = 0.0872\n",
      "Epoch 63: train loss = 0.0037, val loss = 0.0892\n",
      "Epoch 64: train loss = 0.0034, val loss = 0.0876\n",
      "Epoch 65: train loss = 0.0033, val loss = 0.0896\n",
      "Epoch 66: train loss = 0.0033, val loss = 0.0897\n",
      "Epoch 67: train loss = 0.0031, val loss = 0.0896\n",
      "Epoch 68: train loss = 0.0029, val loss = 0.0894\n",
      "Epoch 69: train loss = 0.0028, val loss = 0.0896\n",
      "Epoch 70: train loss = 0.0025, val loss = 0.0922\n",
      "Epoch 71: train loss = 0.0025, val loss = 0.0913\n",
      "Epoch 72: train loss = 0.0024, val loss = 0.0907\n",
      "Epoch 73: train loss = 0.0022, val loss = 0.0905\n",
      "Epoch 74: train loss = 0.0021, val loss = 0.0926\n",
      "Epoch 75: train loss = 0.0021, val loss = 0.0925\n",
      "Epoch 76: train loss = 0.0021, val loss = 0.0922\n",
      "Epoch 77: train loss = 0.0019, val loss = 0.0919\n",
      "Epoch 78: train loss = 0.0019, val loss = 0.0956\n",
      "Epoch 79: train loss = 0.0018, val loss = 0.0914\n",
      "Epoch 80: train loss = 0.0019, val loss = 0.0944\n",
      "Epoch 81: train loss = 0.0018, val loss = 0.0939\n",
      "Epoch 82: train loss = 0.0017, val loss = 0.0934\n",
      "Epoch 83: train loss = 0.0017, val loss = 0.0954\n",
      "Epoch 84: train loss = 0.0016, val loss = 0.0953\n",
      "Epoch 85: train loss = 0.0015, val loss = 0.0937\n",
      "Epoch 86: train loss = 0.0014, val loss = 0.0952\n",
      "Epoch 87: train loss = 0.0012, val loss = 0.0944\n",
      "Epoch 88: train loss = 0.0013, val loss = 0.0958\n",
      "Epoch 89: train loss = 0.0011, val loss = 0.0954\n",
      "Epoch 90: train loss = 0.0012, val loss = 0.0954\n",
      "Epoch 91: train loss = 0.0012, val loss = 0.0962\n",
      "Epoch 92: train loss = 0.0011, val loss = 0.0969\n",
      "Epoch 93: train loss = 0.0011, val loss = 0.0954\n",
      "Epoch 94: train loss = 0.0010, val loss = 0.0973\n",
      "Epoch 95: train loss = 0.0009, val loss = 0.0949\n",
      "Epoch 96: train loss = 0.0009, val loss = 0.0963\n",
      "Epoch 97: train loss = 0.0009, val loss = 0.0969\n",
      "Epoch 98: train loss = 0.0008, val loss = 0.0966\n",
      "Epoch 99: train loss = 0.0009, val loss = 0.0976\n",
      "Epoch 100: train loss = 0.0009, val loss = 0.0977\n",
      "Epoch 101: train loss = 0.0009, val loss = 0.0991\n",
      "Epoch 102: train loss = 0.0009, val loss = 0.0991\n",
      "Epoch 103: train loss = 0.0009, val loss = 0.0984\n",
      "Epoch 104: train loss = 0.0008, val loss = 0.0980\n",
      "Epoch 105: train loss = 0.0008, val loss = 0.0968\n",
      "Epoch 106: train loss = 0.0007, val loss = 0.0984\n",
      "Epoch 107: train loss = 0.0007, val loss = 0.0986\n",
      "Epoch 108: train loss = 0.0007, val loss = 0.0978\n",
      "Epoch 109: train loss = 0.0007, val loss = 0.0988\n",
      "Epoch 110: train loss = 0.0007, val loss = 0.0972\n",
      "Epoch 111: train loss = 0.0006, val loss = 0.0994\n",
      "Epoch 112: train loss = 0.0007, val loss = 0.0993\n",
      "Epoch 113: train loss = 0.0006, val loss = 0.0987\n",
      "Epoch 114: train loss = 0.0006, val loss = 0.0988\n",
      "Epoch 115: train loss = 0.0006, val loss = 0.0992\n",
      "Epoch 116: train loss = 0.0006, val loss = 0.0993\n",
      "Epoch 117: train loss = 0.0006, val loss = 0.0992\n",
      "Epoch 118: train loss = 0.0006, val loss = 0.1004\n",
      "Epoch 119: train loss = 0.0006, val loss = 0.0974\n",
      "Epoch 120: train loss = 0.0007, val loss = 0.1013\n",
      "Epoch 121: train loss = 0.0006, val loss = 0.0992\n",
      "Epoch 122: train loss = 0.0006, val loss = 0.1014\n",
      "Epoch 123: train loss = 0.0005, val loss = 0.0995\n",
      "Epoch 124: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 125: train loss = 0.0005, val loss = 0.0982\n",
      "Epoch 126: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 127: train loss = 0.0004, val loss = 0.1008\n",
      "Epoch 128: train loss = 0.0005, val loss = 0.1014\n",
      "Epoch 129: train loss = 0.0004, val loss = 0.1005\n",
      "Epoch 130: train loss = 0.0004, val loss = 0.1000\n",
      "Epoch 131: train loss = 0.0004, val loss = 0.1006\n",
      "Epoch 132: train loss = 0.0004, val loss = 0.1024\n",
      "Epoch 133: train loss = 0.0004, val loss = 0.1009\n",
      "Epoch 134: train loss = 0.0004, val loss = 0.1004\n",
      "Epoch 135: train loss = 0.0004, val loss = 0.1000\n",
      "Epoch 136: train loss = 0.0005, val loss = 0.1015\n",
      "Epoch 137: train loss = 0.0005, val loss = 0.1011\n",
      "Epoch 138: train loss = 0.0006, val loss = 0.1011\n",
      "Epoch 139: train loss = 0.0006, val loss = 0.1012\n",
      "Epoch 140: train loss = 0.0006, val loss = 0.1026\n",
      "Epoch 141: train loss = 0.0006, val loss = 0.1004\n",
      "Epoch 142: train loss = 0.0006, val loss = 0.1008\n",
      "Epoch 143: train loss = 0.0005, val loss = 0.1013\n",
      "Epoch 144: train loss = 0.0005, val loss = 0.1007\n",
      "Epoch 145: train loss = 0.0005, val loss = 0.1011\n",
      "Epoch 146: train loss = 0.0005, val loss = 0.1018\n",
      "Epoch 147: train loss = 0.0006, val loss = 0.1004\n",
      "Epoch 148: train loss = 0.0005, val loss = 0.1024\n",
      "Epoch 149: train loss = 0.0005, val loss = 0.1014\n",
      "Epoch 150: train loss = 0.0004, val loss = 0.1009\n",
      "Epoch 151: train loss = 0.0004, val loss = 0.1012\n",
      "Epoch 152: train loss = 0.0004, val loss = 0.1018\n",
      "Epoch 153: train loss = 0.0004, val loss = 0.1015\n",
      "Epoch 154: train loss = 0.0003, val loss = 0.1024\n",
      "Epoch 155: train loss = 0.0003, val loss = 0.1007\n",
      "Epoch 156: train loss = 0.0003, val loss = 0.1019\n",
      "Epoch 157: train loss = 0.0003, val loss = 0.1022\n",
      "Epoch 158: train loss = 0.0004, val loss = 0.1011\n",
      "Epoch 159: train loss = 0.0004, val loss = 0.1033\n",
      "Epoch 160: train loss = 0.0005, val loss = 0.1008\n",
      "Epoch 161: train loss = 0.0006, val loss = 0.1029\n",
      "Epoch 162: train loss = 0.0005, val loss = 0.1010\n",
      "Epoch 163: train loss = 0.0006, val loss = 0.1031\n",
      "Epoch 164: train loss = 0.0008, val loss = 0.1001\n",
      "Epoch 165: train loss = 0.0009, val loss = 0.1021\n",
      "Epoch 166: train loss = 0.0009, val loss = 0.1014\n",
      "Epoch 167: train loss = 0.0009, val loss = 0.1022\n",
      "Epoch 168: train loss = 0.0008, val loss = 0.1016\n",
      "Epoch 169: train loss = 0.0009, val loss = 0.1016\n",
      "Epoch 170: train loss = 0.0009, val loss = 0.1025\n",
      "Epoch 171: train loss = 0.0007, val loss = 0.1019\n",
      "Epoch 172: train loss = 0.0007, val loss = 0.0995\n",
      "Epoch 173: train loss = 0.0006, val loss = 0.1001\n",
      "Epoch 174: train loss = 0.0006, val loss = 0.0998\n",
      "Epoch 175: train loss = 0.0006, val loss = 0.1005\n",
      "Epoch 176: train loss = 0.0005, val loss = 0.1015\n",
      "Epoch 177: train loss = 0.0004, val loss = 0.0997\n",
      "Epoch 178: train loss = 0.0003, val loss = 0.1015\n",
      "Epoch 179: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 180: train loss = 0.0003, val loss = 0.1030\n",
      "Epoch 181: train loss = 0.0003, val loss = 0.1019\n",
      "Epoch 182: train loss = 0.0002, val loss = 0.1012\n",
      "Epoch 183: train loss = 0.0002, val loss = 0.1016\n",
      "Epoch 184: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 185: train loss = 0.0003, val loss = 0.1014\n",
      "Epoch 186: train loss = 0.0003, val loss = 0.1027\n",
      "Epoch 187: train loss = 0.0003, val loss = 0.1016\n",
      "Epoch 188: train loss = 0.0003, val loss = 0.1029\n",
      "Epoch 189: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 190: train loss = 0.0002, val loss = 0.1014\n",
      "Epoch 191: train loss = 0.0002, val loss = 0.1024\n",
      "Epoch 192: train loss = 0.0002, val loss = 0.1013\n",
      "Epoch 193: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 194: train loss = 0.0002, val loss = 0.1016\n",
      "Epoch 195: train loss = 0.0003, val loss = 0.1022\n",
      "Epoch 196: train loss = 0.0003, val loss = 0.1023\n",
      "Epoch 197: train loss = 0.0003, val loss = 0.1026\n",
      "Epoch 198: train loss = 0.0003, val loss = 0.1032\n",
      "Epoch 199: train loss = 0.0002, val loss = 0.1025\n",
      "Epoch 200: train loss = 0.0002, val loss = 0.1017\n",
      "Epoch 201: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 202: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 203: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 204: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 205: train loss = 0.0002, val loss = 0.1030\n",
      "Epoch 206: train loss = 0.0002, val loss = 0.1027\n",
      "Epoch 207: train loss = 0.0002, val loss = 0.1036\n",
      "Epoch 208: train loss = 0.0002, val loss = 0.1018\n",
      "Epoch 209: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 210: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 211: train loss = 0.0002, val loss = 0.1031\n",
      "Epoch 212: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 213: train loss = 0.0002, val loss = 0.1031\n",
      "Epoch 214: train loss = 0.0002, val loss = 0.1023\n",
      "Epoch 215: train loss = 0.0003, val loss = 0.1036\n",
      "Epoch 216: train loss = 0.0003, val loss = 0.1035\n",
      "Epoch 217: train loss = 0.0004, val loss = 0.1028\n",
      "Epoch 218: train loss = 0.0004, val loss = 0.1025\n",
      "Epoch 219: train loss = 0.0004, val loss = 0.1029\n",
      "Epoch 220: train loss = 0.0003, val loss = 0.1018\n",
      "Epoch 221: train loss = 0.0004, val loss = 0.1026\n",
      "Epoch 222: train loss = 0.0004, val loss = 0.1030\n",
      "Epoch 223: train loss = 0.0004, val loss = 0.1018\n",
      "Epoch 224: train loss = 0.0004, val loss = 0.1029\n",
      "Epoch 225: train loss = 0.0004, val loss = 0.1030\n",
      "Epoch 226: train loss = 0.0004, val loss = 0.1027\n",
      "Epoch 227: train loss = 0.0004, val loss = 0.1028\n",
      "Epoch 228: train loss = 0.0004, val loss = 0.1019\n",
      "Epoch 229: train loss = 0.0004, val loss = 0.1025\n",
      "Epoch 230: train loss = 0.0005, val loss = 0.1022\n",
      "Epoch 231: train loss = 0.0005, val loss = 0.1031\n",
      "Epoch 232: train loss = 0.0005, val loss = 0.1005\n",
      "Epoch 233: train loss = 0.0005, val loss = 0.1045\n",
      "Epoch 234: train loss = 0.0005, val loss = 0.1020\n",
      "Epoch 235: train loss = 0.0006, val loss = 0.1015\n",
      "Epoch 236: train loss = 0.0007, val loss = 0.1003\n",
      "Epoch 237: train loss = 0.0008, val loss = 0.1026\n",
      "Epoch 238: train loss = 0.0008, val loss = 0.0998\n",
      "Epoch 239: train loss = 0.0007, val loss = 0.1019\n",
      "Epoch 240: train loss = 0.0005, val loss = 0.1024\n",
      "Epoch 241: train loss = 0.0005, val loss = 0.1001\n",
      "Epoch 242: train loss = 0.0004, val loss = 0.1008\n",
      "Epoch 243: train loss = 0.0004, val loss = 0.1010\n",
      "Epoch 244: train loss = 0.0004, val loss = 0.1002\n",
      "Epoch 245: train loss = 0.0004, val loss = 0.1019\n",
      "Epoch 246: train loss = 0.0003, val loss = 0.0997\n",
      "Epoch 247: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 248: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 249: train loss = 0.0003, val loss = 0.1020\n",
      "Epoch 250: train loss = 0.0002, val loss = 0.1013\n",
      "Epoch 251: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 252: train loss = 0.0003, val loss = 0.1010\n",
      "Epoch 253: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 254: train loss = 0.0003, val loss = 0.1017\n",
      "Epoch 255: train loss = 0.0003, val loss = 0.1016\n",
      "Epoch 256: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 257: train loss = 0.0003, val loss = 0.1020\n",
      "Epoch 258: train loss = 0.0003, val loss = 0.1008\n",
      "Epoch 259: train loss = 0.0003, val loss = 0.1024\n",
      "Epoch 260: train loss = 0.0002, val loss = 0.1002\n",
      "Epoch 261: train loss = 0.0002, val loss = 0.1021\n",
      "Epoch 262: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 263: train loss = 0.0002, val loss = 0.1019\n",
      "Epoch 264: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 265: train loss = 0.0002, val loss = 0.1015\n",
      "Epoch 266: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 267: train loss = 0.0002, val loss = 0.1019\n",
      "Epoch 268: train loss = 0.0002, val loss = 0.1000\n",
      "Epoch 269: train loss = 0.0002, val loss = 0.1014\n",
      "Epoch 270: train loss = 0.0003, val loss = 0.1017\n",
      "Epoch 271: train loss = 0.0003, val loss = 0.1013\n",
      "Epoch 272: train loss = 0.0003, val loss = 0.1019\n",
      "Epoch 273: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 274: train loss = 0.0004, val loss = 0.1003\n",
      "Epoch 275: train loss = 0.0004, val loss = 0.1030\n",
      "Epoch 276: train loss = 0.0004, val loss = 0.1016\n",
      "Epoch 277: train loss = 0.0004, val loss = 0.1020\n",
      "Epoch 278: train loss = 0.0003, val loss = 0.1008\n",
      "Epoch 279: train loss = 0.0003, val loss = 0.1015\n",
      "Epoch 280: train loss = 0.0003, val loss = 0.1013\n",
      "Epoch 281: train loss = 0.0003, val loss = 0.1014\n",
      "Epoch 282: train loss = 0.0003, val loss = 0.0999\n",
      "Epoch 283: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 284: train loss = 0.0003, val loss = 0.1001\n",
      "Epoch 285: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 286: train loss = 0.0003, val loss = 0.1007\n",
      "Epoch 287: train loss = 0.0003, val loss = 0.1018\n",
      "Epoch 288: train loss = 0.0003, val loss = 0.0995\n",
      "Epoch 289: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 290: train loss = 0.0002, val loss = 0.1007\n",
      "Epoch 291: train loss = 0.0002, val loss = 0.1009\n",
      "Epoch 292: train loss = 0.0002, val loss = 0.1006\n",
      "Epoch 293: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 294: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 295: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 296: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 297: train loss = 0.0002, val loss = 0.1009\n",
      "Epoch 298: train loss = 0.0003, val loss = 0.1008\n",
      "Epoch 299: train loss = 0.0003, val loss = 0.1013\n",
      "Epoch 300: train loss = 0.0004, val loss = 0.0998\n",
      "Epoch 301: train loss = 0.0004, val loss = 0.1006\n",
      "Epoch 302: train loss = 0.0005, val loss = 0.1012\n",
      "Epoch 303: train loss = 0.0005, val loss = 0.1001\n",
      "Epoch 304: train loss = 0.0005, val loss = 0.1013\n",
      "Epoch 305: train loss = 0.0005, val loss = 0.0992\n",
      "Epoch 306: train loss = 0.0004, val loss = 0.0995\n",
      "Epoch 307: train loss = 0.0004, val loss = 0.0999\n",
      "Epoch 308: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 309: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 310: train loss = 0.0002, val loss = 0.1005\n",
      "Epoch 311: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 312: train loss = 0.0002, val loss = 0.1010\n",
      "Epoch 313: train loss = 0.0002, val loss = 0.1004\n",
      "Epoch 314: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 315: train loss = 0.0003, val loss = 0.1000\n",
      "Epoch 316: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 317: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 318: train loss = 0.0003, val loss = 0.1002\n",
      "Epoch 319: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 320: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 321: train loss = 0.0003, val loss = 0.1002\n",
      "Epoch 322: train loss = 0.0003, val loss = 0.0999\n",
      "Epoch 323: train loss = 0.0003, val loss = 0.1000\n",
      "Epoch 324: train loss = 0.0003, val loss = 0.1015\n",
      "Epoch 325: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 326: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 327: train loss = 0.0004, val loss = 0.0993\n",
      "Epoch 328: train loss = 0.0003, val loss = 0.1014\n",
      "Epoch 329: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 330: train loss = 0.0004, val loss = 0.0997\n",
      "Epoch 331: train loss = 0.0004, val loss = 0.0995\n",
      "Epoch 332: train loss = 0.0004, val loss = 0.0996\n",
      "Epoch 333: train loss = 0.0004, val loss = 0.0993\n",
      "Epoch 334: train loss = 0.0003, val loss = 0.1001\n",
      "Epoch 335: train loss = 0.0003, val loss = 0.0995\n",
      "Epoch 336: train loss = 0.0002, val loss = 0.0999\n",
      "Epoch 337: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 338: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 339: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 340: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 341: train loss = 0.0002, val loss = 0.1001\n",
      "Epoch 342: train loss = 0.0002, val loss = 0.1003\n",
      "Epoch 343: train loss = 0.0002, val loss = 0.1000\n",
      "Epoch 344: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 345: train loss = 0.0002, val loss = 0.1006\n",
      "Epoch 346: train loss = 0.0002, val loss = 0.0999\n",
      "Epoch 347: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 348: train loss = 0.0001, val loss = 0.0998\n",
      "Epoch 349: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 350: train loss = 0.0002, val loss = 0.0995\n",
      "Epoch 351: train loss = 0.0002, val loss = 0.1002\n",
      "Epoch 352: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 353: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 354: train loss = 0.0002, val loss = 0.0993\n",
      "Epoch 355: train loss = 0.0003, val loss = 0.0987\n",
      "Epoch 356: train loss = 0.0003, val loss = 0.0996\n",
      "Epoch 357: train loss = 0.0005, val loss = 0.0977\n",
      "Epoch 358: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 359: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 360: train loss = 0.0005, val loss = 0.0993\n",
      "Epoch 361: train loss = 0.0006, val loss = 0.0977\n",
      "Epoch 362: train loss = 0.0007, val loss = 0.1002\n",
      "Epoch 363: train loss = 0.0008, val loss = 0.0983\n",
      "Epoch 364: train loss = 0.0008, val loss = 0.1002\n",
      "Epoch 365: train loss = 0.0010, val loss = 0.0981\n",
      "Epoch 366: train loss = 0.0008, val loss = 0.0984\n",
      "Epoch 367: train loss = 0.0006, val loss = 0.0982\n",
      "Epoch 368: train loss = 0.0006, val loss = 0.0982\n",
      "Epoch 369: train loss = 0.0005, val loss = 0.0986\n",
      "Epoch 370: train loss = 0.0005, val loss = 0.0979\n",
      "Epoch 371: train loss = 0.0004, val loss = 0.0982\n",
      "Epoch 372: train loss = 0.0004, val loss = 0.0983\n",
      "Epoch 373: train loss = 0.0003, val loss = 0.0975\n",
      "Epoch 374: train loss = 0.0003, val loss = 0.0978\n",
      "Epoch 375: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 376: train loss = 0.0002, val loss = 0.0977\n",
      "Epoch 377: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 378: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 379: train loss = 0.0002, val loss = 0.0978\n",
      "Epoch 380: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 381: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 382: train loss = 0.0002, val loss = 0.0989\n",
      "Epoch 383: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 384: train loss = 0.0001, val loss = 0.0985\n",
      "Epoch 385: train loss = 0.0001, val loss = 0.0993\n",
      "Epoch 386: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 387: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 388: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 389: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 390: train loss = 0.0002, val loss = 0.0987\n",
      "Epoch 391: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 392: train loss = 0.0002, val loss = 0.0981\n",
      "Epoch 393: train loss = 0.0002, val loss = 0.0985\n",
      "Epoch 394: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 395: train loss = 0.0002, val loss = 0.0979\n",
      "Epoch 396: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 397: train loss = 0.0002, val loss = 0.1002\n",
      "Epoch 398: train loss = 0.0003, val loss = 0.0988\n",
      "Epoch 399: train loss = 0.0003, val loss = 0.0987\n",
      "Epoch 400: train loss = 0.0004, val loss = 0.0975\n",
      "Epoch 401: train loss = 0.0005, val loss = 0.0991\n",
      "Epoch 402: train loss = 0.0005, val loss = 0.0989\n",
      "Epoch 403: train loss = 0.0005, val loss = 0.0979\n",
      "Epoch 404: train loss = 0.0005, val loss = 0.0973\n",
      "Epoch 405: train loss = 0.0005, val loss = 0.0986\n",
      "Epoch 406: train loss = 0.0004, val loss = 0.0979\n",
      "Epoch 407: train loss = 0.0004, val loss = 0.0995\n",
      "Epoch 408: train loss = 0.0003, val loss = 0.0985\n",
      "Epoch 409: train loss = 0.0003, val loss = 0.0976\n",
      "Epoch 410: train loss = 0.0002, val loss = 0.0985\n",
      "Epoch 411: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 412: train loss = 0.0002, val loss = 0.0978\n",
      "Epoch 413: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 414: train loss = 0.0002, val loss = 0.0980\n",
      "Epoch 415: train loss = 0.0002, val loss = 0.0984\n",
      "Epoch 416: train loss = 0.0001, val loss = 0.0993\n",
      "Epoch 417: train loss = 0.0001, val loss = 0.0990\n",
      "Epoch 418: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 419: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 420: train loss = 0.0001, val loss = 0.0987\n",
      "Epoch 421: train loss = 0.0001, val loss = 0.0988\n",
      "Epoch 422: train loss = 0.0001, val loss = 0.0992\n",
      "Epoch 423: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 424: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 425: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 426: train loss = 0.0002, val loss = 0.0999\n",
      "Epoch 427: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 428: train loss = 0.0002, val loss = 0.0988\n",
      "Epoch 429: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 430: train loss = 0.0002, val loss = 0.0987\n",
      "Epoch 431: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 432: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 433: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 434: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 435: train loss = 0.0001, val loss = 0.0998\n",
      "Epoch 436: train loss = 0.0001, val loss = 0.0984\n",
      "Epoch 437: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 438: train loss = 0.0001, val loss = 0.0983\n",
      "Epoch 439: train loss = 0.0001, val loss = 0.0988\n",
      "Epoch 440: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 441: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 442: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 443: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 444: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 445: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 446: train loss = 0.0003, val loss = 0.0975\n",
      "Epoch 447: train loss = 0.0003, val loss = 0.0994\n",
      "Epoch 448: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 449: train loss = 0.0004, val loss = 0.0983\n",
      "Epoch 450: train loss = 0.0004, val loss = 0.0965\n",
      "Epoch 451: train loss = 0.0004, val loss = 0.0984\n",
      "Epoch 452: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 453: train loss = 0.0004, val loss = 0.0979\n",
      "Epoch 454: train loss = 0.0004, val loss = 0.0968\n",
      "Epoch 455: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 456: train loss = 0.0003, val loss = 0.0993\n",
      "Epoch 457: train loss = 0.0003, val loss = 0.0995\n",
      "Epoch 458: train loss = 0.0004, val loss = 0.0978\n",
      "Epoch 459: train loss = 0.0004, val loss = 0.0989\n",
      "Epoch 460: train loss = 0.0004, val loss = 0.0964\n",
      "Epoch 461: train loss = 0.0004, val loss = 0.0968\n",
      "Epoch 462: train loss = 0.0004, val loss = 0.0985\n",
      "Epoch 463: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 464: train loss = 0.0003, val loss = 0.0982\n",
      "Epoch 465: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 466: train loss = 0.0003, val loss = 0.0975\n",
      "Epoch 467: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 468: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 469: train loss = 0.0004, val loss = 0.0990\n",
      "Epoch 470: train loss = 0.0004, val loss = 0.0974\n",
      "Epoch 471: train loss = 0.0004, val loss = 0.0978\n",
      "Epoch 472: train loss = 0.0005, val loss = 0.0974\n",
      "Epoch 473: train loss = 0.0005, val loss = 0.0975\n",
      "Epoch 474: train loss = 0.0005, val loss = 0.0973\n",
      "Epoch 475: train loss = 0.0005, val loss = 0.0973\n",
      "Epoch 476: train loss = 0.0005, val loss = 0.0965\n",
      "Epoch 477: train loss = 0.0005, val loss = 0.0990\n",
      "Epoch 478: train loss = 0.0005, val loss = 0.0968\n",
      "Epoch 479: train loss = 0.0005, val loss = 0.0976\n",
      "Epoch 480: train loss = 0.0004, val loss = 0.0974\n",
      "Epoch 481: train loss = 0.0003, val loss = 0.0978\n",
      "Epoch 482: train loss = 0.0003, val loss = 0.0978\n",
      "Epoch 483: train loss = 0.0004, val loss = 0.0978\n",
      "Epoch 484: train loss = 0.0003, val loss = 0.0970\n",
      "Epoch 485: train loss = 0.0003, val loss = 0.0968\n",
      "Epoch 486: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 487: train loss = 0.0003, val loss = 0.0974\n",
      "Epoch 488: train loss = 0.0002, val loss = 0.0980\n",
      "Epoch 489: train loss = 0.0002, val loss = 0.0977\n",
      "Epoch 490: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 491: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 492: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 493: train loss = 0.0002, val loss = 0.0984\n",
      "Epoch 494: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 495: train loss = 0.0002, val loss = 0.0975\n",
      "Epoch 496: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 497: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 498: train loss = 0.0002, val loss = 0.0976\n",
      "Epoch 499: train loss = 0.0003, val loss = 0.0980\n",
      "Epoch 500: train loss = 0.0003, val loss = 0.0986\n",
      "Epoch 501: train loss = 0.0003, val loss = 0.0963\n",
      "Epoch 502: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 503: train loss = 0.0004, val loss = 0.0967\n",
      "Epoch 504: train loss = 0.0004, val loss = 0.0961\n",
      "Epoch 505: train loss = 0.0004, val loss = 0.0979\n",
      "Epoch 506: train loss = 0.0004, val loss = 0.0969\n",
      "Epoch 507: train loss = 0.0005, val loss = 0.0970\n",
      "Epoch 508: train loss = 0.0005, val loss = 0.0984\n",
      "Epoch 509: train loss = 0.0006, val loss = 0.0966\n",
      "Epoch 510: train loss = 0.0006, val loss = 0.0962\n",
      "Epoch 511: train loss = 0.0006, val loss = 0.0986\n",
      "Epoch 512: train loss = 0.0007, val loss = 0.0962\n",
      "Epoch 513: train loss = 0.0007, val loss = 0.0984\n",
      "Epoch 514: train loss = 0.0007, val loss = 0.0949\n",
      "Epoch 515: train loss = 0.0007, val loss = 0.0978\n",
      "Epoch 516: train loss = 0.0006, val loss = 0.0971\n",
      "Epoch 517: train loss = 0.0005, val loss = 0.0955\n",
      "Epoch 518: train loss = 0.0004, val loss = 0.0954\n",
      "Epoch 519: train loss = 0.0003, val loss = 0.0968\n",
      "Epoch 520: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 521: train loss = 0.0003, val loss = 0.0986\n",
      "Epoch 522: train loss = 0.0002, val loss = 0.0974\n",
      "Epoch 523: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 524: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 525: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 526: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 527: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 528: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 529: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 530: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 531: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 532: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 533: train loss = 0.0000, val loss = 0.0971\n",
      "Epoch 534: train loss = 0.0000, val loss = 0.0979\n",
      "Epoch 535: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 536: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 537: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 538: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 539: train loss = 0.0000, val loss = 0.0978\n",
      "Epoch 540: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 541: train loss = 0.0000, val loss = 0.0976\n",
      "Epoch 542: train loss = 0.0000, val loss = 0.0976\n",
      "Epoch 543: train loss = 0.0000, val loss = 0.0974\n",
      "Epoch 544: train loss = 0.0000, val loss = 0.0980\n",
      "Epoch 545: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 546: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 547: train loss = 0.0000, val loss = 0.0977\n",
      "Epoch 548: train loss = 0.0000, val loss = 0.0975\n",
      "Epoch 549: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 550: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 551: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 552: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 553: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 554: train loss = 0.0001, val loss = 0.0983\n",
      "Epoch 555: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 556: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 557: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 558: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 559: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 560: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 561: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 562: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 563: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 564: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 565: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 566: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 567: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 568: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 569: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 570: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 571: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 572: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 573: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 574: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 575: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 576: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 577: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 578: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 579: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 580: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 581: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 582: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 583: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 584: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 585: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 586: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 587: train loss = 0.0002, val loss = 0.0967\n",
      "Epoch 588: train loss = 0.0003, val loss = 0.0971\n",
      "Epoch 589: train loss = 0.0003, val loss = 0.0985\n",
      "Epoch 590: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 591: train loss = 0.0004, val loss = 0.0983\n",
      "Epoch 592: train loss = 0.0004, val loss = 0.0956\n",
      "Epoch 593: train loss = 0.0005, val loss = 0.0977\n",
      "Epoch 594: train loss = 0.0006, val loss = 0.0960\n",
      "Epoch 595: train loss = 0.0007, val loss = 0.0997\n",
      "Epoch 596: train loss = 0.0008, val loss = 0.0953\n",
      "Epoch 597: train loss = 0.0008, val loss = 0.0976\n",
      "Epoch 598: train loss = 0.0008, val loss = 0.0972\n",
      "Epoch 599: train loss = 0.0007, val loss = 0.0938\n",
      "Epoch 600: train loss = 0.0007, val loss = 0.0943\n",
      "Epoch 601: train loss = 0.0007, val loss = 0.0952\n",
      "Epoch 602: train loss = 0.0007, val loss = 0.0964\n",
      "Epoch 603: train loss = 0.0007, val loss = 0.0980\n",
      "Epoch 604: train loss = 0.0009, val loss = 0.0944\n",
      "Epoch 605: train loss = 0.0008, val loss = 0.0981\n",
      "Epoch 606: train loss = 0.0007, val loss = 0.0964\n",
      "Epoch 607: train loss = 0.0006, val loss = 0.0977\n",
      "Epoch 608: train loss = 0.0006, val loss = 0.0965\n",
      "Epoch 609: train loss = 0.0005, val loss = 0.0965\n",
      "Epoch 610: train loss = 0.0005, val loss = 0.0980\n",
      "Epoch 611: train loss = 0.0005, val loss = 0.0974\n",
      "Epoch 612: train loss = 0.0004, val loss = 0.0958\n",
      "Epoch 613: train loss = 0.0004, val loss = 0.0952\n",
      "Epoch 614: train loss = 0.0004, val loss = 0.0975\n",
      "Epoch 615: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 616: train loss = 0.0003, val loss = 0.0955\n",
      "Epoch 617: train loss = 0.0003, val loss = 0.0956\n",
      "Epoch 618: train loss = 0.0003, val loss = 0.0962\n",
      "Epoch 619: train loss = 0.0003, val loss = 0.0976\n",
      "Epoch 620: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 621: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 622: train loss = 0.0002, val loss = 0.0976\n",
      "Epoch 623: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 624: train loss = 0.0002, val loss = 0.0963\n",
      "Epoch 625: train loss = 0.0002, val loss = 0.0965\n",
      "Epoch 626: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 627: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 628: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 629: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 630: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 631: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 632: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 633: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 634: train loss = 0.0000, val loss = 0.0964\n",
      "Epoch 635: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 636: train loss = 0.0000, val loss = 0.0965\n",
      "Epoch 637: train loss = 0.0000, val loss = 0.0965\n",
      "Epoch 638: train loss = 0.0000, val loss = 0.0971\n",
      "Epoch 639: train loss = 0.0000, val loss = 0.0961\n",
      "Epoch 640: train loss = 0.0000, val loss = 0.0964\n",
      "Epoch 641: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 642: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 643: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 644: train loss = 0.0000, val loss = 0.0965\n",
      "Epoch 645: train loss = 0.0000, val loss = 0.0970\n",
      "Epoch 646: train loss = 0.0000, val loss = 0.0964\n",
      "Epoch 647: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 648: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 649: train loss = 0.0000, val loss = 0.0969\n",
      "Epoch 650: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 651: train loss = 0.0000, val loss = 0.0969\n",
      "Epoch 652: train loss = 0.0000, val loss = 0.0970\n",
      "Epoch 653: train loss = 0.0000, val loss = 0.0969\n",
      "Epoch 654: train loss = 0.0000, val loss = 0.0971\n",
      "Epoch 655: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 656: train loss = 0.0000, val loss = 0.0972\n",
      "Epoch 657: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 658: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 659: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 660: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 661: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 662: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 663: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 664: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 665: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 666: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 667: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 668: train loss = 0.0002, val loss = 0.0969\n",
      "Epoch 669: train loss = 0.0003, val loss = 0.0977\n",
      "Epoch 670: train loss = 0.0004, val loss = 0.0965\n",
      "Epoch 671: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 672: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 673: train loss = 0.0004, val loss = 0.0952\n",
      "Epoch 674: train loss = 0.0004, val loss = 0.0958\n",
      "Epoch 675: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 676: train loss = 0.0005, val loss = 0.0964\n",
      "Epoch 677: train loss = 0.0005, val loss = 0.0965\n",
      "Epoch 678: train loss = 0.0005, val loss = 0.0978\n",
      "Epoch 679: train loss = 0.0005, val loss = 0.0961\n",
      "Epoch 680: train loss = 0.0004, val loss = 0.0970\n",
      "Epoch 681: train loss = 0.0005, val loss = 0.0955\n",
      "Epoch 682: train loss = 0.0005, val loss = 0.0955\n",
      "Epoch 683: train loss = 0.0005, val loss = 0.0945\n",
      "Epoch 684: train loss = 0.0005, val loss = 0.0971\n",
      "Epoch 685: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 686: train loss = 0.0004, val loss = 0.0964\n",
      "Epoch 687: train loss = 0.0005, val loss = 0.0946\n",
      "Epoch 688: train loss = 0.0005, val loss = 0.0964\n",
      "Epoch 689: train loss = 0.0005, val loss = 0.0951\n",
      "Epoch 690: train loss = 0.0004, val loss = 0.0959\n",
      "Epoch 691: train loss = 0.0004, val loss = 0.0954\n",
      "Epoch 692: train loss = 0.0004, val loss = 0.0962\n",
      "Epoch 693: train loss = 0.0005, val loss = 0.0954\n",
      "Epoch 694: train loss = 0.0005, val loss = 0.0977\n",
      "Epoch 695: train loss = 0.0005, val loss = 0.0968\n",
      "Epoch 696: train loss = 0.0005, val loss = 0.0946\n",
      "Epoch 697: train loss = 0.0006, val loss = 0.0957\n",
      "Epoch 698: train loss = 0.0005, val loss = 0.0984\n",
      "Epoch 699: train loss = 0.0004, val loss = 0.0947\n",
      "Epoch 700: train loss = 0.0004, val loss = 0.0964\n",
      "Epoch 701: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 702: train loss = 0.0003, val loss = 0.0970\n",
      "Epoch 703: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 704: train loss = 0.0003, val loss = 0.0955\n",
      "Epoch 705: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 706: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 707: train loss = 0.0003, val loss = 0.0959\n",
      "Epoch 708: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 709: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 710: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 711: train loss = 0.0001, val loss = 0.0955\n",
      "Epoch 712: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 713: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 714: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 715: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 716: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 717: train loss = 0.0001, val loss = 0.0955\n",
      "Epoch 718: train loss = 0.0000, val loss = 0.0961\n",
      "Epoch 719: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 720: train loss = 0.0000, val loss = 0.0955\n",
      "Epoch 721: train loss = 0.0000, val loss = 0.0963\n",
      "Epoch 722: train loss = 0.0000, val loss = 0.0955\n",
      "Epoch 723: train loss = 0.0000, val loss = 0.0963\n",
      "Epoch 724: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 725: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 726: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 727: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 728: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 729: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 730: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 731: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 732: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 733: train loss = 0.0000, val loss = 0.0961\n",
      "Epoch 734: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 735: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 736: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 737: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 738: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 739: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 740: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 741: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 742: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 743: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 744: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 745: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 746: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 747: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 748: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 749: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 750: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 751: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 752: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 753: train loss = 0.0002, val loss = 0.0965\n",
      "Epoch 754: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 755: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 756: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 757: train loss = 0.0003, val loss = 0.0962\n",
      "Epoch 758: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 759: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 760: train loss = 0.0003, val loss = 0.0957\n",
      "Epoch 761: train loss = 0.0003, val loss = 0.0971\n",
      "Epoch 762: train loss = 0.0003, val loss = 0.0964\n",
      "Epoch 763: train loss = 0.0003, val loss = 0.0957\n",
      "Epoch 764: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 765: train loss = 0.0004, val loss = 0.0966\n",
      "Epoch 766: train loss = 0.0003, val loss = 0.0961\n",
      "Epoch 767: train loss = 0.0003, val loss = 0.0957\n",
      "Epoch 768: train loss = 0.0003, val loss = 0.0941\n",
      "Epoch 769: train loss = 0.0003, val loss = 0.0965\n",
      "Epoch 770: train loss = 0.0002, val loss = 0.0961\n",
      "Epoch 771: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 772: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 773: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 774: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 775: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 776: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 777: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 778: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 779: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 780: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 781: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 782: train loss = 0.0003, val loss = 0.0976\n",
      "Epoch 783: train loss = 0.0002, val loss = 0.0943\n",
      "Epoch 784: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 785: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 786: train loss = 0.0002, val loss = 0.0975\n",
      "Epoch 787: train loss = 0.0003, val loss = 0.0960\n",
      "Epoch 788: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 789: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 790: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 791: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 792: train loss = 0.0003, val loss = 0.0956\n",
      "Epoch 793: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 794: train loss = 0.0003, val loss = 0.0973\n",
      "Epoch 795: train loss = 0.0002, val loss = 0.0941\n",
      "Epoch 796: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 797: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 798: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 799: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 800: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 801: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 802: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 803: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 804: train loss = 0.0002, val loss = 0.0944\n",
      "Epoch 805: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 806: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 807: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 808: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 809: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 810: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 811: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 812: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 813: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 814: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 815: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 816: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 817: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 818: train loss = 0.0001, val loss = 0.0949\n",
      "Epoch 819: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 820: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 821: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 822: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 823: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 824: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 825: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 826: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 827: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 828: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 829: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 830: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 831: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 832: train loss = 0.0001, val loss = 0.0947\n",
      "Epoch 833: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 834: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 835: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 836: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 837: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 838: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 839: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 840: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 841: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 842: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 843: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 844: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 845: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 846: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 847: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 848: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 849: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 850: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 851: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 852: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 853: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 854: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 855: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 856: train loss = 0.0002, val loss = 0.0967\n",
      "Epoch 857: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 858: train loss = 0.0003, val loss = 0.0956\n",
      "Epoch 859: train loss = 0.0003, val loss = 0.0944\n",
      "Epoch 860: train loss = 0.0003, val loss = 0.0977\n",
      "Epoch 861: train loss = 0.0004, val loss = 0.0936\n",
      "Epoch 862: train loss = 0.0004, val loss = 0.0963\n",
      "Epoch 863: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 864: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 865: train loss = 0.0005, val loss = 0.0950\n",
      "Epoch 866: train loss = 0.0005, val loss = 0.0954\n",
      "Epoch 867: train loss = 0.0006, val loss = 0.0943\n",
      "Epoch 868: train loss = 0.0005, val loss = 0.0966\n",
      "Epoch 869: train loss = 0.0006, val loss = 0.0940\n",
      "Epoch 870: train loss = 0.0006, val loss = 0.0963\n",
      "Epoch 871: train loss = 0.0005, val loss = 0.0947\n",
      "Epoch 872: train loss = 0.0004, val loss = 0.0943\n",
      "Epoch 873: train loss = 0.0004, val loss = 0.0936\n",
      "Epoch 874: train loss = 0.0003, val loss = 0.0959\n",
      "Epoch 875: train loss = 0.0004, val loss = 0.0939\n",
      "Epoch 876: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 877: train loss = 0.0002, val loss = 0.0965\n",
      "Epoch 878: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 879: train loss = 0.0002, val loss = 0.0974\n",
      "Epoch 880: train loss = 0.0002, val loss = 0.0942\n",
      "Epoch 881: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 882: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 883: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 884: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 885: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 886: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 887: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 888: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 889: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 890: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 891: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 892: train loss = 0.0001, val loss = 0.0947\n",
      "Epoch 893: train loss = 0.0001, val loss = 0.0950\n",
      "Epoch 894: train loss = 0.0002, val loss = 0.0949\n",
      "Epoch 895: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 896: train loss = 0.0002, val loss = 0.0938\n",
      "Epoch 897: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 898: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 899: train loss = 0.0002, val loss = 0.0949\n",
      "Epoch 900: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 901: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 902: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 903: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 904: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 905: train loss = 0.0002, val loss = 0.0946\n",
      "Epoch 906: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 907: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 908: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 909: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 910: train loss = 0.0001, val loss = 0.0951\n",
      "Epoch 911: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 912: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 913: train loss = 0.0000, val loss = 0.0951\n",
      "Epoch 914: train loss = 0.0000, val loss = 0.0956\n",
      "Epoch 915: train loss = 0.0000, val loss = 0.0956\n",
      "Epoch 916: train loss = 0.0000, val loss = 0.0952\n",
      "Epoch 917: train loss = 0.0000, val loss = 0.0951\n",
      "Epoch 918: train loss = 0.0000, val loss = 0.0954\n",
      "Epoch 919: train loss = 0.0000, val loss = 0.0951\n",
      "Epoch 920: train loss = 0.0000, val loss = 0.0963\n",
      "Epoch 921: train loss = 0.0000, val loss = 0.0956\n",
      "Epoch 922: train loss = 0.0000, val loss = 0.0948\n",
      "Epoch 923: train loss = 0.0000, val loss = 0.0957\n",
      "Epoch 924: train loss = 0.0000, val loss = 0.0957\n",
      "Epoch 925: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 926: train loss = 0.0000, val loss = 0.0950\n",
      "Epoch 927: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 928: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 929: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 930: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 931: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 932: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 933: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 934: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 935: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 936: train loss = 0.0001, val loss = 0.0955\n",
      "Epoch 937: train loss = 0.0001, val loss = 0.0946\n",
      "Epoch 938: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 939: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 940: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 941: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 942: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 943: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 944: train loss = 0.0001, val loss = 0.0944\n",
      "Epoch 945: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 946: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 947: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 948: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 949: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 950: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 951: train loss = 0.0002, val loss = 0.0944\n",
      "Epoch 952: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 953: train loss = 0.0002, val loss = 0.0944\n",
      "Epoch 954: train loss = 0.0003, val loss = 0.0962\n",
      "Epoch 955: train loss = 0.0004, val loss = 0.0952\n",
      "Epoch 956: train loss = 0.0004, val loss = 0.0934\n",
      "Epoch 957: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 958: train loss = 0.0004, val loss = 0.0971\n",
      "Epoch 959: train loss = 0.0005, val loss = 0.0933\n",
      "Epoch 960: train loss = 0.0004, val loss = 0.0945\n",
      "Epoch 961: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 962: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 963: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 964: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 965: train loss = 0.0001, val loss = 0.0943\n",
      "Epoch 966: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 967: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 968: train loss = 0.0001, val loss = 0.0940\n",
      "Epoch 969: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 970: train loss = 0.0001, val loss = 0.0944\n",
      "Epoch 971: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 972: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 973: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 974: train loss = 0.0001, val loss = 0.0951\n",
      "Epoch 975: train loss = 0.0001, val loss = 0.0941\n",
      "Epoch 976: train loss = 0.0001, val loss = 0.0950\n",
      "Epoch 977: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 978: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 979: train loss = 0.0001, val loss = 0.0943\n",
      "Epoch 980: train loss = 0.0001, val loss = 0.0951\n",
      "Epoch 981: train loss = 0.0001, val loss = 0.0950\n",
      "Epoch 982: train loss = 0.0001, val loss = 0.0946\n",
      "Epoch 983: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 984: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 985: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 986: train loss = 0.0001, val loss = 0.0949\n",
      "Epoch 987: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 988: train loss = 0.0001, val loss = 0.0949\n",
      "Epoch 989: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 990: train loss = 0.0001, val loss = 0.0943\n",
      "Epoch 991: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 992: train loss = 0.0001, val loss = 0.0946\n",
      "Epoch 993: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 994: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 995: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 996: train loss = 0.0002, val loss = 0.0942\n",
      "Epoch 997: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 998: train loss = 0.0002, val loss = 0.0956\n",
      "Epoch 999: train loss = 0.0002, val loss = 0.0947\n",
      "Epoch 1000: train loss = 0.0002, val loss = 0.0955\n"
     ]
    }
   ],
   "source": [
    "model = train_final_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e0c8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mood_predictions(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83af6f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1562\n"
     ]
    }
   ],
   "source": [
    "from mood_RNN_classifier import get_accuracy_rate\n",
    "accuracy = get_accuracy_rate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39a1aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  predicted_mood_class\n",
      "0   AS14.16                    31\n",
      "1   AS14.02                    25\n",
      "2   AS14.12                    22\n",
      "3   AS14.30                    32\n",
      "4   AS14.19                    29\n",
      "5   AS14.09                    29\n",
      "6   AS14.29                    28\n",
      "7   AS14.01                    33\n",
      "8   AS14.14                    30\n",
      "9   AS14.25                    28\n",
      "10  AS14.07                    27\n",
      "11  AS14.23                    30\n",
      "12  AS14.20                    24\n",
      "13  AS14.15                    33\n",
      "14  AS14.17                    28\n",
      "15  AS14.31                    28\n",
      "16  AS14.26                    27\n",
      "17  AS14.33                    29\n",
      "18  AS14.13                    28\n",
      "19  AS14.24                    28\n",
      "20  AS14.08                    28\n",
      "21  AS14.27                    28\n",
      "22  AS14.32                    28\n",
      "23  AS14.03                    32\n",
      "24  AS14.06                    28\n",
      "25  AS14.05                    30\n",
      "26  AS14.28                    28\n"
     ]
    }
   ],
   "source": [
    "# Run predictions on test_df\n",
    "test_predictions = predict(model, test_df, id_map, device)\n",
    "\n",
    "# Attach predictions to test_df\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted_mood_class'] = test_predictions\n",
    "\n",
    "# Optional: save to CSV or examine\n",
    "print(test_df_with_preds[['id', 'predicted_mood_class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3933caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "        31, 32, 33, 34, 35, 36, 38], dtype=int32),\n",
       " mood\n",
       " 28    301\n",
       " 29    138\n",
       " 32    128\n",
       " 30    124\n",
       " 26    103\n",
       " 27     98\n",
       " 31     98\n",
       " 24     66\n",
       " 25     65\n",
       " 33     21\n",
       " 22     17\n",
       " 23     17\n",
       " 34     13\n",
       " 20      7\n",
       " 35      6\n",
       " 21      5\n",
       " 36      4\n",
       " 14      3\n",
       " 18      3\n",
       " 19      2\n",
       " 15      2\n",
       " 12      1\n",
       " 16      1\n",
       " 38      1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sort_values('mood')\n",
    "train_df['mood'].unique(), train_df['mood'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
