{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60ca131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the path (/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1) to sys.path\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ./initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf5c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from data_loading import DataPreprocessor\n",
    "from mood_RNN_classifier import RNNClassifier, MoodDataset, OrdinalLabelSmoothingLoss, split_train_val, objective, train_epoch, train_final_model, evaluate, predict, plot_mood_predictions\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c42a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataPreprocessor()\n",
    "train_df, test_df = data_loader.load_and_preprocess_data(\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6928e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume train_df and test_df are loaded and preprocessed\n",
    "id_map = {id_: idx for idx, id_ in enumerate(train_df['id'].unique())}\n",
    "input_dim = train_df.drop(columns=['id', 'mood', 'date']).shape[1]\n",
    "id_count = len(id_map)\n",
    "output_dim = train_df['mood'].max() + 1\n",
    "\n",
    "train_df_split, val_df_split = split_train_val(train_df, fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c89292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 20:37:04,691] A new study created in memory with name: no-name-85b43180-378f-4c0e-b075-dc9d4924e46a\n",
      "[I 2025-04-17 20:37:09,868] Trial 0 finished with value: 0.22316743433475494 and parameters: {'hidden_dim': 63, 'id_embed_dim': 5, 'lr': 0.0049639865353721425, 'batch_size': 64, 'alpha': 0.04456595327275025}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:09,994] Trial 1 finished with value: 0.5566116571426392 and parameters: {'hidden_dim': 57, 'id_embed_dim': 16, 'lr': 0.0016419874840177464, 'batch_size': 128, 'alpha': 0.10412491869201401}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,227] Trial 2 finished with value: 0.4580661691725254 and parameters: {'hidden_dim': 36, 'id_embed_dim': 8, 'lr': 0.004214208282860192, 'batch_size': 32, 'alpha': 0.1144483469299451}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,460] Trial 3 finished with value: 1.4468481838703156 and parameters: {'hidden_dim': 48, 'id_embed_dim': 15, 'lr': 0.00012847477898188172, 'batch_size': 32, 'alpha': 0.15952204213141782}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,639] Trial 4 finished with value: 0.559240072965622 and parameters: {'hidden_dim': 109, 'id_embed_dim': 11, 'lr': 0.00285847636000973, 'batch_size': 64, 'alpha': 0.14133919374307705}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:10,900] Trial 5 finished with value: 0.3959162849932909 and parameters: {'hidden_dim': 94, 'id_embed_dim': 9, 'lr': 0.000959004775064577, 'batch_size': 32, 'alpha': 0.07691987547508385}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,154] Trial 6 finished with value: 0.5475961491465569 and parameters: {'hidden_dim': 90, 'id_embed_dim': 10, 'lr': 0.0009305727342693942, 'batch_size': 32, 'alpha': 0.1294617525351189}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,278] Trial 7 finished with value: 1.0174967050552368 and parameters: {'hidden_dim': 54, 'id_embed_dim': 12, 'lr': 0.0015246137802307094, 'batch_size': 128, 'alpha': 0.27467109750960694}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,518] Trial 8 finished with value: 0.3998469691723585 and parameters: {'hidden_dim': 57, 'id_embed_dim': 13, 'lr': 0.008393230184686289, 'batch_size': 32, 'alpha': 0.07733352254597115}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,768] Trial 9 finished with value: 1.049925409257412 and parameters: {'hidden_dim': 96, 'id_embed_dim': 15, 'lr': 0.0002470537425886334, 'batch_size': 32, 'alpha': 0.19342570626978045}. Best is trial 0 with value: 0.22316743433475494.\n",
      "[I 2025-04-17 20:37:11,952] Trial 10 finished with value: 0.1018451415002346 and parameters: {'hidden_dim': 125, 'id_embed_dim': 4, 'lr': 0.009116117543999267, 'batch_size': 64, 'alpha': 0.017935477351516377}. Best is trial 10 with value: 0.1018451415002346.\n",
      "[I 2025-04-17 20:37:12,134] Trial 11 finished with value: 0.06147644203156233 and parameters: {'hidden_dim': 128, 'id_embed_dim': 4, 'lr': 0.008759792413482445, 'batch_size': 64, 'alpha': 0.01070053856604862}. Best is trial 11 with value: 0.06147644203156233.\n",
      "[I 2025-04-17 20:37:12,323] Trial 12 finished with value: 0.05555207468569279 and parameters: {'hidden_dim': 126, 'id_embed_dim': 4, 'lr': 0.009723243485482763, 'batch_size': 64, 'alpha': 0.01039165986937}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:12,512] Trial 13 finished with value: 0.3668849319219589 and parameters: {'hidden_dim': 126, 'id_embed_dim': 6, 'lr': 0.00038319311099752816, 'batch_size': 64, 'alpha': 0.01803616878395828}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:12,690] Trial 14 finished with value: 0.6974070072174072 and parameters: {'hidden_dim': 112, 'id_embed_dim': 7, 'lr': 0.005700538186922354, 'batch_size': 64, 'alpha': 0.2341684716924466}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:12,870] Trial 15 finished with value: 0.324625700712204 and parameters: {'hidden_dim': 112, 'id_embed_dim': 4, 'lr': 0.0025488009305596, 'batch_size': 64, 'alpha': 0.06251628951415193}. Best is trial 12 with value: 0.05555207468569279.\n",
      "[I 2025-04-17 20:37:13,050] Trial 16 finished with value: 0.05536685138940811 and parameters: {'hidden_dim': 79, 'id_embed_dim': 6, 'lr': 0.009930284719147366, 'batch_size': 64, 'alpha': 0.010955559852850221}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,226] Trial 17 finished with value: 0.4988250583410263 and parameters: {'hidden_dim': 74, 'id_embed_dim': 6, 'lr': 0.0006252286941811653, 'batch_size': 64, 'alpha': 0.04388138393101973}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,356] Trial 18 finished with value: 0.6931120455265045 and parameters: {'hidden_dim': 76, 'id_embed_dim': 7, 'lr': 0.0029314477938574843, 'batch_size': 128, 'alpha': 0.1921392448052608}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,534] Trial 19 finished with value: 0.420554481446743 and parameters: {'hidden_dim': 84, 'id_embed_dim': 6, 'lr': 0.0057387334553696875, 'batch_size': 64, 'alpha': 0.09427768177308873}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,714] Trial 20 finished with value: 0.26087402552366257 and parameters: {'hidden_dim': 102, 'id_embed_dim': 8, 'lr': 0.001988994116096351, 'batch_size': 64, 'alpha': 0.044878590958239456}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:13,898] Trial 21 finished with value: 0.06413324177265167 and parameters: {'hidden_dim': 119, 'id_embed_dim': 4, 'lr': 0.0073368607440338724, 'batch_size': 64, 'alpha': 0.011846065487222753}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,070] Trial 22 finished with value: 0.05565113667398691 and parameters: {'hidden_dim': 70, 'id_embed_dim': 5, 'lr': 0.00997520346590688, 'batch_size': 64, 'alpha': 0.010158583398398013}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,243] Trial 23 finished with value: 0.2660074643790722 and parameters: {'hidden_dim': 68, 'id_embed_dim': 5, 'lr': 0.004197223125338242, 'batch_size': 64, 'alpha': 0.04342911575304767}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,423] Trial 24 finished with value: 0.3306473195552826 and parameters: {'hidden_dim': 85, 'id_embed_dim': 5, 'lr': 0.0097167946819779, 'batch_size': 64, 'alpha': 0.07071447202375256}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,560] Trial 25 finished with value: 0.21617744117975235 and parameters: {'hidden_dim': 68, 'id_embed_dim': 7, 'lr': 0.003912400210109566, 'batch_size': 128, 'alpha': 0.03317413910782839}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,749] Trial 26 finished with value: 0.3042144775390625 and parameters: {'hidden_dim': 80, 'id_embed_dim': 5, 'lr': 0.006227090453544527, 'batch_size': 64, 'alpha': 0.06127471442641197}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:14,923] Trial 27 finished with value: 0.16927653178572655 and parameters: {'hidden_dim': 39, 'id_embed_dim': 9, 'lr': 0.006649160220888407, 'batch_size': 64, 'alpha': 0.032283342237739834}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,116] Trial 28 finished with value: 0.6607734262943268 and parameters: {'hidden_dim': 102, 'id_embed_dim': 6, 'lr': 0.0035005476099699917, 'batch_size': 64, 'alpha': 0.17658555527897957}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,300] Trial 29 finished with value: 0.8906346559524536 and parameters: {'hidden_dim': 67, 'id_embed_dim': 8, 'lr': 0.005008206668819342, 'batch_size': 64, 'alpha': 0.2976309885174498}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,435] Trial 30 finished with value: 1.3775851726531982 and parameters: {'hidden_dim': 75, 'id_embed_dim': 5, 'lr': 0.00011131643757060093, 'batch_size': 128, 'alpha': 0.09441402428081767}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,625] Trial 31 finished with value: 0.08706812746822834 and parameters: {'hidden_dim': 118, 'id_embed_dim': 4, 'lr': 0.008309291417593059, 'batch_size': 64, 'alpha': 0.0151719934274589}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,809] Trial 32 finished with value: 0.06259273085743189 and parameters: {'hidden_dim': 128, 'id_embed_dim': 4, 'lr': 0.009978012670993654, 'batch_size': 64, 'alpha': 0.010061884513249313}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:15,981] Trial 33 finished with value: 0.19907968491315842 and parameters: {'hidden_dim': 61, 'id_embed_dim': 5, 'lr': 0.00657413070160299, 'batch_size': 64, 'alpha': 0.034048527533920625}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,166] Trial 34 finished with value: 0.3062892146408558 and parameters: {'hidden_dim': 119, 'id_embed_dim': 6, 'lr': 0.004761498591740868, 'batch_size': 64, 'alpha': 0.05789074837651728}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,355] Trial 35 finished with value: 0.16827015951275826 and parameters: {'hidden_dim': 104, 'id_embed_dim': 4, 'lr': 0.007583168662560991, 'batch_size': 64, 'alpha': 0.030106047205182066}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,529] Trial 36 finished with value: 0.28724484518170357 and parameters: {'hidden_dim': 42, 'id_embed_dim': 7, 'lr': 0.009914170249512907, 'batch_size': 64, 'alpha': 0.05347435066686643}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,676] Trial 37 finished with value: 0.6882201731204987 and parameters: {'hidden_dim': 88, 'id_embed_dim': 5, 'lr': 0.00140295320176409, 'batch_size': 128, 'alpha': 0.08584669716052898}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:16,941] Trial 38 finished with value: 0.5062135756015778 and parameters: {'hidden_dim': 95, 'id_embed_dim': 6, 'lr': 0.002294469321109287, 'batch_size': 32, 'alpha': 0.12959847542515063}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,106] Trial 39 finished with value: 0.44425083696842194 and parameters: {'hidden_dim': 48, 'id_embed_dim': 9, 'lr': 0.003467994104801525, 'batch_size': 64, 'alpha': 0.10673677939340484}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,357] Trial 40 finished with value: 0.153709358535707 and parameters: {'hidden_dim': 63, 'id_embed_dim': 8, 'lr': 0.00481805492642966, 'batch_size': 32, 'alpha': 0.026767100387599257}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,548] Trial 41 finished with value: 0.11302952654659748 and parameters: {'hidden_dim': 128, 'id_embed_dim': 4, 'lr': 0.007769729117487667, 'batch_size': 64, 'alpha': 0.019684021861826186}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,737] Trial 42 finished with value: 0.05648484732955694 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.009825678354526446, 'batch_size': 64, 'alpha': 0.010266545326245108}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:17,925] Trial 43 finished with value: 0.26384858787059784 and parameters: {'hidden_dim': 122, 'id_embed_dim': 4, 'lr': 0.007283738338370547, 'batch_size': 64, 'alpha': 0.044273368798980015}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,110] Trial 44 finished with value: 0.762955829501152 and parameters: {'hidden_dim': 110, 'id_embed_dim': 5, 'lr': 0.005789970478190222, 'batch_size': 64, 'alpha': 0.23885823001968826}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,339] Trial 45 finished with value: 0.05621304828673601 and parameters: {'hidden_dim': 122, 'id_embed_dim': 10, 'lr': 0.008756333164861916, 'batch_size': 64, 'alpha': 0.010432801168808375}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,524] Trial 46 finished with value: 0.624946340918541 and parameters: {'hidden_dim': 115, 'id_embed_dim': 11, 'lr': 0.00021645533144171747, 'batch_size': 64, 'alpha': 0.026340910199197754}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,788] Trial 47 finished with value: 0.38894139789044857 and parameters: {'hidden_dim': 107, 'id_embed_dim': 14, 'lr': 0.0006342763983333198, 'batch_size': 32, 'alpha': 0.0755028804209961}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:18,973] Trial 48 finished with value: 0.2500622980296612 and parameters: {'hidden_dim': 122, 'id_embed_dim': 12, 'lr': 0.005252859479777643, 'batch_size': 64, 'alpha': 0.051626824840157404}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,116] Trial 49 finished with value: 0.21327482163906097 and parameters: {'hidden_dim': 115, 'id_embed_dim': 10, 'lr': 0.008344913381255753, 'batch_size': 128, 'alpha': 0.039471777443053066}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,293] Trial 50 finished with value: 0.12452213652431965 and parameters: {'hidden_dim': 81, 'id_embed_dim': 16, 'lr': 0.0030605415292296323, 'batch_size': 64, 'alpha': 0.023841023224252807}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,480] Trial 51 finished with value: 0.05639892816543579 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.008400546263323459, 'batch_size': 64, 'alpha': 0.010238484605193793}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,665] Trial 52 finished with value: 0.13186048530042171 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.006730970434684805, 'batch_size': 64, 'alpha': 0.021132081437540414}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:19,850] Trial 53 finished with value: 0.06888443324714899 and parameters: {'hidden_dim': 115, 'id_embed_dim': 5, 'lr': 0.009949593966263989, 'batch_size': 64, 'alpha': 0.010991476280675823}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,039] Trial 54 finished with value: 0.341814287006855 and parameters: {'hidden_dim': 123, 'id_embed_dim': 11, 'lr': 0.008256304127431849, 'batch_size': 64, 'alpha': 0.06597479619770283}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,212] Trial 55 finished with value: 0.5782202258706093 and parameters: {'hidden_dim': 70, 'id_embed_dim': 7, 'lr': 0.006106626497902465, 'batch_size': 64, 'alpha': 0.15107362069574082}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,393] Trial 56 finished with value: 0.20185798406600952 and parameters: {'hidden_dim': 91, 'id_embed_dim': 13, 'lr': 0.004263599714508849, 'batch_size': 64, 'alpha': 0.039453808304277985}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,576] Trial 57 finished with value: 0.2933184653520584 and parameters: {'hidden_dim': 99, 'id_embed_dim': 6, 'lr': 0.008891271080099787, 'batch_size': 64, 'alpha': 0.05297863712779189}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:20,844] Trial 58 finished with value: 0.1428525522351265 and parameters: {'hidden_dim': 107, 'id_embed_dim': 5, 'lr': 0.00708308341936109, 'batch_size': 32, 'alpha': 0.022943602027970193}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,014] Trial 59 finished with value: 0.10879471898078918 and parameters: {'hidden_dim': 55, 'id_embed_dim': 6, 'lr': 0.0012664202061740074, 'batch_size': 64, 'alpha': 0.010653877727459182}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,203] Trial 60 finished with value: 0.21048878505825996 and parameters: {'hidden_dim': 125, 'id_embed_dim': 4, 'lr': 0.005697975628653981, 'batch_size': 64, 'alpha': 0.033385118728693736}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,390] Trial 61 finished with value: 0.12178081646561623 and parameters: {'hidden_dim': 118, 'id_embed_dim': 4, 'lr': 0.008177931093611559, 'batch_size': 64, 'alpha': 0.019190171841690157}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,573] Trial 62 finished with value: 0.21307244524359703 and parameters: {'hidden_dim': 128, 'id_embed_dim': 5, 'lr': 0.008745844394833559, 'batch_size': 64, 'alpha': 0.03948552845607741}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,747] Trial 63 finished with value: 0.17184165865182877 and parameters: {'hidden_dim': 78, 'id_embed_dim': 4, 'lr': 0.009927815475980843, 'batch_size': 64, 'alpha': 0.026455628570633438}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:21,934] Trial 64 finished with value: 0.05993795022368431 and parameters: {'hidden_dim': 121, 'id_embed_dim': 5, 'lr': 0.00689935907692606, 'batch_size': 64, 'alpha': 0.011671484969369799}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,115] Trial 65 finished with value: 0.25783900171518326 and parameters: {'hidden_dim': 112, 'id_embed_dim': 15, 'lr': 0.006718610343814702, 'batch_size': 64, 'alpha': 0.04732047971930044}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,248] Trial 66 finished with value: 0.7708775997161865 and parameters: {'hidden_dim': 71, 'id_embed_dim': 5, 'lr': 0.007435536296321056, 'batch_size': 128, 'alpha': 0.23711177218629545}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,432] Trial 67 finished with value: 0.10629190690815449 and parameters: {'hidden_dim': 116, 'id_embed_dim': 5, 'lr': 0.0054610981155523005, 'batch_size': 64, 'alpha': 0.017154105150882042}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,619] Trial 68 finished with value: 0.207490012049675 and parameters: {'hidden_dim': 121, 'id_embed_dim': 6, 'lr': 0.004644731081792112, 'batch_size': 64, 'alpha': 0.033816862322997986}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,783] Trial 69 finished with value: 1.1484081447124481 and parameters: {'hidden_dim': 32, 'id_embed_dim': 7, 'lr': 0.0007772477786342009, 'batch_size': 64, 'alpha': 0.21425401916947281}. Best is trial 16 with value: 0.05536685138940811.\n",
      "[I 2025-04-17 20:37:22,971] Trial 70 finished with value: 0.051440260373055935 and parameters: {'hidden_dim': 125, 'id_embed_dim': 10, 'lr': 0.008844073033562001, 'batch_size': 64, 'alpha': 0.011230085926425425}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,158] Trial 71 finished with value: 0.06483070272952318 and parameters: {'hidden_dim': 125, 'id_embed_dim': 10, 'lr': 0.008957022628983636, 'batch_size': 64, 'alpha': 0.01043840051283027}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,348] Trial 72 finished with value: 0.12975355237722397 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.0062400218468134025, 'batch_size': 64, 'alpha': 0.021546278165679715}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,534] Trial 73 finished with value: 0.5211273729801178 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.00037706860760636777, 'batch_size': 64, 'alpha': 0.031169231894628783}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,717] Trial 74 finished with value: 0.09577189013361931 and parameters: {'hidden_dim': 120, 'id_embed_dim': 4, 'lr': 0.007389538177617447, 'batch_size': 64, 'alpha': 0.016653697470470682}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:23,902] Trial 75 finished with value: 0.059726348146796227 and parameters: {'hidden_dim': 113, 'id_embed_dim': 10, 'lr': 0.008835495910413384, 'batch_size': 64, 'alpha': 0.010200116210348583}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,084] Trial 76 finished with value: 0.2095451056957245 and parameters: {'hidden_dim': 112, 'id_embed_dim': 12, 'lr': 0.008670812326750373, 'batch_size': 64, 'alpha': 0.03750654112316294}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,340] Trial 77 finished with value: 0.1761326091364026 and parameters: {'hidden_dim': 83, 'id_embed_dim': 9, 'lr': 0.0099259477162702, 'batch_size': 32, 'alpha': 0.02781674807233452}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,469] Trial 78 finished with value: 0.26632437109947205 and parameters: {'hidden_dim': 64, 'id_embed_dim': 10, 'lr': 0.0036825758157731513, 'batch_size': 128, 'alpha': 0.049487753210005814}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,640] Trial 79 finished with value: 0.09831168316304684 and parameters: {'hidden_dim': 72, 'id_embed_dim': 10, 'lr': 0.008114936005826617, 'batch_size': 64, 'alpha': 0.017775012248496234}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:24,830] Trial 80 finished with value: 0.3058706223964691 and parameters: {'hidden_dim': 124, 'id_embed_dim': 11, 'lr': 0.006192622930155039, 'batch_size': 64, 'alpha': 0.06010433847869577}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,015] Trial 81 finished with value: 0.052613225765526295 and parameters: {'hidden_dim': 120, 'id_embed_dim': 8, 'lr': 0.007231094930573221, 'batch_size': 64, 'alpha': 0.010098681772663948}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,204] Trial 82 finished with value: 0.1442696377635002 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.009063713055381846, 'batch_size': 64, 'alpha': 0.026682832175969957}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,386] Trial 83 finished with value: 0.051656538620591164 and parameters: {'hidden_dim': 128, 'id_embed_dim': 8, 'lr': 0.0075294991585114525, 'batch_size': 64, 'alpha': 0.010206491488676736}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,569] Trial 84 finished with value: 0.10502895899116993 and parameters: {'hidden_dim': 128, 'id_embed_dim': 8, 'lr': 0.005190314765674963, 'batch_size': 64, 'alpha': 0.01912367060665684}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,752] Trial 85 finished with value: 0.2412664331495762 and parameters: {'hidden_dim': 120, 'id_embed_dim': 7, 'lr': 0.007712178002742582, 'batch_size': 64, 'alpha': 0.04263924977221345}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:25,935] Trial 86 finished with value: 0.15927043929696083 and parameters: {'hidden_dim': 77, 'id_embed_dim': 9, 'lr': 0.007452558192094029, 'batch_size': 64, 'alpha': 0.03091918091885756}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,123] Trial 87 finished with value: 0.5280983224511147 and parameters: {'hidden_dim': 126, 'id_embed_dim': 8, 'lr': 0.0018194647722065806, 'batch_size': 64, 'alpha': 0.12286002021792271}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,309] Trial 88 finished with value: 0.1690085120499134 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.004284617031979871, 'batch_size': 64, 'alpha': 0.024138404640477014}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,486] Trial 89 finished with value: 0.7620307803153992 and parameters: {'hidden_dim': 86, 'id_embed_dim': 6, 'lr': 0.005954173975885699, 'batch_size': 64, 'alpha': 0.2622175259959613}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,758] Trial 90 finished with value: 0.10942597966641188 and parameters: {'hidden_dim': 118, 'id_embed_dim': 8, 'lr': 0.0066950732815876865, 'batch_size': 32, 'alpha': 0.016784287378305146}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:26,946] Trial 91 finished with value: 0.059697492979466915 and parameters: {'hidden_dim': 115, 'id_embed_dim': 10, 'lr': 0.008951044064300204, 'batch_size': 64, 'alpha': 0.011699831204134377}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,216] Trial 92 finished with value: 0.6090877801179886 and parameters: {'hidden_dim': 109, 'id_embed_dim': 9, 'lr': 0.007993397965357371, 'batch_size': 64, 'alpha': 0.17045157450177437}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,401] Trial 93 finished with value: 0.09811156801879406 and parameters: {'hidden_dim': 128, 'id_embed_dim': 9, 'lr': 0.009279680616981596, 'batch_size': 64, 'alpha': 0.015263240419761584}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,587] Trial 94 finished with value: 0.8092063665390015 and parameters: {'hidden_dim': 120, 'id_embed_dim': 10, 'lr': 0.0001529354390673257, 'batch_size': 64, 'alpha': 0.037473656733363746}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,773] Trial 95 finished with value: 0.14140361547470093 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.009308926062824751, 'batch_size': 64, 'alpha': 0.024014081201971044}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:27,959] Trial 96 finished with value: 0.05683023761957884 and parameters: {'hidden_dim': 118, 'id_embed_dim': 10, 'lr': 0.007847668518056568, 'batch_size': 64, 'alpha': 0.010016271198901961}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,103] Trial 97 finished with value: 0.15159650146961212 and parameters: {'hidden_dim': 118, 'id_embed_dim': 12, 'lr': 0.007064437776123548, 'batch_size': 128, 'alpha': 0.03044670787209806}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,276] Trial 98 finished with value: 0.11919774487614632 and parameters: {'hidden_dim': 73, 'id_embed_dim': 5, 'lr': 0.00811994303179023, 'batch_size': 64, 'alpha': 0.02185217800457917}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,464] Trial 99 finished with value: 0.08479415997862816 and parameters: {'hidden_dim': 126, 'id_embed_dim': 7, 'lr': 0.009979199085238206, 'batch_size': 64, 'alpha': 0.015631973197691273}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,650] Trial 100 finished with value: 0.19536328688263893 and parameters: {'hidden_dim': 122, 'id_embed_dim': 4, 'lr': 0.005485115263021394, 'batch_size': 64, 'alpha': 0.033788518550011894}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:28,821] Trial 101 finished with value: 0.0623770859092474 and parameters: {'hidden_dim': 60, 'id_embed_dim': 10, 'lr': 0.00636480376742213, 'batch_size': 64, 'alpha': 0.011970151921424213}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,005] Trial 102 finished with value: 0.11838086135685444 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.00778910209524276, 'batch_size': 64, 'alpha': 0.021575857611973854}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,188] Trial 103 finished with value: 0.05408777203410864 and parameters: {'hidden_dim': 116, 'id_embed_dim': 9, 'lr': 0.008785813487703143, 'batch_size': 64, 'alpha': 0.010387851449657561}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,361] Trial 104 finished with value: 0.15692763403058052 and parameters: {'hidden_dim': 66, 'id_embed_dim': 8, 'lr': 0.006748068112492149, 'batch_size': 64, 'alpha': 0.028457977749848425}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,543] Trial 105 finished with value: 0.09015006572008133 and parameters: {'hidden_dim': 92, 'id_embed_dim': 9, 'lr': 0.008418457325466071, 'batch_size': 64, 'alpha': 0.015415489977318389}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,730] Trial 106 finished with value: 0.24438275396823883 and parameters: {'hidden_dim': 117, 'id_embed_dim': 9, 'lr': 0.007374934165076103, 'batch_size': 64, 'alpha': 0.04500515599563171}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:29,914] Trial 107 finished with value: 0.055488359183073044 and parameters: {'hidden_dim': 120, 'id_embed_dim': 11, 'lr': 0.009259071446640246, 'batch_size': 64, 'alpha': 0.01012406757861981}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,103] Trial 108 finished with value: 0.19923456385731697 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.009089225991716689, 'batch_size': 64, 'alpha': 0.03620764771802616}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,288] Trial 109 finished with value: 0.1425265371799469 and parameters: {'hidden_dim': 124, 'id_embed_dim': 5, 'lr': 0.00996389199700516, 'batch_size': 64, 'alpha': 0.02399780152618844}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,542] Trial 110 finished with value: 0.1049658334814012 and parameters: {'hidden_dim': 79, 'id_embed_dim': 8, 'lr': 0.004683682065754066, 'batch_size': 32, 'alpha': 0.018721914166454363}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,734] Trial 111 finished with value: 0.0617807749658823 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.008147056945449161, 'batch_size': 64, 'alpha': 0.011673795581766202}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:30,917] Trial 112 finished with value: 0.05365587119013071 and parameters: {'hidden_dim': 111, 'id_embed_dim': 10, 'lr': 0.007094695071862744, 'batch_size': 64, 'alpha': 0.010407544700953399}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,104] Trial 113 finished with value: 0.13983620330691338 and parameters: {'hidden_dim': 120, 'id_embed_dim': 9, 'lr': 0.006952315102169736, 'batch_size': 64, 'alpha': 0.027489525471284602}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,287] Trial 114 finished with value: 0.10455440171062946 and parameters: {'hidden_dim': 111, 'id_embed_dim': 11, 'lr': 0.006020846146492399, 'batch_size': 64, 'alpha': 0.019888082402370784}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,475] Trial 115 finished with value: 0.09726104140281677 and parameters: {'hidden_dim': 123, 'id_embed_dim': 4, 'lr': 0.008703971060801608, 'batch_size': 64, 'alpha': 0.01668663923054104}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,659] Trial 116 finished with value: 0.4586992785334587 and parameters: {'hidden_dim': 116, 'id_embed_dim': 4, 'lr': 0.0004165303276020622, 'batch_size': 64, 'alpha': 0.025365045217407498}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,844] Trial 117 finished with value: 0.21301452815532684 and parameters: {'hidden_dim': 108, 'id_embed_dim': 12, 'lr': 0.009349589016335341, 'batch_size': 64, 'alpha': 0.04112794933316675}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:31,984] Trial 118 finished with value: 0.05161529406905174 and parameters: {'hidden_dim': 128, 'id_embed_dim': 10, 'lr': 0.006474238289639191, 'batch_size': 128, 'alpha': 0.010273284449224871}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,124] Trial 119 finished with value: 0.16531460732221603 and parameters: {'hidden_dim': 105, 'id_embed_dim': 10, 'lr': 0.005336260515213286, 'batch_size': 128, 'alpha': 0.03205181559752514}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,268] Trial 120 finished with value: 0.08465771749615669 and parameters: {'hidden_dim': 126, 'id_embed_dim': 10, 'lr': 0.007227592659310704, 'batch_size': 128, 'alpha': 0.016266937490808518}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,412] Trial 121 finished with value: 0.1255403310060501 and parameters: {'hidden_dim': 127, 'id_embed_dim': 9, 'lr': 0.00656310377446087, 'batch_size': 128, 'alpha': 0.02200772966283972}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,555] Trial 122 finished with value: 0.05349251069128513 and parameters: {'hidden_dim': 122, 'id_embed_dim': 11, 'lr': 0.008489682790677747, 'batch_size': 128, 'alpha': 0.010458047215296962}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,699] Trial 123 finished with value: 0.054100871086120605 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.008475492188014515, 'batch_size': 128, 'alpha': 0.010351592456744576}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,843] Trial 124 finished with value: 0.0869884192943573 and parameters: {'hidden_dim': 119, 'id_embed_dim': 11, 'lr': 0.007644926532639134, 'batch_size': 128, 'alpha': 0.0154538167344406}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:32,978] Trial 125 finished with value: 0.14334271848201752 and parameters: {'hidden_dim': 75, 'id_embed_dim': 11, 'lr': 0.0058175357549638184, 'batch_size': 128, 'alpha': 0.02718655752781548}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,121] Trial 126 finished with value: 0.11283138021826744 and parameters: {'hidden_dim': 121, 'id_embed_dim': 12, 'lr': 0.008330059617605285, 'batch_size': 128, 'alpha': 0.02067049223131956}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,261] Trial 127 finished with value: 0.11557397991418839 and parameters: {'hidden_dim': 128, 'id_embed_dim': 11, 'lr': 0.0011961001373963847, 'batch_size': 128, 'alpha': 0.0108750578022667}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,405] Trial 128 finished with value: 0.19316331297159195 and parameters: {'hidden_dim': 124, 'id_embed_dim': 10, 'lr': 0.008969530793784392, 'batch_size': 128, 'alpha': 0.03507438240927843}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,550] Trial 129 finished with value: 0.05220457911491394 and parameters: {'hidden_dim': 114, 'id_embed_dim': 11, 'lr': 0.006481188122078644, 'batch_size': 128, 'alpha': 0.010168246495813047}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,691] Trial 130 finished with value: 0.09440067037940025 and parameters: {'hidden_dim': 97, 'id_embed_dim': 11, 'lr': 0.005052910753362683, 'batch_size': 128, 'alpha': 0.017771118585101223}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,826] Trial 131 finished with value: 0.054690705612301826 and parameters: {'hidden_dim': 82, 'id_embed_dim': 11, 'lr': 0.006388565106139582, 'batch_size': 128, 'alpha': 0.01043470832306722}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:33,955] Trial 132 finished with value: 0.1266445480287075 and parameters: {'hidden_dim': 48, 'id_embed_dim': 12, 'lr': 0.006631209861241481, 'batch_size': 128, 'alpha': 0.02353417917236858}. Best is trial 70 with value: 0.051440260373055935.\n",
      "[I 2025-04-17 20:37:34,090] Trial 133 finished with value: 0.05090113542973995 and parameters: {'hidden_dim': 81, 'id_embed_dim': 11, 'lr': 0.007275992778172948, 'batch_size': 128, 'alpha': 0.010078136576243972}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,225] Trial 134 finished with value: 0.08768726512789726 and parameters: {'hidden_dim': 82, 'id_embed_dim': 11, 'lr': 0.006070573569936727, 'batch_size': 128, 'alpha': 0.015648231844048878}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,365] Trial 135 finished with value: 0.16624315828084946 and parameters: {'hidden_dim': 87, 'id_embed_dim': 13, 'lr': 0.0076536517667785955, 'batch_size': 128, 'alpha': 0.03030065302704323}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,508] Trial 136 finished with value: 0.06472321227192879 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.002415266659827341, 'batch_size': 128, 'alpha': 0.010482263499157143}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,645] Trial 137 finished with value: 0.12118079885840416 and parameters: {'hidden_dim': 84, 'id_embed_dim': 11, 'lr': 0.0055315415197884415, 'batch_size': 128, 'alpha': 0.021299009790294286}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,788] Trial 138 finished with value: 0.08092004060745239 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.007108110671844618, 'batch_size': 128, 'alpha': 0.015981044594742908}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:34,925] Trial 139 finished with value: 0.15860337018966675 and parameters: {'hidden_dim': 81, 'id_embed_dim': 12, 'lr': 0.004481902814131419, 'batch_size': 128, 'alpha': 0.028002046011718625}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,069] Trial 140 finished with value: 0.11584538966417313 and parameters: {'hidden_dim': 125, 'id_embed_dim': 10, 'lr': 0.0064741675869571785, 'batch_size': 128, 'alpha': 0.021086958976777266}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,209] Trial 141 finished with value: 0.05720410495996475 and parameters: {'hidden_dim': 70, 'id_embed_dim': 11, 'lr': 0.00991054133775717, 'batch_size': 128, 'alpha': 0.010548050151411328}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,347] Trial 142 finished with value: 0.6459639668464661 and parameters: {'hidden_dim': 89, 'id_embed_dim': 11, 'lr': 0.00800214695491973, 'batch_size': 128, 'alpha': 0.1972589442861588}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,479] Trial 143 finished with value: 0.07903076708316803 and parameters: {'hidden_dim': 76, 'id_embed_dim': 10, 'lr': 0.009179360814912604, 'batch_size': 128, 'alpha': 0.015203730247797215}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,622] Trial 144 finished with value: 0.13921266794204712 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.007369967348885671, 'batch_size': 128, 'alpha': 0.024810926538758694}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,754] Trial 145 finished with value: 0.08689390495419502 and parameters: {'hidden_dim': 58, 'id_embed_dim': 10, 'lr': 0.00847486687044206, 'batch_size': 128, 'alpha': 0.01642601322269499}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:35,898] Trial 146 finished with value: 0.10970665886998177 and parameters: {'hidden_dim': 115, 'id_embed_dim': 11, 'lr': 0.0069876369936395415, 'batch_size': 128, 'alpha': 0.020724731206204695}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,039] Trial 147 finished with value: 0.15838222950696945 and parameters: {'hidden_dim': 128, 'id_embed_dim': 11, 'lr': 0.006171605240084891, 'batch_size': 128, 'alpha': 0.030539199305943045}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,169] Trial 148 finished with value: 0.058270152658224106 and parameters: {'hidden_dim': 53, 'id_embed_dim': 12, 'lr': 0.008319594306217117, 'batch_size': 128, 'alpha': 0.011841416892421113}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,423] Trial 149 finished with value: 0.42758868262171745 and parameters: {'hidden_dim': 79, 'id_embed_dim': 10, 'lr': 0.009237679486967692, 'batch_size': 32, 'alpha': 0.08942445497423067}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,568] Trial 150 finished with value: 0.48056089878082275 and parameters: {'hidden_dim': 122, 'id_embed_dim': 6, 'lr': 0.007642463576190737, 'batch_size': 128, 'alpha': 0.10497598511803088}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,799] Trial 151 finished with value: 0.07247111853212118 and parameters: {'hidden_dim': 124, 'id_embed_dim': 10, 'lr': 0.008589530545470798, 'batch_size': 64, 'alpha': 0.011701147136901376}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:36,943] Trial 152 finished with value: 0.05259040556848049 and parameters: {'hidden_dim': 121, 'id_embed_dim': 9, 'lr': 0.009414109318818378, 'batch_size': 128, 'alpha': 0.010322931985936036}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,087] Trial 153 finished with value: 0.09702982380986214 and parameters: {'hidden_dim': 121, 'id_embed_dim': 9, 'lr': 0.009886653688139924, 'batch_size': 128, 'alpha': 0.017702812468014452}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,232] Trial 154 finished with value: 0.05513021908700466 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.007968598842272042, 'batch_size': 128, 'alpha': 0.010449140854516595}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,377] Trial 155 finished with value: 0.05857452191412449 and parameters: {'hidden_dim': 114, 'id_embed_dim': 8, 'lr': 0.006672539417249449, 'batch_size': 128, 'alpha': 0.010011778383335598}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,519] Trial 156 finished with value: 0.14669062942266464 and parameters: {'hidden_dim': 110, 'id_embed_dim': 7, 'lr': 0.007596131527995706, 'batch_size': 128, 'alpha': 0.024860203843522596}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,663] Trial 157 finished with value: 0.09143709391355515 and parameters: {'hidden_dim': 119, 'id_embed_dim': 9, 'lr': 0.007101067170284066, 'batch_size': 128, 'alpha': 0.01692644177688149}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,806] Trial 158 finished with value: 0.1308250054717064 and parameters: {'hidden_dim': 116, 'id_embed_dim': 8, 'lr': 0.005666301064924639, 'batch_size': 128, 'alpha': 0.023341014040730745}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:37,951] Trial 159 finished with value: 0.05205344595015049 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.008037387972004074, 'batch_size': 128, 'alpha': 0.01026022767799177}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,095] Trial 160 finished with value: 0.19356537610292435 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.006268427253045301, 'batch_size': 128, 'alpha': 0.03708208476323883}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,239] Trial 161 finished with value: 0.08927670121192932 and parameters: {'hidden_dim': 122, 'id_embed_dim': 8, 'lr': 0.008134299363326745, 'batch_size': 128, 'alpha': 0.015934113990517862}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,384] Trial 162 finished with value: 0.06054350174963474 and parameters: {'hidden_dim': 126, 'id_embed_dim': 9, 'lr': 0.00907468810631862, 'batch_size': 128, 'alpha': 0.010783176504340339}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,527] Trial 163 finished with value: 0.05360429920256138 and parameters: {'hidden_dim': 120, 'id_embed_dim': 8, 'lr': 0.007777035865963799, 'batch_size': 128, 'alpha': 0.010188637406594446}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,670] Trial 164 finished with value: 0.54863041639328 and parameters: {'hidden_dim': 120, 'id_embed_dim': 8, 'lr': 0.007705892630040006, 'batch_size': 128, 'alpha': 0.14191405310713326}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,814] Trial 165 finished with value: 0.11340060830116272 and parameters: {'hidden_dim': 118, 'id_embed_dim': 7, 'lr': 0.007196721861867506, 'batch_size': 128, 'alpha': 0.01945293459301741}. Best is trial 133 with value: 0.05090113542973995.\n",
      "[I 2025-04-17 20:37:38,959] Trial 166 finished with value: 0.05008469149470329 and parameters: {'hidden_dim': 123, 'id_embed_dim': 8, 'lr': 0.008485165951808546, 'batch_size': 128, 'alpha': 0.010445818879173832}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,104] Trial 167 finished with value: 0.8175391852855682 and parameters: {'hidden_dim': 123, 'id_embed_dim': 8, 'lr': 0.006541273374203244, 'batch_size': 128, 'alpha': 0.2924565112787347}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,247] Trial 168 finished with value: 0.13165225833654404 and parameters: {'hidden_dim': 117, 'id_embed_dim': 8, 'lr': 0.008276270195782755, 'batch_size': 128, 'alpha': 0.02005062227645945}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,391] Trial 169 finished with value: 0.1525122970342636 and parameters: {'hidden_dim': 124, 'id_embed_dim': 9, 'lr': 0.007042323225318343, 'batch_size': 128, 'alpha': 0.0288565693655648}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,537] Trial 170 finished with value: 0.05784558691084385 and parameters: {'hidden_dim': 127, 'id_embed_dim': 7, 'lr': 0.005790593354439077, 'batch_size': 128, 'alpha': 0.01017858938101399}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,683] Trial 171 finished with value: 0.054975297302007675 and parameters: {'hidden_dim': 121, 'id_embed_dim': 8, 'lr': 0.009008282233692133, 'batch_size': 128, 'alpha': 0.010176324583181316}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,829] Trial 172 finished with value: 0.0862882174551487 and parameters: {'hidden_dim': 121, 'id_embed_dim': 8, 'lr': 0.008663415120651998, 'batch_size': 128, 'alpha': 0.016167876731178074}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:39,973] Trial 173 finished with value: 0.05297590792179108 and parameters: {'hidden_dim': 122, 'id_embed_dim': 8, 'lr': 0.007990600382312066, 'batch_size': 128, 'alpha': 0.010076454207975764}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,119] Trial 174 finished with value: 0.06127040646970272 and parameters: {'hidden_dim': 122, 'id_embed_dim': 8, 'lr': 0.007799090731871563, 'batch_size': 128, 'alpha': 0.01005383603739881}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,263] Trial 175 finished with value: 0.08967378363013268 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.0068753487127741495, 'batch_size': 128, 'alpha': 0.016240364386087697}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,407] Trial 176 finished with value: 0.1335640847682953 and parameters: {'hidden_dim': 124, 'id_embed_dim': 8, 'lr': 0.00799381773916382, 'batch_size': 128, 'alpha': 0.024887454422873757}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,547] Trial 177 finished with value: 0.10670970007777214 and parameters: {'hidden_dim': 112, 'id_embed_dim': 9, 'lr': 0.005035526809767247, 'batch_size': 128, 'alpha': 0.02081229612395726}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,689] Trial 178 finished with value: 0.07823453471064568 and parameters: {'hidden_dim': 116, 'id_embed_dim': 8, 'lr': 0.008736218762670778, 'batch_size': 128, 'alpha': 0.015435621895675159}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,836] Trial 179 finished with value: 0.15515702962875366 and parameters: {'hidden_dim': 126, 'id_embed_dim': 7, 'lr': 0.0061704060140859406, 'batch_size': 128, 'alpha': 0.02485808187484984}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:40,981] Trial 180 finished with value: 0.08294307813048363 and parameters: {'hidden_dim': 121, 'id_embed_dim': 9, 'lr': 0.007412327123500602, 'batch_size': 128, 'alpha': 0.015934990705048578}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,132] Trial 181 finished with value: 0.059299347922205925 and parameters: {'hidden_dim': 128, 'id_embed_dim': 8, 'lr': 0.009276107678972929, 'batch_size': 128, 'alpha': 0.010337629836183644}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,277] Trial 182 finished with value: 0.10835566371679306 and parameters: {'hidden_dim': 118, 'id_embed_dim': 8, 'lr': 0.009961905893953418, 'batch_size': 128, 'alpha': 0.02023191537233988}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,424] Trial 183 finished with value: 0.09070723876357079 and parameters: {'hidden_dim': 125, 'id_embed_dim': 9, 'lr': 0.008386878051114807, 'batch_size': 128, 'alpha': 0.015499096760508299}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,568] Trial 184 finished with value: 0.08224388211965561 and parameters: {'hidden_dim': 123, 'id_embed_dim': 7, 'lr': 0.007757101913755014, 'batch_size': 128, 'alpha': 0.014866272065112231}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,711] Trial 185 finished with value: 0.05335717648267746 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.008940569539033673, 'batch_size': 128, 'alpha': 0.010023882581139017}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:41,857] Trial 186 finished with value: 0.052620502188801765 and parameters: {'hidden_dim': 116, 'id_embed_dim': 10, 'lr': 0.006828229023600156, 'batch_size': 128, 'alpha': 0.010273130270962182}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,010] Trial 187 finished with value: 0.11930907517671585 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.006765151075034555, 'batch_size': 128, 'alpha': 0.021091023428463442}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,156] Trial 188 finished with value: 0.1543382927775383 and parameters: {'hidden_dim': 113, 'id_embed_dim': 10, 'lr': 0.007254472660357189, 'batch_size': 128, 'alpha': 0.028588699621335197}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,300] Trial 189 finished with value: 0.10691815987229347 and parameters: {'hidden_dim': 116, 'id_embed_dim': 10, 'lr': 0.006259978849764696, 'batch_size': 128, 'alpha': 0.019517368565244456}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,440] Trial 190 finished with value: 0.055666208267211914 and parameters: {'hidden_dim': 110, 'id_embed_dim': 10, 'lr': 0.008780384573511792, 'batch_size': 128, 'alpha': 0.01036029627030003}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,586] Trial 191 finished with value: 0.09126255661249161 and parameters: {'hidden_dim': 119, 'id_embed_dim': 8, 'lr': 0.008028710027483014, 'batch_size': 128, 'alpha': 0.015117503773939141}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,729] Trial 192 finished with value: 0.05440042167901993 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.007466120712074496, 'batch_size': 128, 'alpha': 0.010838113872028935}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:42,877] Trial 193 finished with value: 0.08801053464412689 and parameters: {'hidden_dim': 123, 'id_embed_dim': 11, 'lr': 0.007134935735652707, 'batch_size': 128, 'alpha': 0.01621754134346199}. Best is trial 166 with value: 0.05008469149470329.\n",
      "[I 2025-04-17 20:37:43,027] Trial 194 finished with value: 0.04822372645139694 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.006456160517681038, 'batch_size': 128, 'alpha': 0.010000905656281955}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,173] Trial 195 finished with value: 0.1511356681585312 and parameters: {'hidden_dim': 101, 'id_embed_dim': 11, 'lr': 0.00584231340781724, 'batch_size': 128, 'alpha': 0.02601138894289126}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,321] Trial 196 finished with value: 0.11274676769971848 and parameters: {'hidden_dim': 117, 'id_embed_dim': 11, 'lr': 0.006405997200660003, 'batch_size': 128, 'alpha': 0.02118117394596252}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,471] Trial 197 finished with value: 0.08045604452490807 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.005156847399428343, 'batch_size': 128, 'alpha': 0.015550574982799995}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,617] Trial 198 finished with value: 0.12036525458097458 and parameters: {'hidden_dim': 114, 'id_embed_dim': 10, 'lr': 0.006719099199685368, 'batch_size': 128, 'alpha': 0.022338880647603927}. Best is trial 194 with value: 0.04822372645139694.\n",
      "[I 2025-04-17 20:37:43,892] Trial 199 finished with value: 0.0624529286287725 and parameters: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.00744227393327017, 'batch_size': 32, 'alpha': 0.010039802584321517}. Best is trial 194 with value: 0.04822372645139694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'hidden_dim': 121, 'id_embed_dim': 11, 'lr': 0.006456160517681038, 'batch_size': 128, 'alpha': 0.010000905656281955}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, train_df_split, val_df_split, id_map, input_dim, id_count, output_dim, device), n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparams:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c89fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "model = RNNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    id_count=id_count,\n",
    "    id_embed_dim=best_params['id_embed_dim'],\n",
    "    output_dim=output_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = OrdinalLabelSmoothingLoss(num_classes=output_dim, alpha=best_params['alpha'])\n",
    "\n",
    "train_loader = DataLoader(MoodDataset(train_df_split, id_map), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(MoodDataset(val_df_split, id_map), batch_size=best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0bfbe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.3811, val loss = 0.2817\n",
      "Epoch 2: train loss = 0.1403, val loss = 0.1171\n",
      "Epoch 3: train loss = 0.0815, val loss = 0.0823\n",
      "Epoch 4: train loss = 0.0624, val loss = 0.0661\n",
      "Epoch 5: train loss = 0.0534, val loss = 0.0604\n",
      "Epoch 6: train loss = 0.0485, val loss = 0.0571\n",
      "Epoch 7: train loss = 0.0457, val loss = 0.0565\n",
      "Epoch 8: train loss = 0.0432, val loss = 0.0549\n",
      "Epoch 9: train loss = 0.0411, val loss = 0.0524\n",
      "Epoch 10: train loss = 0.0389, val loss = 0.0546\n",
      "Epoch 11: train loss = 0.0374, val loss = 0.0531\n",
      "Epoch 12: train loss = 0.0355, val loss = 0.0529\n",
      "Epoch 13: train loss = 0.0340, val loss = 0.0546\n",
      "Epoch 14: train loss = 0.0319, val loss = 0.0536\n",
      "Epoch 15: train loss = 0.0309, val loss = 0.0544\n",
      "Epoch 16: train loss = 0.0297, val loss = 0.0570\n",
      "Epoch 17: train loss = 0.0278, val loss = 0.0579\n",
      "Epoch 18: train loss = 0.0268, val loss = 0.0587\n",
      "Epoch 19: train loss = 0.0256, val loss = 0.0587\n",
      "Epoch 20: train loss = 0.0243, val loss = 0.0611\n",
      "Epoch 21: train loss = 0.0232, val loss = 0.0631\n",
      "Epoch 22: train loss = 0.0227, val loss = 0.0635\n",
      "Epoch 23: train loss = 0.0211, val loss = 0.0648\n",
      "Epoch 24: train loss = 0.0206, val loss = 0.0637\n",
      "Epoch 25: train loss = 0.0192, val loss = 0.0666\n",
      "Epoch 26: train loss = 0.0183, val loss = 0.0674\n",
      "Epoch 27: train loss = 0.0174, val loss = 0.0683\n",
      "Epoch 28: train loss = 0.0169, val loss = 0.0691\n",
      "Epoch 29: train loss = 0.0157, val loss = 0.0714\n",
      "Epoch 30: train loss = 0.0152, val loss = 0.0707\n",
      "Epoch 31: train loss = 0.0143, val loss = 0.0756\n",
      "Epoch 32: train loss = 0.0136, val loss = 0.0738\n",
      "Epoch 33: train loss = 0.0131, val loss = 0.0769\n",
      "Epoch 34: train loss = 0.0127, val loss = 0.0755\n",
      "Epoch 35: train loss = 0.0120, val loss = 0.0769\n",
      "Epoch 36: train loss = 0.0115, val loss = 0.0781\n",
      "Epoch 37: train loss = 0.0107, val loss = 0.0767\n",
      "Epoch 38: train loss = 0.0102, val loss = 0.0802\n",
      "Epoch 39: train loss = 0.0099, val loss = 0.0792\n",
      "Epoch 40: train loss = 0.0098, val loss = 0.0778\n",
      "Epoch 41: train loss = 0.0093, val loss = 0.0804\n",
      "Epoch 42: train loss = 0.0087, val loss = 0.0810\n",
      "Epoch 43: train loss = 0.0083, val loss = 0.0792\n",
      "Epoch 44: train loss = 0.0078, val loss = 0.0811\n",
      "Epoch 45: train loss = 0.0075, val loss = 0.0834\n",
      "Epoch 46: train loss = 0.0070, val loss = 0.0824\n",
      "Epoch 47: train loss = 0.0069, val loss = 0.0820\n",
      "Epoch 48: train loss = 0.0063, val loss = 0.0808\n",
      "Epoch 49: train loss = 0.0059, val loss = 0.0832\n",
      "Epoch 50: train loss = 0.0056, val loss = 0.0841\n",
      "Epoch 51: train loss = 0.0054, val loss = 0.0838\n",
      "Epoch 52: train loss = 0.0052, val loss = 0.0846\n",
      "Epoch 53: train loss = 0.0047, val loss = 0.0831\n",
      "Epoch 54: train loss = 0.0046, val loss = 0.0839\n",
      "Epoch 55: train loss = 0.0046, val loss = 0.0849\n",
      "Epoch 56: train loss = 0.0042, val loss = 0.0858\n",
      "Epoch 57: train loss = 0.0040, val loss = 0.0855\n",
      "Epoch 58: train loss = 0.0039, val loss = 0.0869\n",
      "Epoch 59: train loss = 0.0040, val loss = 0.0860\n",
      "Epoch 60: train loss = 0.0036, val loss = 0.0875\n",
      "Epoch 61: train loss = 0.0036, val loss = 0.0887\n",
      "Epoch 62: train loss = 0.0037, val loss = 0.0872\n",
      "Epoch 63: train loss = 0.0037, val loss = 0.0892\n",
      "Epoch 64: train loss = 0.0034, val loss = 0.0876\n",
      "Epoch 65: train loss = 0.0033, val loss = 0.0896\n",
      "Epoch 66: train loss = 0.0033, val loss = 0.0897\n",
      "Epoch 67: train loss = 0.0031, val loss = 0.0896\n",
      "Epoch 68: train loss = 0.0029, val loss = 0.0894\n",
      "Epoch 69: train loss = 0.0028, val loss = 0.0896\n",
      "Epoch 70: train loss = 0.0025, val loss = 0.0922\n",
      "Epoch 71: train loss = 0.0025, val loss = 0.0913\n",
      "Epoch 72: train loss = 0.0024, val loss = 0.0907\n",
      "Epoch 73: train loss = 0.0022, val loss = 0.0905\n",
      "Epoch 74: train loss = 0.0021, val loss = 0.0926\n",
      "Epoch 75: train loss = 0.0021, val loss = 0.0925\n",
      "Epoch 76: train loss = 0.0021, val loss = 0.0922\n",
      "Epoch 77: train loss = 0.0019, val loss = 0.0919\n",
      "Epoch 78: train loss = 0.0019, val loss = 0.0956\n",
      "Epoch 79: train loss = 0.0018, val loss = 0.0914\n",
      "Epoch 80: train loss = 0.0019, val loss = 0.0944\n",
      "Epoch 81: train loss = 0.0018, val loss = 0.0939\n",
      "Epoch 82: train loss = 0.0017, val loss = 0.0934\n",
      "Epoch 83: train loss = 0.0017, val loss = 0.0954\n",
      "Epoch 84: train loss = 0.0016, val loss = 0.0953\n",
      "Epoch 85: train loss = 0.0015, val loss = 0.0937\n",
      "Epoch 86: train loss = 0.0014, val loss = 0.0952\n",
      "Epoch 87: train loss = 0.0012, val loss = 0.0944\n",
      "Epoch 88: train loss = 0.0013, val loss = 0.0958\n",
      "Epoch 89: train loss = 0.0011, val loss = 0.0954\n",
      "Epoch 90: train loss = 0.0012, val loss = 0.0954\n",
      "Epoch 91: train loss = 0.0012, val loss = 0.0962\n",
      "Epoch 92: train loss = 0.0011, val loss = 0.0969\n",
      "Epoch 93: train loss = 0.0011, val loss = 0.0954\n",
      "Epoch 94: train loss = 0.0010, val loss = 0.0973\n",
      "Epoch 95: train loss = 0.0009, val loss = 0.0949\n",
      "Epoch 96: train loss = 0.0009, val loss = 0.0963\n",
      "Epoch 97: train loss = 0.0009, val loss = 0.0969\n",
      "Epoch 98: train loss = 0.0008, val loss = 0.0966\n",
      "Epoch 99: train loss = 0.0009, val loss = 0.0976\n",
      "Epoch 100: train loss = 0.0009, val loss = 0.0977\n",
      "Epoch 101: train loss = 0.0009, val loss = 0.0991\n",
      "Epoch 102: train loss = 0.0009, val loss = 0.0991\n",
      "Epoch 103: train loss = 0.0009, val loss = 0.0984\n",
      "Epoch 104: train loss = 0.0008, val loss = 0.0980\n",
      "Epoch 105: train loss = 0.0008, val loss = 0.0968\n",
      "Epoch 106: train loss = 0.0007, val loss = 0.0984\n",
      "Epoch 107: train loss = 0.0007, val loss = 0.0986\n",
      "Epoch 108: train loss = 0.0007, val loss = 0.0978\n",
      "Epoch 109: train loss = 0.0007, val loss = 0.0988\n",
      "Epoch 110: train loss = 0.0007, val loss = 0.0972\n",
      "Epoch 111: train loss = 0.0006, val loss = 0.0994\n",
      "Epoch 112: train loss = 0.0007, val loss = 0.0993\n",
      "Epoch 113: train loss = 0.0006, val loss = 0.0987\n",
      "Epoch 114: train loss = 0.0006, val loss = 0.0988\n",
      "Epoch 115: train loss = 0.0006, val loss = 0.0992\n",
      "Epoch 116: train loss = 0.0006, val loss = 0.0993\n",
      "Epoch 117: train loss = 0.0006, val loss = 0.0992\n",
      "Epoch 118: train loss = 0.0006, val loss = 0.1004\n",
      "Epoch 119: train loss = 0.0006, val loss = 0.0974\n",
      "Epoch 120: train loss = 0.0007, val loss = 0.1013\n",
      "Epoch 121: train loss = 0.0006, val loss = 0.0992\n",
      "Epoch 122: train loss = 0.0006, val loss = 0.1014\n",
      "Epoch 123: train loss = 0.0005, val loss = 0.0995\n",
      "Epoch 124: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 125: train loss = 0.0005, val loss = 0.0982\n",
      "Epoch 126: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 127: train loss = 0.0004, val loss = 0.1008\n",
      "Epoch 128: train loss = 0.0005, val loss = 0.1014\n",
      "Epoch 129: train loss = 0.0004, val loss = 0.1005\n",
      "Epoch 130: train loss = 0.0004, val loss = 0.1000\n",
      "Epoch 131: train loss = 0.0004, val loss = 0.1006\n",
      "Epoch 132: train loss = 0.0004, val loss = 0.1024\n",
      "Epoch 133: train loss = 0.0004, val loss = 0.1009\n",
      "Epoch 134: train loss = 0.0004, val loss = 0.1004\n",
      "Epoch 135: train loss = 0.0004, val loss = 0.1000\n",
      "Epoch 136: train loss = 0.0005, val loss = 0.1015\n",
      "Epoch 137: train loss = 0.0005, val loss = 0.1011\n",
      "Epoch 138: train loss = 0.0006, val loss = 0.1011\n",
      "Epoch 139: train loss = 0.0006, val loss = 0.1012\n",
      "Epoch 140: train loss = 0.0006, val loss = 0.1026\n",
      "Epoch 141: train loss = 0.0006, val loss = 0.1004\n",
      "Epoch 142: train loss = 0.0006, val loss = 0.1008\n",
      "Epoch 143: train loss = 0.0005, val loss = 0.1013\n",
      "Epoch 144: train loss = 0.0005, val loss = 0.1007\n",
      "Epoch 145: train loss = 0.0005, val loss = 0.1011\n",
      "Epoch 146: train loss = 0.0005, val loss = 0.1018\n",
      "Epoch 147: train loss = 0.0006, val loss = 0.1004\n",
      "Epoch 148: train loss = 0.0005, val loss = 0.1024\n",
      "Epoch 149: train loss = 0.0005, val loss = 0.1014\n",
      "Epoch 150: train loss = 0.0004, val loss = 0.1009\n",
      "Epoch 151: train loss = 0.0004, val loss = 0.1012\n",
      "Epoch 152: train loss = 0.0004, val loss = 0.1018\n",
      "Epoch 153: train loss = 0.0004, val loss = 0.1015\n",
      "Epoch 154: train loss = 0.0003, val loss = 0.1024\n",
      "Epoch 155: train loss = 0.0003, val loss = 0.1007\n",
      "Epoch 156: train loss = 0.0003, val loss = 0.1019\n",
      "Epoch 157: train loss = 0.0003, val loss = 0.1022\n",
      "Epoch 158: train loss = 0.0004, val loss = 0.1011\n",
      "Epoch 159: train loss = 0.0004, val loss = 0.1033\n",
      "Epoch 160: train loss = 0.0005, val loss = 0.1008\n",
      "Epoch 161: train loss = 0.0006, val loss = 0.1029\n",
      "Epoch 162: train loss = 0.0005, val loss = 0.1010\n",
      "Epoch 163: train loss = 0.0006, val loss = 0.1031\n",
      "Epoch 164: train loss = 0.0008, val loss = 0.1001\n",
      "Epoch 165: train loss = 0.0009, val loss = 0.1021\n",
      "Epoch 166: train loss = 0.0009, val loss = 0.1014\n",
      "Epoch 167: train loss = 0.0009, val loss = 0.1022\n",
      "Epoch 168: train loss = 0.0008, val loss = 0.1016\n",
      "Epoch 169: train loss = 0.0009, val loss = 0.1016\n",
      "Epoch 170: train loss = 0.0009, val loss = 0.1025\n",
      "Epoch 171: train loss = 0.0007, val loss = 0.1019\n",
      "Epoch 172: train loss = 0.0007, val loss = 0.0995\n",
      "Epoch 173: train loss = 0.0006, val loss = 0.1001\n",
      "Epoch 174: train loss = 0.0006, val loss = 0.0998\n",
      "Epoch 175: train loss = 0.0006, val loss = 0.1005\n",
      "Epoch 176: train loss = 0.0005, val loss = 0.1015\n",
      "Epoch 177: train loss = 0.0004, val loss = 0.0997\n",
      "Epoch 178: train loss = 0.0003, val loss = 0.1015\n",
      "Epoch 179: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 180: train loss = 0.0003, val loss = 0.1030\n",
      "Epoch 181: train loss = 0.0003, val loss = 0.1019\n",
      "Epoch 182: train loss = 0.0002, val loss = 0.1012\n",
      "Epoch 183: train loss = 0.0002, val loss = 0.1016\n",
      "Epoch 184: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 185: train loss = 0.0003, val loss = 0.1014\n",
      "Epoch 186: train loss = 0.0003, val loss = 0.1027\n",
      "Epoch 187: train loss = 0.0003, val loss = 0.1016\n",
      "Epoch 188: train loss = 0.0003, val loss = 0.1029\n",
      "Epoch 189: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 190: train loss = 0.0002, val loss = 0.1014\n",
      "Epoch 191: train loss = 0.0002, val loss = 0.1024\n",
      "Epoch 192: train loss = 0.0002, val loss = 0.1013\n",
      "Epoch 193: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 194: train loss = 0.0002, val loss = 0.1016\n",
      "Epoch 195: train loss = 0.0003, val loss = 0.1022\n",
      "Epoch 196: train loss = 0.0003, val loss = 0.1023\n",
      "Epoch 197: train loss = 0.0003, val loss = 0.1026\n",
      "Epoch 198: train loss = 0.0003, val loss = 0.1032\n",
      "Epoch 199: train loss = 0.0002, val loss = 0.1025\n",
      "Epoch 200: train loss = 0.0002, val loss = 0.1017\n",
      "Epoch 201: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 202: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 203: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 204: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 205: train loss = 0.0002, val loss = 0.1030\n",
      "Epoch 206: train loss = 0.0002, val loss = 0.1027\n",
      "Epoch 207: train loss = 0.0002, val loss = 0.1036\n",
      "Epoch 208: train loss = 0.0002, val loss = 0.1018\n",
      "Epoch 209: train loss = 0.0002, val loss = 0.1026\n",
      "Epoch 210: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 211: train loss = 0.0002, val loss = 0.1031\n",
      "Epoch 212: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 213: train loss = 0.0002, val loss = 0.1031\n",
      "Epoch 214: train loss = 0.0002, val loss = 0.1023\n",
      "Epoch 215: train loss = 0.0003, val loss = 0.1036\n",
      "Epoch 216: train loss = 0.0003, val loss = 0.1035\n",
      "Epoch 217: train loss = 0.0004, val loss = 0.1028\n",
      "Epoch 218: train loss = 0.0004, val loss = 0.1025\n",
      "Epoch 219: train loss = 0.0004, val loss = 0.1029\n",
      "Epoch 220: train loss = 0.0003, val loss = 0.1018\n",
      "Epoch 221: train loss = 0.0004, val loss = 0.1026\n",
      "Epoch 222: train loss = 0.0004, val loss = 0.1030\n",
      "Epoch 223: train loss = 0.0004, val loss = 0.1018\n",
      "Epoch 224: train loss = 0.0004, val loss = 0.1029\n",
      "Epoch 225: train loss = 0.0004, val loss = 0.1030\n",
      "Epoch 226: train loss = 0.0004, val loss = 0.1027\n",
      "Epoch 227: train loss = 0.0004, val loss = 0.1028\n",
      "Epoch 228: train loss = 0.0004, val loss = 0.1019\n",
      "Epoch 229: train loss = 0.0004, val loss = 0.1025\n",
      "Epoch 230: train loss = 0.0005, val loss = 0.1022\n",
      "Epoch 231: train loss = 0.0005, val loss = 0.1031\n",
      "Epoch 232: train loss = 0.0005, val loss = 0.1005\n",
      "Epoch 233: train loss = 0.0005, val loss = 0.1045\n",
      "Epoch 234: train loss = 0.0005, val loss = 0.1020\n",
      "Epoch 235: train loss = 0.0006, val loss = 0.1015\n",
      "Epoch 236: train loss = 0.0007, val loss = 0.1003\n",
      "Epoch 237: train loss = 0.0008, val loss = 0.1026\n",
      "Epoch 238: train loss = 0.0008, val loss = 0.0998\n",
      "Epoch 239: train loss = 0.0007, val loss = 0.1019\n",
      "Epoch 240: train loss = 0.0005, val loss = 0.1024\n",
      "Epoch 241: train loss = 0.0005, val loss = 0.1001\n",
      "Epoch 242: train loss = 0.0004, val loss = 0.1008\n",
      "Epoch 243: train loss = 0.0004, val loss = 0.1010\n",
      "Epoch 244: train loss = 0.0004, val loss = 0.1002\n",
      "Epoch 245: train loss = 0.0004, val loss = 0.1019\n",
      "Epoch 246: train loss = 0.0003, val loss = 0.0997\n",
      "Epoch 247: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 248: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 249: train loss = 0.0003, val loss = 0.1020\n",
      "Epoch 250: train loss = 0.0002, val loss = 0.1013\n",
      "Epoch 251: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 252: train loss = 0.0003, val loss = 0.1010\n",
      "Epoch 253: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 254: train loss = 0.0003, val loss = 0.1017\n",
      "Epoch 255: train loss = 0.0003, val loss = 0.1016\n",
      "Epoch 256: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 257: train loss = 0.0003, val loss = 0.1020\n",
      "Epoch 258: train loss = 0.0003, val loss = 0.1008\n",
      "Epoch 259: train loss = 0.0003, val loss = 0.1024\n",
      "Epoch 260: train loss = 0.0002, val loss = 0.1002\n",
      "Epoch 261: train loss = 0.0002, val loss = 0.1021\n",
      "Epoch 262: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 263: train loss = 0.0002, val loss = 0.1019\n",
      "Epoch 264: train loss = 0.0002, val loss = 0.1022\n",
      "Epoch 265: train loss = 0.0002, val loss = 0.1015\n",
      "Epoch 266: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 267: train loss = 0.0002, val loss = 0.1019\n",
      "Epoch 268: train loss = 0.0002, val loss = 0.1000\n",
      "Epoch 269: train loss = 0.0002, val loss = 0.1014\n",
      "Epoch 270: train loss = 0.0003, val loss = 0.1017\n",
      "Epoch 271: train loss = 0.0003, val loss = 0.1013\n",
      "Epoch 272: train loss = 0.0003, val loss = 0.1019\n",
      "Epoch 273: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 274: train loss = 0.0004, val loss = 0.1003\n",
      "Epoch 275: train loss = 0.0004, val loss = 0.1030\n",
      "Epoch 276: train loss = 0.0004, val loss = 0.1016\n",
      "Epoch 277: train loss = 0.0004, val loss = 0.1020\n",
      "Epoch 278: train loss = 0.0003, val loss = 0.1008\n",
      "Epoch 279: train loss = 0.0003, val loss = 0.1015\n",
      "Epoch 280: train loss = 0.0003, val loss = 0.1013\n",
      "Epoch 281: train loss = 0.0003, val loss = 0.1014\n",
      "Epoch 282: train loss = 0.0003, val loss = 0.0999\n",
      "Epoch 283: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 284: train loss = 0.0003, val loss = 0.1001\n",
      "Epoch 285: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 286: train loss = 0.0003, val loss = 0.1007\n",
      "Epoch 287: train loss = 0.0003, val loss = 0.1018\n",
      "Epoch 288: train loss = 0.0003, val loss = 0.0995\n",
      "Epoch 289: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 290: train loss = 0.0002, val loss = 0.1007\n",
      "Epoch 291: train loss = 0.0002, val loss = 0.1009\n",
      "Epoch 292: train loss = 0.0002, val loss = 0.1006\n",
      "Epoch 293: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 294: train loss = 0.0002, val loss = 0.1011\n",
      "Epoch 295: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 296: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 297: train loss = 0.0002, val loss = 0.1009\n",
      "Epoch 298: train loss = 0.0003, val loss = 0.1008\n",
      "Epoch 299: train loss = 0.0003, val loss = 0.1013\n",
      "Epoch 300: train loss = 0.0004, val loss = 0.0998\n",
      "Epoch 301: train loss = 0.0004, val loss = 0.1006\n",
      "Epoch 302: train loss = 0.0005, val loss = 0.1012\n",
      "Epoch 303: train loss = 0.0005, val loss = 0.1001\n",
      "Epoch 304: train loss = 0.0005, val loss = 0.1013\n",
      "Epoch 305: train loss = 0.0005, val loss = 0.0992\n",
      "Epoch 306: train loss = 0.0004, val loss = 0.0995\n",
      "Epoch 307: train loss = 0.0004, val loss = 0.0999\n",
      "Epoch 308: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 309: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 310: train loss = 0.0002, val loss = 0.1005\n",
      "Epoch 311: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 312: train loss = 0.0002, val loss = 0.1010\n",
      "Epoch 313: train loss = 0.0002, val loss = 0.1004\n",
      "Epoch 314: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 315: train loss = 0.0003, val loss = 0.1000\n",
      "Epoch 316: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 317: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 318: train loss = 0.0003, val loss = 0.1002\n",
      "Epoch 319: train loss = 0.0003, val loss = 0.1009\n",
      "Epoch 320: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 321: train loss = 0.0003, val loss = 0.1002\n",
      "Epoch 322: train loss = 0.0003, val loss = 0.0999\n",
      "Epoch 323: train loss = 0.0003, val loss = 0.1000\n",
      "Epoch 324: train loss = 0.0003, val loss = 0.1015\n",
      "Epoch 325: train loss = 0.0003, val loss = 0.1005\n",
      "Epoch 326: train loss = 0.0003, val loss = 0.1012\n",
      "Epoch 327: train loss = 0.0004, val loss = 0.0993\n",
      "Epoch 328: train loss = 0.0003, val loss = 0.1014\n",
      "Epoch 329: train loss = 0.0003, val loss = 0.1004\n",
      "Epoch 330: train loss = 0.0004, val loss = 0.0997\n",
      "Epoch 331: train loss = 0.0004, val loss = 0.0995\n",
      "Epoch 332: train loss = 0.0004, val loss = 0.0996\n",
      "Epoch 333: train loss = 0.0004, val loss = 0.0993\n",
      "Epoch 334: train loss = 0.0003, val loss = 0.1001\n",
      "Epoch 335: train loss = 0.0003, val loss = 0.0995\n",
      "Epoch 336: train loss = 0.0002, val loss = 0.0999\n",
      "Epoch 337: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 338: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 339: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 340: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 341: train loss = 0.0002, val loss = 0.1001\n",
      "Epoch 342: train loss = 0.0002, val loss = 0.1003\n",
      "Epoch 343: train loss = 0.0002, val loss = 0.1000\n",
      "Epoch 344: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 345: train loss = 0.0002, val loss = 0.1006\n",
      "Epoch 346: train loss = 0.0002, val loss = 0.0999\n",
      "Epoch 347: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 348: train loss = 0.0001, val loss = 0.0998\n",
      "Epoch 349: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 350: train loss = 0.0002, val loss = 0.0995\n",
      "Epoch 351: train loss = 0.0002, val loss = 0.1002\n",
      "Epoch 352: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 353: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 354: train loss = 0.0002, val loss = 0.0993\n",
      "Epoch 355: train loss = 0.0003, val loss = 0.0987\n",
      "Epoch 356: train loss = 0.0003, val loss = 0.0996\n",
      "Epoch 357: train loss = 0.0005, val loss = 0.0977\n",
      "Epoch 358: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 359: train loss = 0.0005, val loss = 0.1004\n",
      "Epoch 360: train loss = 0.0005, val loss = 0.0993\n",
      "Epoch 361: train loss = 0.0006, val loss = 0.0977\n",
      "Epoch 362: train loss = 0.0007, val loss = 0.1002\n",
      "Epoch 363: train loss = 0.0008, val loss = 0.0983\n",
      "Epoch 364: train loss = 0.0008, val loss = 0.1002\n",
      "Epoch 365: train loss = 0.0010, val loss = 0.0981\n",
      "Epoch 366: train loss = 0.0008, val loss = 0.0984\n",
      "Epoch 367: train loss = 0.0006, val loss = 0.0982\n",
      "Epoch 368: train loss = 0.0006, val loss = 0.0982\n",
      "Epoch 369: train loss = 0.0005, val loss = 0.0986\n",
      "Epoch 370: train loss = 0.0005, val loss = 0.0979\n",
      "Epoch 371: train loss = 0.0004, val loss = 0.0982\n",
      "Epoch 372: train loss = 0.0004, val loss = 0.0983\n",
      "Epoch 373: train loss = 0.0003, val loss = 0.0975\n",
      "Epoch 374: train loss = 0.0003, val loss = 0.0978\n",
      "Epoch 375: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 376: train loss = 0.0002, val loss = 0.0977\n",
      "Epoch 377: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 378: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 379: train loss = 0.0002, val loss = 0.0978\n",
      "Epoch 380: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 381: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 382: train loss = 0.0002, val loss = 0.0989\n",
      "Epoch 383: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 384: train loss = 0.0001, val loss = 0.0985\n",
      "Epoch 385: train loss = 0.0001, val loss = 0.0993\n",
      "Epoch 386: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 387: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 388: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 389: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 390: train loss = 0.0002, val loss = 0.0987\n",
      "Epoch 391: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 392: train loss = 0.0002, val loss = 0.0981\n",
      "Epoch 393: train loss = 0.0002, val loss = 0.0985\n",
      "Epoch 394: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 395: train loss = 0.0002, val loss = 0.0979\n",
      "Epoch 396: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 397: train loss = 0.0002, val loss = 0.1002\n",
      "Epoch 398: train loss = 0.0003, val loss = 0.0988\n",
      "Epoch 399: train loss = 0.0003, val loss = 0.0987\n",
      "Epoch 400: train loss = 0.0004, val loss = 0.0975\n",
      "Epoch 401: train loss = 0.0005, val loss = 0.0991\n",
      "Epoch 402: train loss = 0.0005, val loss = 0.0989\n",
      "Epoch 403: train loss = 0.0005, val loss = 0.0979\n",
      "Epoch 404: train loss = 0.0005, val loss = 0.0973\n",
      "Epoch 405: train loss = 0.0005, val loss = 0.0986\n",
      "Epoch 406: train loss = 0.0004, val loss = 0.0979\n",
      "Epoch 407: train loss = 0.0004, val loss = 0.0995\n",
      "Epoch 408: train loss = 0.0003, val loss = 0.0985\n",
      "Epoch 409: train loss = 0.0003, val loss = 0.0976\n",
      "Epoch 410: train loss = 0.0002, val loss = 0.0985\n",
      "Epoch 411: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 412: train loss = 0.0002, val loss = 0.0978\n",
      "Epoch 413: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 414: train loss = 0.0002, val loss = 0.0980\n",
      "Epoch 415: train loss = 0.0002, val loss = 0.0984\n",
      "Epoch 416: train loss = 0.0001, val loss = 0.0993\n",
      "Epoch 417: train loss = 0.0001, val loss = 0.0990\n",
      "Epoch 418: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 419: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 420: train loss = 0.0001, val loss = 0.0987\n",
      "Epoch 421: train loss = 0.0001, val loss = 0.0988\n",
      "Epoch 422: train loss = 0.0001, val loss = 0.0992\n",
      "Epoch 423: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 424: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 425: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 426: train loss = 0.0002, val loss = 0.0999\n",
      "Epoch 427: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 428: train loss = 0.0002, val loss = 0.0988\n",
      "Epoch 429: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 430: train loss = 0.0002, val loss = 0.0987\n",
      "Epoch 431: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 432: train loss = 0.0002, val loss = 0.0990\n",
      "Epoch 433: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 434: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 435: train loss = 0.0001, val loss = 0.0998\n",
      "Epoch 436: train loss = 0.0001, val loss = 0.0984\n",
      "Epoch 437: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 438: train loss = 0.0001, val loss = 0.0983\n",
      "Epoch 439: train loss = 0.0001, val loss = 0.0988\n",
      "Epoch 440: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 441: train loss = 0.0002, val loss = 0.0982\n",
      "Epoch 442: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 443: train loss = 0.0002, val loss = 0.0992\n",
      "Epoch 444: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 445: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 446: train loss = 0.0003, val loss = 0.0975\n",
      "Epoch 447: train loss = 0.0003, val loss = 0.0994\n",
      "Epoch 448: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 449: train loss = 0.0004, val loss = 0.0983\n",
      "Epoch 450: train loss = 0.0004, val loss = 0.0965\n",
      "Epoch 451: train loss = 0.0004, val loss = 0.0984\n",
      "Epoch 452: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 453: train loss = 0.0004, val loss = 0.0979\n",
      "Epoch 454: train loss = 0.0004, val loss = 0.0968\n",
      "Epoch 455: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 456: train loss = 0.0003, val loss = 0.0993\n",
      "Epoch 457: train loss = 0.0003, val loss = 0.0995\n",
      "Epoch 458: train loss = 0.0004, val loss = 0.0978\n",
      "Epoch 459: train loss = 0.0004, val loss = 0.0989\n",
      "Epoch 460: train loss = 0.0004, val loss = 0.0964\n",
      "Epoch 461: train loss = 0.0004, val loss = 0.0968\n",
      "Epoch 462: train loss = 0.0004, val loss = 0.0985\n",
      "Epoch 463: train loss = 0.0003, val loss = 0.0984\n",
      "Epoch 464: train loss = 0.0003, val loss = 0.0982\n",
      "Epoch 465: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 466: train loss = 0.0003, val loss = 0.0975\n",
      "Epoch 467: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 468: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 469: train loss = 0.0004, val loss = 0.0990\n",
      "Epoch 470: train loss = 0.0004, val loss = 0.0974\n",
      "Epoch 471: train loss = 0.0004, val loss = 0.0978\n",
      "Epoch 472: train loss = 0.0005, val loss = 0.0974\n",
      "Epoch 473: train loss = 0.0005, val loss = 0.0975\n",
      "Epoch 474: train loss = 0.0005, val loss = 0.0973\n",
      "Epoch 475: train loss = 0.0005, val loss = 0.0973\n",
      "Epoch 476: train loss = 0.0005, val loss = 0.0965\n",
      "Epoch 477: train loss = 0.0005, val loss = 0.0990\n",
      "Epoch 478: train loss = 0.0005, val loss = 0.0968\n",
      "Epoch 479: train loss = 0.0005, val loss = 0.0976\n",
      "Epoch 480: train loss = 0.0004, val loss = 0.0974\n",
      "Epoch 481: train loss = 0.0003, val loss = 0.0978\n",
      "Epoch 482: train loss = 0.0003, val loss = 0.0978\n",
      "Epoch 483: train loss = 0.0004, val loss = 0.0978\n",
      "Epoch 484: train loss = 0.0003, val loss = 0.0970\n",
      "Epoch 485: train loss = 0.0003, val loss = 0.0968\n",
      "Epoch 486: train loss = 0.0003, val loss = 0.0983\n",
      "Epoch 487: train loss = 0.0003, val loss = 0.0974\n",
      "Epoch 488: train loss = 0.0002, val loss = 0.0980\n",
      "Epoch 489: train loss = 0.0002, val loss = 0.0977\n",
      "Epoch 490: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 491: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 492: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 493: train loss = 0.0002, val loss = 0.0984\n",
      "Epoch 494: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 495: train loss = 0.0002, val loss = 0.0975\n",
      "Epoch 496: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 497: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 498: train loss = 0.0002, val loss = 0.0976\n",
      "Epoch 499: train loss = 0.0003, val loss = 0.0980\n",
      "Epoch 500: train loss = 0.0003, val loss = 0.0986\n",
      "Epoch 501: train loss = 0.0003, val loss = 0.0963\n",
      "Epoch 502: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 503: train loss = 0.0004, val loss = 0.0967\n",
      "Epoch 504: train loss = 0.0004, val loss = 0.0961\n",
      "Epoch 505: train loss = 0.0004, val loss = 0.0979\n",
      "Epoch 506: train loss = 0.0004, val loss = 0.0969\n",
      "Epoch 507: train loss = 0.0005, val loss = 0.0970\n",
      "Epoch 508: train loss = 0.0005, val loss = 0.0984\n",
      "Epoch 509: train loss = 0.0006, val loss = 0.0966\n",
      "Epoch 510: train loss = 0.0006, val loss = 0.0962\n",
      "Epoch 511: train loss = 0.0006, val loss = 0.0986\n",
      "Epoch 512: train loss = 0.0007, val loss = 0.0962\n",
      "Epoch 513: train loss = 0.0007, val loss = 0.0984\n",
      "Epoch 514: train loss = 0.0007, val loss = 0.0949\n",
      "Epoch 515: train loss = 0.0007, val loss = 0.0978\n",
      "Epoch 516: train loss = 0.0006, val loss = 0.0971\n",
      "Epoch 517: train loss = 0.0005, val loss = 0.0955\n",
      "Epoch 518: train loss = 0.0004, val loss = 0.0954\n",
      "Epoch 519: train loss = 0.0003, val loss = 0.0968\n",
      "Epoch 520: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 521: train loss = 0.0003, val loss = 0.0986\n",
      "Epoch 522: train loss = 0.0002, val loss = 0.0974\n",
      "Epoch 523: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 524: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 525: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 526: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 527: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 528: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 529: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 530: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 531: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 532: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 533: train loss = 0.0000, val loss = 0.0971\n",
      "Epoch 534: train loss = 0.0000, val loss = 0.0979\n",
      "Epoch 535: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 536: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 537: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 538: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 539: train loss = 0.0000, val loss = 0.0978\n",
      "Epoch 540: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 541: train loss = 0.0000, val loss = 0.0976\n",
      "Epoch 542: train loss = 0.0000, val loss = 0.0976\n",
      "Epoch 543: train loss = 0.0000, val loss = 0.0974\n",
      "Epoch 544: train loss = 0.0000, val loss = 0.0980\n",
      "Epoch 545: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 546: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 547: train loss = 0.0000, val loss = 0.0977\n",
      "Epoch 548: train loss = 0.0000, val loss = 0.0975\n",
      "Epoch 549: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 550: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 551: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 552: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 553: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 554: train loss = 0.0001, val loss = 0.0983\n",
      "Epoch 555: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 556: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 557: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 558: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 559: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 560: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 561: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 562: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 563: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 564: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 565: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 566: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 567: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 568: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 569: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 570: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 571: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 572: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 573: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 574: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 575: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 576: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 577: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 578: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 579: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 580: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 581: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 582: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 583: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 584: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 585: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 586: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 587: train loss = 0.0002, val loss = 0.0967\n",
      "Epoch 588: train loss = 0.0003, val loss = 0.0971\n",
      "Epoch 589: train loss = 0.0003, val loss = 0.0985\n",
      "Epoch 590: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 591: train loss = 0.0004, val loss = 0.0983\n",
      "Epoch 592: train loss = 0.0004, val loss = 0.0956\n",
      "Epoch 593: train loss = 0.0005, val loss = 0.0977\n",
      "Epoch 594: train loss = 0.0006, val loss = 0.0960\n",
      "Epoch 595: train loss = 0.0007, val loss = 0.0997\n",
      "Epoch 596: train loss = 0.0008, val loss = 0.0953\n",
      "Epoch 597: train loss = 0.0008, val loss = 0.0976\n",
      "Epoch 598: train loss = 0.0008, val loss = 0.0972\n",
      "Epoch 599: train loss = 0.0007, val loss = 0.0938\n",
      "Epoch 600: train loss = 0.0007, val loss = 0.0943\n",
      "Epoch 601: train loss = 0.0007, val loss = 0.0952\n",
      "Epoch 602: train loss = 0.0007, val loss = 0.0964\n",
      "Epoch 603: train loss = 0.0007, val loss = 0.0980\n",
      "Epoch 604: train loss = 0.0009, val loss = 0.0944\n",
      "Epoch 605: train loss = 0.0008, val loss = 0.0981\n",
      "Epoch 606: train loss = 0.0007, val loss = 0.0964\n",
      "Epoch 607: train loss = 0.0006, val loss = 0.0977\n",
      "Epoch 608: train loss = 0.0006, val loss = 0.0965\n",
      "Epoch 609: train loss = 0.0005, val loss = 0.0965\n",
      "Epoch 610: train loss = 0.0005, val loss = 0.0980\n",
      "Epoch 611: train loss = 0.0005, val loss = 0.0974\n",
      "Epoch 612: train loss = 0.0004, val loss = 0.0958\n",
      "Epoch 613: train loss = 0.0004, val loss = 0.0952\n",
      "Epoch 614: train loss = 0.0004, val loss = 0.0975\n",
      "Epoch 615: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 616: train loss = 0.0003, val loss = 0.0955\n",
      "Epoch 617: train loss = 0.0003, val loss = 0.0956\n",
      "Epoch 618: train loss = 0.0003, val loss = 0.0962\n",
      "Epoch 619: train loss = 0.0003, val loss = 0.0976\n",
      "Epoch 620: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 621: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 622: train loss = 0.0002, val loss = 0.0976\n",
      "Epoch 623: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 624: train loss = 0.0002, val loss = 0.0963\n",
      "Epoch 625: train loss = 0.0002, val loss = 0.0965\n",
      "Epoch 626: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 627: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 628: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 629: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 630: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 631: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 632: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 633: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 634: train loss = 0.0000, val loss = 0.0964\n",
      "Epoch 635: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 636: train loss = 0.0000, val loss = 0.0965\n",
      "Epoch 637: train loss = 0.0000, val loss = 0.0965\n",
      "Epoch 638: train loss = 0.0000, val loss = 0.0971\n",
      "Epoch 639: train loss = 0.0000, val loss = 0.0961\n",
      "Epoch 640: train loss = 0.0000, val loss = 0.0964\n",
      "Epoch 641: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 642: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 643: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 644: train loss = 0.0000, val loss = 0.0965\n",
      "Epoch 645: train loss = 0.0000, val loss = 0.0970\n",
      "Epoch 646: train loss = 0.0000, val loss = 0.0964\n",
      "Epoch 647: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 648: train loss = 0.0000, val loss = 0.0966\n",
      "Epoch 649: train loss = 0.0000, val loss = 0.0969\n",
      "Epoch 650: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 651: train loss = 0.0000, val loss = 0.0969\n",
      "Epoch 652: train loss = 0.0000, val loss = 0.0970\n",
      "Epoch 653: train loss = 0.0000, val loss = 0.0969\n",
      "Epoch 654: train loss = 0.0000, val loss = 0.0971\n",
      "Epoch 655: train loss = 0.0000, val loss = 0.0967\n",
      "Epoch 656: train loss = 0.0000, val loss = 0.0972\n",
      "Epoch 657: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 658: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 659: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 660: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 661: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 662: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 663: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 664: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 665: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 666: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 667: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 668: train loss = 0.0002, val loss = 0.0969\n",
      "Epoch 669: train loss = 0.0003, val loss = 0.0977\n",
      "Epoch 670: train loss = 0.0004, val loss = 0.0965\n",
      "Epoch 671: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 672: train loss = 0.0004, val loss = 0.0986\n",
      "Epoch 673: train loss = 0.0004, val loss = 0.0952\n",
      "Epoch 674: train loss = 0.0004, val loss = 0.0958\n",
      "Epoch 675: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 676: train loss = 0.0005, val loss = 0.0964\n",
      "Epoch 677: train loss = 0.0005, val loss = 0.0965\n",
      "Epoch 678: train loss = 0.0005, val loss = 0.0978\n",
      "Epoch 679: train loss = 0.0005, val loss = 0.0961\n",
      "Epoch 680: train loss = 0.0004, val loss = 0.0970\n",
      "Epoch 681: train loss = 0.0005, val loss = 0.0955\n",
      "Epoch 682: train loss = 0.0005, val loss = 0.0955\n",
      "Epoch 683: train loss = 0.0005, val loss = 0.0945\n",
      "Epoch 684: train loss = 0.0005, val loss = 0.0971\n",
      "Epoch 685: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 686: train loss = 0.0004, val loss = 0.0964\n",
      "Epoch 687: train loss = 0.0005, val loss = 0.0946\n",
      "Epoch 688: train loss = 0.0005, val loss = 0.0964\n",
      "Epoch 689: train loss = 0.0005, val loss = 0.0951\n",
      "Epoch 690: train loss = 0.0004, val loss = 0.0959\n",
      "Epoch 691: train loss = 0.0004, val loss = 0.0954\n",
      "Epoch 692: train loss = 0.0004, val loss = 0.0962\n",
      "Epoch 693: train loss = 0.0005, val loss = 0.0954\n",
      "Epoch 694: train loss = 0.0005, val loss = 0.0977\n",
      "Epoch 695: train loss = 0.0005, val loss = 0.0968\n",
      "Epoch 696: train loss = 0.0005, val loss = 0.0946\n",
      "Epoch 697: train loss = 0.0006, val loss = 0.0957\n",
      "Epoch 698: train loss = 0.0005, val loss = 0.0984\n",
      "Epoch 699: train loss = 0.0004, val loss = 0.0947\n",
      "Epoch 700: train loss = 0.0004, val loss = 0.0964\n",
      "Epoch 701: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 702: train loss = 0.0003, val loss = 0.0970\n",
      "Epoch 703: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 704: train loss = 0.0003, val loss = 0.0955\n",
      "Epoch 705: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 706: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 707: train loss = 0.0003, val loss = 0.0959\n",
      "Epoch 708: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 709: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 710: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 711: train loss = 0.0001, val loss = 0.0955\n",
      "Epoch 712: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 713: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 714: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 715: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 716: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 717: train loss = 0.0001, val loss = 0.0955\n",
      "Epoch 718: train loss = 0.0000, val loss = 0.0961\n",
      "Epoch 719: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 720: train loss = 0.0000, val loss = 0.0955\n",
      "Epoch 721: train loss = 0.0000, val loss = 0.0963\n",
      "Epoch 722: train loss = 0.0000, val loss = 0.0955\n",
      "Epoch 723: train loss = 0.0000, val loss = 0.0963\n",
      "Epoch 724: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 725: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 726: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 727: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 728: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 729: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 730: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 731: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 732: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 733: train loss = 0.0000, val loss = 0.0961\n",
      "Epoch 734: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 735: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 736: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 737: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 738: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 739: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 740: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 741: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 742: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 743: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 744: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 745: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 746: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 747: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 748: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 749: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 750: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 751: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 752: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 753: train loss = 0.0002, val loss = 0.0965\n",
      "Epoch 754: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 755: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 756: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 757: train loss = 0.0003, val loss = 0.0962\n",
      "Epoch 758: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 759: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 760: train loss = 0.0003, val loss = 0.0957\n",
      "Epoch 761: train loss = 0.0003, val loss = 0.0971\n",
      "Epoch 762: train loss = 0.0003, val loss = 0.0964\n",
      "Epoch 763: train loss = 0.0003, val loss = 0.0957\n",
      "Epoch 764: train loss = 0.0003, val loss = 0.0969\n",
      "Epoch 765: train loss = 0.0004, val loss = 0.0966\n",
      "Epoch 766: train loss = 0.0003, val loss = 0.0961\n",
      "Epoch 767: train loss = 0.0003, val loss = 0.0957\n",
      "Epoch 768: train loss = 0.0003, val loss = 0.0941\n",
      "Epoch 769: train loss = 0.0003, val loss = 0.0965\n",
      "Epoch 770: train loss = 0.0002, val loss = 0.0961\n",
      "Epoch 771: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 772: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 773: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 774: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 775: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 776: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 777: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 778: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 779: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 780: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 781: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 782: train loss = 0.0003, val loss = 0.0976\n",
      "Epoch 783: train loss = 0.0002, val loss = 0.0943\n",
      "Epoch 784: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 785: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 786: train loss = 0.0002, val loss = 0.0975\n",
      "Epoch 787: train loss = 0.0003, val loss = 0.0960\n",
      "Epoch 788: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 789: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 790: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 791: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 792: train loss = 0.0003, val loss = 0.0956\n",
      "Epoch 793: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 794: train loss = 0.0003, val loss = 0.0973\n",
      "Epoch 795: train loss = 0.0002, val loss = 0.0941\n",
      "Epoch 796: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 797: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 798: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 799: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 800: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 801: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 802: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 803: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 804: train loss = 0.0002, val loss = 0.0944\n",
      "Epoch 805: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 806: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 807: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 808: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 809: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 810: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 811: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 812: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 813: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 814: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 815: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 816: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 817: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 818: train loss = 0.0001, val loss = 0.0949\n",
      "Epoch 819: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 820: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 821: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 822: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 823: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 824: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 825: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 826: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 827: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 828: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 829: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 830: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 831: train loss = 0.0001, val loss = 0.0960\n",
      "Epoch 832: train loss = 0.0001, val loss = 0.0947\n",
      "Epoch 833: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 834: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 835: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 836: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 837: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 838: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 839: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 840: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 841: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 842: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 843: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 844: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 845: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 846: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 847: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 848: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 849: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 850: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 851: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 852: train loss = 0.0001, val loss = 0.0964\n",
      "Epoch 853: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 854: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 855: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 856: train loss = 0.0002, val loss = 0.0967\n",
      "Epoch 857: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 858: train loss = 0.0003, val loss = 0.0956\n",
      "Epoch 859: train loss = 0.0003, val loss = 0.0944\n",
      "Epoch 860: train loss = 0.0003, val loss = 0.0977\n",
      "Epoch 861: train loss = 0.0004, val loss = 0.0936\n",
      "Epoch 862: train loss = 0.0004, val loss = 0.0963\n",
      "Epoch 863: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 864: train loss = 0.0004, val loss = 0.0951\n",
      "Epoch 865: train loss = 0.0005, val loss = 0.0950\n",
      "Epoch 866: train loss = 0.0005, val loss = 0.0954\n",
      "Epoch 867: train loss = 0.0006, val loss = 0.0943\n",
      "Epoch 868: train loss = 0.0005, val loss = 0.0966\n",
      "Epoch 869: train loss = 0.0006, val loss = 0.0940\n",
      "Epoch 870: train loss = 0.0006, val loss = 0.0963\n",
      "Epoch 871: train loss = 0.0005, val loss = 0.0947\n",
      "Epoch 872: train loss = 0.0004, val loss = 0.0943\n",
      "Epoch 873: train loss = 0.0004, val loss = 0.0936\n",
      "Epoch 874: train loss = 0.0003, val loss = 0.0959\n",
      "Epoch 875: train loss = 0.0004, val loss = 0.0939\n",
      "Epoch 876: train loss = 0.0003, val loss = 0.0953\n",
      "Epoch 877: train loss = 0.0002, val loss = 0.0965\n",
      "Epoch 878: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 879: train loss = 0.0002, val loss = 0.0974\n",
      "Epoch 880: train loss = 0.0002, val loss = 0.0942\n",
      "Epoch 881: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 882: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 883: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 884: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 885: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 886: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 887: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 888: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 889: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 890: train loss = 0.0002, val loss = 0.0952\n",
      "Epoch 891: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 892: train loss = 0.0001, val loss = 0.0947\n",
      "Epoch 893: train loss = 0.0001, val loss = 0.0950\n",
      "Epoch 894: train loss = 0.0002, val loss = 0.0949\n",
      "Epoch 895: train loss = 0.0002, val loss = 0.0962\n",
      "Epoch 896: train loss = 0.0002, val loss = 0.0938\n",
      "Epoch 897: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 898: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 899: train loss = 0.0002, val loss = 0.0949\n",
      "Epoch 900: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 901: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 902: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 903: train loss = 0.0002, val loss = 0.0948\n",
      "Epoch 904: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 905: train loss = 0.0002, val loss = 0.0946\n",
      "Epoch 906: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 907: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 908: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 909: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 910: train loss = 0.0001, val loss = 0.0951\n",
      "Epoch 911: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 912: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 913: train loss = 0.0000, val loss = 0.0951\n",
      "Epoch 914: train loss = 0.0000, val loss = 0.0956\n",
      "Epoch 915: train loss = 0.0000, val loss = 0.0956\n",
      "Epoch 916: train loss = 0.0000, val loss = 0.0952\n",
      "Epoch 917: train loss = 0.0000, val loss = 0.0951\n",
      "Epoch 918: train loss = 0.0000, val loss = 0.0954\n",
      "Epoch 919: train loss = 0.0000, val loss = 0.0951\n",
      "Epoch 920: train loss = 0.0000, val loss = 0.0963\n",
      "Epoch 921: train loss = 0.0000, val loss = 0.0956\n",
      "Epoch 922: train loss = 0.0000, val loss = 0.0948\n",
      "Epoch 923: train loss = 0.0000, val loss = 0.0957\n",
      "Epoch 924: train loss = 0.0000, val loss = 0.0957\n",
      "Epoch 925: train loss = 0.0001, val loss = 0.0962\n",
      "Epoch 926: train loss = 0.0000, val loss = 0.0950\n",
      "Epoch 927: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 928: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 929: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 930: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 931: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 932: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 933: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 934: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 935: train loss = 0.0001, val loss = 0.0957\n",
      "Epoch 936: train loss = 0.0001, val loss = 0.0955\n",
      "Epoch 937: train loss = 0.0001, val loss = 0.0946\n",
      "Epoch 938: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 939: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 940: train loss = 0.0001, val loss = 0.0961\n",
      "Epoch 941: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 942: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 943: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 944: train loss = 0.0001, val loss = 0.0944\n",
      "Epoch 945: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 946: train loss = 0.0002, val loss = 0.0953\n",
      "Epoch 947: train loss = 0.0002, val loss = 0.0955\n",
      "Epoch 948: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 949: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 950: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 951: train loss = 0.0002, val loss = 0.0944\n",
      "Epoch 952: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 953: train loss = 0.0002, val loss = 0.0944\n",
      "Epoch 954: train loss = 0.0003, val loss = 0.0962\n",
      "Epoch 955: train loss = 0.0004, val loss = 0.0952\n",
      "Epoch 956: train loss = 0.0004, val loss = 0.0934\n",
      "Epoch 957: train loss = 0.0003, val loss = 0.0951\n",
      "Epoch 958: train loss = 0.0004, val loss = 0.0971\n",
      "Epoch 959: train loss = 0.0005, val loss = 0.0933\n",
      "Epoch 960: train loss = 0.0004, val loss = 0.0945\n",
      "Epoch 961: train loss = 0.0002, val loss = 0.0945\n",
      "Epoch 962: train loss = 0.0002, val loss = 0.0958\n",
      "Epoch 963: train loss = 0.0002, val loss = 0.0964\n",
      "Epoch 964: train loss = 0.0002, val loss = 0.0954\n",
      "Epoch 965: train loss = 0.0001, val loss = 0.0943\n",
      "Epoch 966: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 967: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 968: train loss = 0.0001, val loss = 0.0940\n",
      "Epoch 969: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 970: train loss = 0.0001, val loss = 0.0944\n",
      "Epoch 971: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 972: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 973: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 974: train loss = 0.0001, val loss = 0.0951\n",
      "Epoch 975: train loss = 0.0001, val loss = 0.0941\n",
      "Epoch 976: train loss = 0.0001, val loss = 0.0950\n",
      "Epoch 977: train loss = 0.0001, val loss = 0.0958\n",
      "Epoch 978: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 979: train loss = 0.0001, val loss = 0.0943\n",
      "Epoch 980: train loss = 0.0001, val loss = 0.0951\n",
      "Epoch 981: train loss = 0.0001, val loss = 0.0950\n",
      "Epoch 982: train loss = 0.0001, val loss = 0.0946\n",
      "Epoch 983: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 984: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 985: train loss = 0.0001, val loss = 0.0948\n",
      "Epoch 986: train loss = 0.0001, val loss = 0.0949\n",
      "Epoch 987: train loss = 0.0001, val loss = 0.0952\n",
      "Epoch 988: train loss = 0.0001, val loss = 0.0949\n",
      "Epoch 989: train loss = 0.0001, val loss = 0.0956\n",
      "Epoch 990: train loss = 0.0001, val loss = 0.0943\n",
      "Epoch 991: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 992: train loss = 0.0001, val loss = 0.0946\n",
      "Epoch 993: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 994: train loss = 0.0002, val loss = 0.0959\n",
      "Epoch 995: train loss = 0.0002, val loss = 0.0951\n",
      "Epoch 996: train loss = 0.0002, val loss = 0.0942\n",
      "Epoch 997: train loss = 0.0002, val loss = 0.0950\n",
      "Epoch 998: train loss = 0.0002, val loss = 0.0956\n",
      "Epoch 999: train loss = 0.0002, val loss = 0.0947\n",
      "Epoch 1000: train loss = 0.0002, val loss = 0.0955\n"
     ]
    }
   ],
   "source": [
    "model = train_final_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e0c8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mood_predictions(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39a1aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  predicted_mood_class\n",
      "0   AS14.16                    31\n",
      "1   AS14.02                    25\n",
      "2   AS14.12                    22\n",
      "3   AS14.30                    32\n",
      "4   AS14.19                    29\n",
      "5   AS14.09                    29\n",
      "6   AS14.29                    28\n",
      "7   AS14.01                    33\n",
      "8   AS14.14                    30\n",
      "9   AS14.25                    28\n",
      "10  AS14.07                    27\n",
      "11  AS14.23                    30\n",
      "12  AS14.20                    24\n",
      "13  AS14.15                    33\n",
      "14  AS14.17                    28\n",
      "15  AS14.31                    28\n",
      "16  AS14.26                    27\n",
      "17  AS14.33                    29\n",
      "18  AS14.13                    28\n",
      "19  AS14.24                    28\n",
      "20  AS14.08                    28\n",
      "21  AS14.27                    28\n",
      "22  AS14.32                    28\n",
      "23  AS14.03                    32\n",
      "24  AS14.06                    28\n",
      "25  AS14.05                    30\n",
      "26  AS14.28                    28\n"
     ]
    }
   ],
   "source": [
    "# Run predictions on test_df\n",
    "test_predictions = predict(model, test_df, id_map, device)\n",
    "\n",
    "# Attach predictions to test_df\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted_mood_class'] = test_predictions\n",
    "\n",
    "# Optional: save to CSV or examine\n",
    "print(test_df_with_preds[['id', 'predicted_mood_class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3933caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "        31, 32, 33, 34, 35, 36, 38], dtype=int32),\n",
       " mood\n",
       " 28    301\n",
       " 29    138\n",
       " 32    128\n",
       " 30    124\n",
       " 26    103\n",
       " 27     98\n",
       " 31     98\n",
       " 24     66\n",
       " 25     65\n",
       " 33     21\n",
       " 22     17\n",
       " 23     17\n",
       " 34     13\n",
       " 20      7\n",
       " 35      6\n",
       " 21      5\n",
       " 36      4\n",
       " 14      3\n",
       " 18      3\n",
       " 19      2\n",
       " 15      2\n",
       " 12      1\n",
       " 16      1\n",
       " 38      1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sort_values('mood')\n",
    "train_df['mood'].unique(), train_df['mood'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
