{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60ca131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the path (/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1) to sys.path\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ./initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf5c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from data_loading import DataPreprocessor\n",
    "from mood_RNN_classifier import RNNClassifier, MoodDataset, OrdinalLabelSmoothingLoss, split_train_val, objective, train_epoch, evaluate, predict, plot_mood_predictions\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c42a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_973,)\n",
      "Series: 'appCat.weather' [f64]\n",
      "[\n",
      "\tnull\n",
      "\tnull\n",
      "\tnull\n",
      "\tnull\n",
      "\tnull\n",
      "\tâ€¦\n",
      "\tnull\n",
      "\tnull\n",
      "\tnull\n",
      "\tnull\n",
      "\tnull\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataPreprocessor()\n",
    "train_df, test_df = data_loader.load_and_preprocess_data(\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6928e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume train_df and test_df are loaded and preprocessed\n",
    "id_map = {id_: idx for idx, id_ in enumerate(train_df['id'].unique())}\n",
    "input_dim = train_df.drop(columns=['id', 'mood', 'date']).shape[1]\n",
    "id_count = len(id_map)\n",
    "output_dim = train_df['mood'].max() + 1\n",
    "\n",
    "train_df_split, val_df_split = split_train_val(train_df, fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c89292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 19:55:46,029] A new study created in memory with name: no-name-5d9b97ca-7a65-4476-9f44-e9e082f1ed50\n",
      "[I 2025-04-17 19:55:51,103] Trial 0 finished with value: 0.8934529423713684 and parameters: {'hidden_dim': 95, 'id_embed_dim': 8, 'lr': 0.0007393780875508301, 'batch_size': 64, 'alpha': 0.27028523778182717}. Best is trial 0 with value: 0.8934529423713684.\n",
      "[I 2025-04-17 19:55:51,286] Trial 1 finished with value: 0.5743939876556396 and parameters: {'hidden_dim': 117, 'id_embed_dim': 15, 'lr': 0.008814430375396134, 'batch_size': 64, 'alpha': 0.15535824506239235}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:51,464] Trial 2 finished with value: 0.5766007751226425 and parameters: {'hidden_dim': 105, 'id_embed_dim': 6, 'lr': 0.0015547601672354615, 'batch_size': 64, 'alpha': 0.15861584247916563}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:51,630] Trial 3 finished with value: 1.5530798435211182 and parameters: {'hidden_dim': 58, 'id_embed_dim': 16, 'lr': 0.00027751135861145296, 'batch_size': 64, 'alpha': 0.2889177410030744}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:51,755] Trial 4 finished with value: 0.9870100617408752 and parameters: {'hidden_dim': 51, 'id_embed_dim': 15, 'lr': 0.000854498808515667, 'batch_size': 128, 'alpha': 0.13761337807030233}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:52,008] Trial 5 finished with value: 0.7690834775567055 and parameters: {'hidden_dim': 93, 'id_embed_dim': 6, 'lr': 0.00900805475495009, 'batch_size': 32, 'alpha': 0.2401059502527144}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:52,132] Trial 6 finished with value: 1.0617163181304932 and parameters: {'hidden_dim': 57, 'id_embed_dim': 4, 'lr': 0.0007268022633656317, 'batch_size': 128, 'alpha': 0.10597268775788744}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:52,258] Trial 7 finished with value: 1.2906275391578674 and parameters: {'hidden_dim': 67, 'id_embed_dim': 7, 'lr': 0.0001865441745933778, 'batch_size': 128, 'alpha': 0.07894704033153857}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:52,393] Trial 8 finished with value: 1.1993943452835083 and parameters: {'hidden_dim': 105, 'id_embed_dim': 9, 'lr': 0.0002939405655727248, 'batch_size': 128, 'alpha': 0.09130478883914542}. Best is trial 1 with value: 0.5743939876556396.\n",
      "[I 2025-04-17 19:55:52,515] Trial 9 finished with value: 0.5729303658008575 and parameters: {'hidden_dim': 39, 'id_embed_dim': 13, 'lr': 0.0016452748642286665, 'batch_size': 128, 'alpha': 0.08988019803798564}. Best is trial 9 with value: 0.5729303658008575.\n",
      "[I 2025-04-17 19:55:52,754] Trial 10 finished with value: 0.08393737161532044 and parameters: {'hidden_dim': 32, 'id_embed_dim': 12, 'lr': 0.0028149940319926008, 'batch_size': 32, 'alpha': 0.016069152918494062}. Best is trial 10 with value: 0.08393737161532044.\n",
      "[I 2025-04-17 19:55:52,989] Trial 11 finished with value: 0.10357045289129019 and parameters: {'hidden_dim': 36, 'id_embed_dim': 12, 'lr': 0.0025527369375800486, 'batch_size': 32, 'alpha': 0.02011515311914558}. Best is trial 10 with value: 0.08393737161532044.\n",
      "[I 2025-04-17 19:55:53,226] Trial 12 finished with value: 0.07898129522800446 and parameters: {'hidden_dim': 35, 'id_embed_dim': 12, 'lr': 0.004092997814892129, 'batch_size': 32, 'alpha': 0.015220859844908062}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:53,459] Trial 13 finished with value: 0.08408407401293516 and parameters: {'hidden_dim': 33, 'id_embed_dim': 11, 'lr': 0.0043711860637456156, 'batch_size': 32, 'alpha': 0.015598602841779552}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:53,712] Trial 14 finished with value: 0.21905013918876648 and parameters: {'hidden_dim': 74, 'id_embed_dim': 13, 'lr': 0.0041554541497660444, 'batch_size': 32, 'alpha': 0.04762524057862886}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:53,956] Trial 15 finished with value: 0.2521090731024742 and parameters: {'hidden_dim': 45, 'id_embed_dim': 11, 'lr': 0.004102925117292241, 'batch_size': 32, 'alpha': 0.052653099161311376}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:54,213] Trial 16 finished with value: 0.6672439873218536 and parameters: {'hidden_dim': 66, 'id_embed_dim': 10, 'lr': 0.00239811448967888, 'batch_size': 32, 'alpha': 0.22123021847701352}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:54,462] Trial 17 finished with value: 0.25408664532005787 and parameters: {'hidden_dim': 47, 'id_embed_dim': 12, 'lr': 0.005166619637566261, 'batch_size': 32, 'alpha': 0.05316263210050469}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:54,719] Trial 18 finished with value: 0.6033386364579201 and parameters: {'hidden_dim': 83, 'id_embed_dim': 14, 'lr': 0.001308970918183712, 'batch_size': 32, 'alpha': 0.19369561301546412}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:54,989] Trial 19 finished with value: 1.2144692242145538 and parameters: {'hidden_dim': 127, 'id_embed_dim': 10, 'lr': 0.00010277589448641646, 'batch_size': 32, 'alpha': 0.12037250895899886}. Best is trial 12 with value: 0.07898129522800446.\n",
      "[I 2025-04-17 19:55:55,240] Trial 20 finished with value: 0.050522041507065296 and parameters: {'hidden_dim': 41, 'id_embed_dim': 13, 'lr': 0.002990305228524454, 'batch_size': 32, 'alpha': 0.010469047193386061}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:55,477] Trial 21 finished with value: 0.10750630777329206 and parameters: {'hidden_dim': 32, 'id_embed_dim': 13, 'lr': 0.002550270758771422, 'batch_size': 32, 'alpha': 0.020876818260114567}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:55,727] Trial 22 finished with value: 0.05242922576144338 and parameters: {'hidden_dim': 44, 'id_embed_dim': 12, 'lr': 0.00674828648715818, 'batch_size': 32, 'alpha': 0.01047728081556263}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:55,969] Trial 23 finished with value: 0.21894664876163006 and parameters: {'hidden_dim': 43, 'id_embed_dim': 14, 'lr': 0.00571615727830607, 'batch_size': 32, 'alpha': 0.048173183245152254}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:56,215] Trial 24 finished with value: 0.3167676515877247 and parameters: {'hidden_dim': 53, 'id_embed_dim': 11, 'lr': 0.006874713024262113, 'batch_size': 32, 'alpha': 0.06960247836213293}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:56,475] Trial 25 finished with value: 0.18279249779880047 and parameters: {'hidden_dim': 63, 'id_embed_dim': 9, 'lr': 0.0032168528116414963, 'batch_size': 32, 'alpha': 0.037277435856271704}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:56,742] Trial 26 finished with value: 0.2975310683250427 and parameters: {'hidden_dim': 45, 'id_embed_dim': 14, 'lr': 0.006467747490182499, 'batch_size': 32, 'alpha': 0.06515499853870113}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:56,998] Trial 27 finished with value: 0.1900834571570158 and parameters: {'hidden_dim': 78, 'id_embed_dim': 16, 'lr': 0.009775113728910139, 'batch_size': 32, 'alpha': 0.03574289462215981}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:57,244] Trial 28 finished with value: 0.05740249902009964 and parameters: {'hidden_dim': 40, 'id_embed_dim': 12, 'lr': 0.0011966423379062118, 'batch_size': 32, 'alpha': 0.01046967281533016}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:57,417] Trial 29 finished with value: 0.6972466856241226 and parameters: {'hidden_dim': 52, 'id_embed_dim': 9, 'lr': 0.0007202013138188763, 'batch_size': 64, 'alpha': 0.10547793572164177}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:57,586] Trial 30 finished with value: 0.4430115595459938 and parameters: {'hidden_dim': 42, 'id_embed_dim': 15, 'lr': 0.0005210198684201092, 'batch_size': 64, 'alpha': 0.03688290132918097}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:57,830] Trial 31 finished with value: 0.05990464938804507 and parameters: {'hidden_dim': 39, 'id_embed_dim': 12, 'lr': 0.001853485754159034, 'batch_size': 32, 'alpha': 0.012294701738158307}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:58,067] Trial 32 finished with value: 0.15539257507771254 and parameters: {'hidden_dim': 40, 'id_embed_dim': 13, 'lr': 0.0017480456569502385, 'batch_size': 32, 'alpha': 0.03059355517595707}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:58,317] Trial 33 finished with value: 0.06494499463588 and parameters: {'hidden_dim': 59, 'id_embed_dim': 11, 'lr': 0.0011451475003188636, 'batch_size': 32, 'alpha': 0.0126163892299706}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:58,487] Trial 34 finished with value: 0.3007395565509796 and parameters: {'hidden_dim': 50, 'id_embed_dim': 12, 'lr': 0.002134475323774098, 'batch_size': 64, 'alpha': 0.06372423049960935}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:58,765] Trial 35 finished with value: 0.17570434883236885 and parameters: {'hidden_dim': 39, 'id_embed_dim': 14, 'lr': 0.0009782403593514518, 'batch_size': 32, 'alpha': 0.032647638980715446}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:59,013] Trial 36 finished with value: 0.48756228387355804 and parameters: {'hidden_dim': 54, 'id_embed_dim': 15, 'lr': 0.001770982889954418, 'batch_size': 32, 'alpha': 0.13980086164126584}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:59,184] Trial 37 finished with value: 0.5379989296197891 and parameters: {'hidden_dim': 72, 'id_embed_dim': 10, 'lr': 0.0032326746875995234, 'batch_size': 64, 'alpha': 0.16159495173419958}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:59,315] Trial 38 finished with value: 0.9921905994415283 and parameters: {'hidden_dim': 59, 'id_embed_dim': 13, 'lr': 0.0005493986232891054, 'batch_size': 128, 'alpha': 0.08187159671510869}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:59,576] Trial 39 finished with value: 0.7701846286654472 and parameters: {'hidden_dim': 91, 'id_embed_dim': 8, 'lr': 0.007476501894564729, 'batch_size': 32, 'alpha': 0.2631503880096728}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:59,705] Trial 40 finished with value: 0.9153576493263245 and parameters: {'hidden_dim': 48, 'id_embed_dim': 11, 'lr': 0.0013604660020761733, 'batch_size': 128, 'alpha': 0.1786846197814706}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:55:59,956] Trial 41 finished with value: 0.05955635383725166 and parameters: {'hidden_dim': 59, 'id_embed_dim': 11, 'lr': 0.0011173393467253546, 'batch_size': 32, 'alpha': 0.011166692484694348}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:00,195] Trial 42 finished with value: 0.14677431713789701 and parameters: {'hidden_dim': 37, 'id_embed_dim': 12, 'lr': 0.0020238381048152377, 'batch_size': 32, 'alpha': 0.02821637577325888}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:00,438] Trial 43 finished with value: 0.06665629288181663 and parameters: {'hidden_dim': 43, 'id_embed_dim': 11, 'lr': 0.0009013834991779036, 'batch_size': 32, 'alpha': 0.010665150081814146}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:00,681] Trial 44 finished with value: 0.2680319622159004 and parameters: {'hidden_dim': 49, 'id_embed_dim': 12, 'lr': 0.0006158777369352801, 'batch_size': 32, 'alpha': 0.041042494909350954}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:00,931] Trial 45 finished with value: 0.05088962847366929 and parameters: {'hidden_dim': 62, 'id_embed_dim': 13, 'lr': 0.001156367889943854, 'batch_size': 32, 'alpha': 0.010173870688713607}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:01,182] Trial 46 finished with value: 0.14042283315211535 and parameters: {'hidden_dim': 64, 'id_embed_dim': 13, 'lr': 0.0011257050545911268, 'batch_size': 32, 'alpha': 0.026057649533910675}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:01,316] Trial 47 finished with value: 0.9586246013641357 and parameters: {'hidden_dim': 85, 'id_embed_dim': 14, 'lr': 0.0003257123155183358, 'batch_size': 128, 'alpha': 0.05798954323094284}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:01,568] Trial 48 finished with value: 0.3366126399487257 and parameters: {'hidden_dim': 69, 'id_embed_dim': 5, 'lr': 0.0014428647536053974, 'batch_size': 32, 'alpha': 0.07399853129798928}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:01,740] Trial 49 finished with value: 0.3263557367026806 and parameters: {'hidden_dim': 55, 'id_embed_dim': 15, 'lr': 0.0007616961075963062, 'batch_size': 64, 'alpha': 0.04462772034630455}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:02,005] Trial 50 finished with value: 0.1975296102464199 and parameters: {'hidden_dim': 105, 'id_embed_dim': 10, 'lr': 0.00038124620999406246, 'batch_size': 32, 'alpha': 0.026014312062801193}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:02,244] Trial 51 finished with value: 0.06878265552222729 and parameters: {'hidden_dim': 36, 'id_embed_dim': 12, 'lr': 0.0012144464196499182, 'batch_size': 32, 'alpha': 0.012097005837551295}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:02,495] Trial 52 finished with value: 0.11698723770678043 and parameters: {'hidden_dim': 62, 'id_embed_dim': 13, 'lr': 0.0034060395907977944, 'batch_size': 32, 'alpha': 0.02218604065614722}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:02,732] Trial 53 finished with value: 0.050584346521645784 and parameters: {'hidden_dim': 40, 'id_embed_dim': 11, 'lr': 0.0019436784149606898, 'batch_size': 32, 'alpha': 0.010335271794742222}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:02,976] Trial 54 finished with value: 0.23219604045152664 and parameters: {'hidden_dim': 47, 'id_embed_dim': 11, 'lr': 0.001482873434935814, 'batch_size': 32, 'alpha': 0.04794267055408333}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:03,224] Trial 55 finished with value: 0.7593124061822891 and parameters: {'hidden_dim': 58, 'id_embed_dim': 10, 'lr': 0.0010384354370392066, 'batch_size': 32, 'alpha': 0.2910508467915037}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:03,460] Trial 56 finished with value: 0.12617065850645304 and parameters: {'hidden_dim': 34, 'id_embed_dim': 11, 'lr': 0.002327195469372216, 'batch_size': 32, 'alpha': 0.023855709104156155}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:03,702] Trial 57 finished with value: 0.26930419355630875 and parameters: {'hidden_dim': 46, 'id_embed_dim': 8, 'lr': 0.0029289489890831568, 'batch_size': 32, 'alpha': 0.058411119318168}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:03,832] Trial 58 finished with value: 0.3844992518424988 and parameters: {'hidden_dim': 52, 'id_embed_dim': 12, 'lr': 0.00541477452992099, 'batch_size': 128, 'alpha': 0.09359630311668456}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:04,078] Trial 59 finished with value: 0.19880402088165283 and parameters: {'hidden_dim': 42, 'id_embed_dim': 13, 'lr': 0.0008483700865013995, 'batch_size': 32, 'alpha': 0.03776022703612079}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:04,400] Trial 60 finished with value: 0.10639043804258108 and parameters: {'hidden_dim': 76, 'id_embed_dim': 9, 'lr': 0.007968467850378944, 'batch_size': 32, 'alpha': 0.020392532123050956}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:04,637] Trial 61 finished with value: 0.056009816471487284 and parameters: {'hidden_dim': 38, 'id_embed_dim': 12, 'lr': 0.001880974897691156, 'batch_size': 32, 'alpha': 0.011095517932861173}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:04,872] Trial 62 finished with value: 0.06454971572384238 and parameters: {'hidden_dim': 32, 'id_embed_dim': 13, 'lr': 0.0016420992038430624, 'batch_size': 32, 'alpha': 0.012154239086499475}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:05,110] Trial 63 finished with value: 0.151848747394979 and parameters: {'hidden_dim': 37, 'id_embed_dim': 14, 'lr': 0.004609757445935878, 'batch_size': 32, 'alpha': 0.03034453541546403}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:05,351] Trial 64 finished with value: 0.10952220484614372 and parameters: {'hidden_dim': 41, 'id_embed_dim': 12, 'lr': 0.0020412096483216355, 'batch_size': 32, 'alpha': 0.021085763300711116}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:05,593] Trial 65 finished with value: 0.22985221445560455 and parameters: {'hidden_dim': 46, 'id_embed_dim': 11, 'lr': 0.003753342914925785, 'batch_size': 32, 'alpha': 0.04649068514311115}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:05,835] Trial 66 finished with value: 0.6323834657669067 and parameters: {'hidden_dim': 43, 'id_embed_dim': 10, 'lr': 0.002856595661347606, 'batch_size': 32, 'alpha': 0.21170077154384787}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:06,085] Trial 67 finished with value: 0.18478931207209826 and parameters: {'hidden_dim': 56, 'id_embed_dim': 13, 'lr': 0.0013295893597964374, 'batch_size': 32, 'alpha': 0.036308889980676405}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:06,331] Trial 68 finished with value: 0.05522735230624676 and parameters: {'hidden_dim': 50, 'id_embed_dim': 12, 'lr': 0.0023787371517791024, 'batch_size': 32, 'alpha': 0.01113588996948207}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:06,509] Trial 69 finished with value: 0.11296183802187443 and parameters: {'hidden_dim': 50, 'id_embed_dim': 12, 'lr': 0.002482541683138001, 'batch_size': 64, 'alpha': 0.02098118550714917}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:06,784] Trial 70 finished with value: 0.26219253428280354 and parameters: {'hidden_dim': 35, 'id_embed_dim': 13, 'lr': 0.0015828822296981133, 'batch_size': 32, 'alpha': 0.05471898703823722}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:07,030] Trial 71 finished with value: 0.07322991359978914 and parameters: {'hidden_dim': 39, 'id_embed_dim': 11, 'lr': 0.0019363865764421126, 'batch_size': 32, 'alpha': 0.014166546227738244}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:07,277] Trial 72 finished with value: 0.15060623362660408 and parameters: {'hidden_dim': 44, 'id_embed_dim': 12, 'lr': 0.0022621465727235136, 'batch_size': 32, 'alpha': 0.028823688262663065}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:07,530] Trial 73 finished with value: 0.05504886107519269 and parameters: {'hidden_dim': 49, 'id_embed_dim': 14, 'lr': 0.001030466850809905, 'batch_size': 32, 'alpha': 0.0104064946693568}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:07,774] Trial 74 finished with value: 0.23236088454723358 and parameters: {'hidden_dim': 39, 'id_embed_dim': 14, 'lr': 0.0009844777948177597, 'batch_size': 32, 'alpha': 0.0421036656298862}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:08,019] Trial 75 finished with value: 0.10142255295068026 and parameters: {'hidden_dim': 51, 'id_embed_dim': 14, 'lr': 0.0012386975779388992, 'batch_size': 32, 'alpha': 0.01920230721780559}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:08,263] Trial 76 finished with value: 0.1550852358341217 and parameters: {'hidden_dim': 48, 'id_embed_dim': 16, 'lr': 0.0027091805425703886, 'batch_size': 32, 'alpha': 0.03204725791302231}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:08,507] Trial 77 finished with value: 0.12582926079630852 and parameters: {'hidden_dim': 45, 'id_embed_dim': 13, 'lr': 0.0006468854738515952, 'batch_size': 32, 'alpha': 0.017753624853685066}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:08,637] Trial 78 finished with value: 0.055031685158610344 and parameters: {'hidden_dim': 54, 'id_embed_dim': 15, 'lr': 0.004753183293246477, 'batch_size': 128, 'alpha': 0.011005169633115968}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:08,768] Trial 79 finished with value: 0.7134899199008942 and parameters: {'hidden_dim': 54, 'id_embed_dim': 15, 'lr': 0.0034360167490876934, 'batch_size': 128, 'alpha': 0.2564556042656843}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:08,900] Trial 80 finished with value: 0.48007889091968536 and parameters: {'hidden_dim': 62, 'id_embed_dim': 15, 'lr': 0.004869704795110647, 'batch_size': 128, 'alpha': 0.12885944644971434}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:09,029] Trial 81 finished with value: 0.05135465785861015 and parameters: {'hidden_dim': 49, 'id_embed_dim': 14, 'lr': 0.005845983539432255, 'batch_size': 128, 'alpha': 0.010380786921716}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:09,159] Trial 82 finished with value: 0.14454679191112518 and parameters: {'hidden_dim': 50, 'id_embed_dim': 15, 'lr': 0.005953004514570601, 'batch_size': 128, 'alpha': 0.028220102976277334}. Best is trial 20 with value: 0.050522041507065296.\n",
      "[I 2025-04-17 19:56:09,289] Trial 83 finished with value: 0.04883657582104206 and parameters: {'hidden_dim': 56, 'id_embed_dim': 14, 'lr': 0.00830903500407986, 'batch_size': 128, 'alpha': 0.010179025617234596}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:09,422] Trial 84 finished with value: 0.1884947568178177 and parameters: {'hidden_dim': 67, 'id_embed_dim': 14, 'lr': 0.008162818409426939, 'batch_size': 128, 'alpha': 0.037626040873978854}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:09,552] Trial 85 finished with value: 0.11541066691279411 and parameters: {'hidden_dim': 56, 'id_embed_dim': 16, 'lr': 0.00972518350468385, 'batch_size': 128, 'alpha': 0.02199742864763144}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:09,682] Trial 86 finished with value: 0.09381759911775589 and parameters: {'hidden_dim': 53, 'id_embed_dim': 14, 'lr': 0.006388226538605793, 'batch_size': 128, 'alpha': 0.018165129150888387}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:09,813] Trial 87 finished with value: 0.17255451530218124 and parameters: {'hidden_dim': 59, 'id_embed_dim': 14, 'lr': 0.007204311727353523, 'batch_size': 128, 'alpha': 0.03263791591848922}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:09,949] Trial 88 finished with value: 0.7625041007995605 and parameters: {'hidden_dim': 48, 'id_embed_dim': 15, 'lr': 0.004346101667036869, 'batch_size': 128, 'alpha': 0.28173671082546686}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,081] Trial 89 finished with value: 0.13108707964420319 and parameters: {'hidden_dim': 61, 'id_embed_dim': 13, 'lr': 0.0052487955832402405, 'batch_size': 128, 'alpha': 0.025130917065154364}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,213] Trial 90 finished with value: 0.2419658899307251 and parameters: {'hidden_dim': 70, 'id_embed_dim': 15, 'lr': 0.0061107567680205935, 'batch_size': 128, 'alpha': 0.05167761371019002}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,342] Trial 91 finished with value: 0.05521375685930252 and parameters: {'hidden_dim': 44, 'id_embed_dim': 14, 'lr': 0.0037722094356840413, 'batch_size': 128, 'alpha': 0.010805286798487893}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,470] Trial 92 finished with value: 0.08600281178951263 and parameters: {'hidden_dim': 44, 'id_embed_dim': 14, 'lr': 0.003672457662528036, 'batch_size': 128, 'alpha': 0.016421753131433486}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,603] Trial 93 finished with value: 0.21316442638635635 and parameters: {'hidden_dim': 65, 'id_embed_dim': 13, 'lr': 0.008642679049773595, 'batch_size': 128, 'alpha': 0.04305255362749616}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,734] Trial 94 finished with value: 0.14354408532381058 and parameters: {'hidden_dim': 53, 'id_embed_dim': 14, 'lr': 0.0038234030577996314, 'batch_size': 128, 'alpha': 0.02613147033674497}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,862] Trial 95 finished with value: 0.09747258946299553 and parameters: {'hidden_dim': 41, 'id_embed_dim': 14, 'lr': 0.0030699356846685018, 'batch_size': 128, 'alpha': 0.016846056353388955}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:10,993] Trial 96 finished with value: 0.05824713222682476 and parameters: {'hidden_dim': 57, 'id_embed_dim': 16, 'lr': 0.006917565094732239, 'batch_size': 128, 'alpha': 0.011970343208053706}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:11,138] Trial 97 finished with value: 0.17861425876617432 and parameters: {'hidden_dim': 127, 'id_embed_dim': 15, 'lr': 0.004823207466878945, 'batch_size': 128, 'alpha': 0.03412796657087165}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:11,307] Trial 98 finished with value: 0.7595278769731522 and parameters: {'hidden_dim': 47, 'id_embed_dim': 13, 'lr': 0.0001255037716481441, 'batch_size': 64, 'alpha': 0.025190543908323315}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:11,437] Trial 99 finished with value: 0.09095675870776176 and parameters: {'hidden_dim': 51, 'id_embed_dim': 14, 'lr': 0.005563646873777583, 'batch_size': 128, 'alpha': 0.016906843062563702}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:11,577] Trial 100 finished with value: 0.052975477650761604 and parameters: {'hidden_dim': 102, 'id_embed_dim': 15, 'lr': 0.004143254394538904, 'batch_size': 128, 'alpha': 0.011037904301272768}. Best is trial 83 with value: 0.04883657582104206.\n",
      "[I 2025-04-17 19:56:11,723] Trial 101 finished with value: 0.04876003414392471 and parameters: {'hidden_dim': 114, 'id_embed_dim': 15, 'lr': 0.004681094893387186, 'batch_size': 128, 'alpha': 0.010041902435707507}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:11,864] Trial 102 finished with value: 0.05423063598573208 and parameters: {'hidden_dim': 104, 'id_embed_dim': 15, 'lr': 0.004111060743452437, 'batch_size': 128, 'alpha': 0.01106137678292239}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:12,004] Trial 103 finished with value: 0.11842392385005951 and parameters: {'hidden_dim': 102, 'id_embed_dim': 16, 'lr': 0.005681185177250834, 'batch_size': 128, 'alpha': 0.0226614705728228}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:12,147] Trial 104 finished with value: 0.15935018658638 and parameters: {'hidden_dim': 116, 'id_embed_dim': 15, 'lr': 0.004229199951238828, 'batch_size': 128, 'alpha': 0.031036319932956233}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:12,292] Trial 105 finished with value: 0.19227801263332367 and parameters: {'hidden_dim': 114, 'id_embed_dim': 15, 'lr': 0.004987805835962582, 'batch_size': 128, 'alpha': 0.039078820611889456}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:12,435] Trial 106 finished with value: 0.049261124804615974 and parameters: {'hidden_dim': 121, 'id_embed_dim': 16, 'lr': 0.006600238346826338, 'batch_size': 128, 'alpha': 0.010001832207875162}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:12,579] Trial 107 finished with value: 0.09799585863947868 and parameters: {'hidden_dim': 120, 'id_embed_dim': 16, 'lr': 0.0076184977152895916, 'batch_size': 128, 'alpha': 0.019240250555973823}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:12,723] Trial 108 finished with value: 0.14360368251800537 and parameters: {'hidden_dim': 122, 'id_embed_dim': 16, 'lr': 0.006550778438468459, 'batch_size': 128, 'alpha': 0.026778623676792283}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:12,863] Trial 109 finished with value: 0.5498628169298172 and parameters: {'hidden_dim': 112, 'id_embed_dim': 16, 'lr': 0.004527821514682215, 'batch_size': 128, 'alpha': 0.16779563447542042}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:13,004] Trial 110 finished with value: 0.07756970450282097 and parameters: {'hidden_dim': 98, 'id_embed_dim': 15, 'lr': 0.00405220798360121, 'batch_size': 128, 'alpha': 0.015393776222674196}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:13,157] Trial 111 finished with value: 0.05231437087059021 and parameters: {'hidden_dim': 108, 'id_embed_dim': 15, 'lr': 0.006837797578049806, 'batch_size': 128, 'alpha': 0.0103046274608872}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:13,300] Trial 112 finished with value: 0.0903637632727623 and parameters: {'hidden_dim': 109, 'id_embed_dim': 15, 'lr': 0.008869601947062017, 'batch_size': 128, 'alpha': 0.017114136953168536}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:13,442] Trial 113 finished with value: 0.17921200394630432 and parameters: {'hidden_dim': 108, 'id_embed_dim': 16, 'lr': 0.006087494071846256, 'batch_size': 128, 'alpha': 0.03288562373051899}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:13,583] Trial 114 finished with value: 0.049205953255295753 and parameters: {'hidden_dim': 100, 'id_embed_dim': 15, 'lr': 0.007008915148300382, 'batch_size': 128, 'alpha': 0.010117129645339914}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:13,725] Trial 115 finished with value: 0.12918641418218613 and parameters: {'hidden_dim': 102, 'id_embed_dim': 15, 'lr': 0.006997496842632299, 'batch_size': 128, 'alpha': 0.024278055960440716}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:13,865] Trial 116 finished with value: 0.10426836833357811 and parameters: {'hidden_dim': 95, 'id_embed_dim': 16, 'lr': 0.0097726158595732, 'batch_size': 128, 'alpha': 0.019850075792839313}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:14,007] Trial 117 finished with value: 0.14493947103619576 and parameters: {'hidden_dim': 107, 'id_embed_dim': 15, 'lr': 0.008379969636267939, 'batch_size': 128, 'alpha': 0.028229963879996753}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:14,150] Trial 118 finished with value: 0.054282840341329575 and parameters: {'hidden_dim': 111, 'id_embed_dim': 15, 'lr': 0.007534885445602351, 'batch_size': 128, 'alpha': 0.010501073538568977}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:14,335] Trial 119 finished with value: 0.2069106623530388 and parameters: {'hidden_dim': 103, 'id_embed_dim': 15, 'lr': 0.005368620833645548, 'batch_size': 64, 'alpha': 0.041104157314001445}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:14,477] Trial 120 finished with value: 0.5120398104190826 and parameters: {'hidden_dim': 99, 'id_embed_dim': 16, 'lr': 0.005936612727343547, 'batch_size': 128, 'alpha': 0.14864843979789108}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:14,620] Trial 121 finished with value: 0.086821548640728 and parameters: {'hidden_dim': 110, 'id_embed_dim': 15, 'lr': 0.007832534310663062, 'batch_size': 128, 'alpha': 0.016904364516006073}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:14,766] Trial 122 finished with value: 0.05996336787939072 and parameters: {'hidden_dim': 118, 'id_embed_dim': 15, 'lr': 0.006457889148882562, 'batch_size': 128, 'alpha': 0.011347586659132842}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:14,908] Trial 123 finished with value: 0.11218243837356567 and parameters: {'hidden_dim': 106, 'id_embed_dim': 14, 'lr': 0.007439301505987214, 'batch_size': 128, 'alpha': 0.02144969209554308}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:15,054] Trial 124 finished with value: 0.4417073279619217 and parameters: {'hidden_dim': 124, 'id_embed_dim': 15, 'lr': 0.006985051212191012, 'batch_size': 128, 'alpha': 0.1111990766548492}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:15,199] Trial 125 finished with value: 0.08764978870749474 and parameters: {'hidden_dim': 111, 'id_embed_dim': 14, 'lr': 0.00883894828456715, 'batch_size': 128, 'alpha': 0.016998618144519285}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:15,339] Trial 126 finished with value: 0.053194884210824966 and parameters: {'hidden_dim': 89, 'id_embed_dim': 16, 'lr': 0.005176786266500589, 'batch_size': 128, 'alpha': 0.010763054262381179}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:15,477] Trial 127 finished with value: 0.12928015738725662 and parameters: {'hidden_dim': 85, 'id_embed_dim': 16, 'lr': 0.00522586257882787, 'batch_size': 128, 'alpha': 0.024732826903822343}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:15,616] Trial 128 finished with value: 0.1822042316198349 and parameters: {'hidden_dim': 89, 'id_embed_dim': 16, 'lr': 0.003276187426611239, 'batch_size': 128, 'alpha': 0.03573168577934762}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:15,759] Trial 129 finished with value: 0.05302244797348976 and parameters: {'hidden_dim': 99, 'id_embed_dim': 13, 'lr': 0.004464280764419048, 'batch_size': 128, 'alpha': 0.010509800667041049}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:15,900] Trial 130 finished with value: 0.15968547761440277 and parameters: {'hidden_dim': 100, 'id_embed_dim': 13, 'lr': 0.00556410277265553, 'batch_size': 128, 'alpha': 0.030594803890337335}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:16,041] Trial 131 finished with value: 0.08913573622703552 and parameters: {'hidden_dim': 94, 'id_embed_dim': 13, 'lr': 0.004090767123504698, 'batch_size': 128, 'alpha': 0.01684134112054127}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:16,181] Trial 132 finished with value: 0.05128107778728008 and parameters: {'hidden_dim': 104, 'id_embed_dim': 14, 'lr': 0.004473031110740719, 'batch_size': 128, 'alpha': 0.010060032919690577}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:16,323] Trial 133 finished with value: 0.12485610693693161 and parameters: {'hidden_dim': 97, 'id_embed_dim': 14, 'lr': 0.004845034114523838, 'batch_size': 128, 'alpha': 0.024319549236878426}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:16,467] Trial 134 finished with value: 0.05385191924870014 and parameters: {'hidden_dim': 115, 'id_embed_dim': 13, 'lr': 0.0034615475315098155, 'batch_size': 128, 'alpha': 0.010321068179335032}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:16,605] Trial 135 finished with value: 0.11015097051858902 and parameters: {'hidden_dim': 88, 'id_embed_dim': 12, 'lr': 0.006495398674754673, 'batch_size': 128, 'alpha': 0.020452216516797773}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:16,787] Trial 136 finished with value: 0.08782818540930748 and parameters: {'hidden_dim': 91, 'id_embed_dim': 4, 'lr': 0.005904073099732276, 'batch_size': 64, 'alpha': 0.0160432112701896}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:16,936] Trial 137 finished with value: 0.623266726732254 and parameters: {'hidden_dim': 101, 'id_embed_dim': 13, 'lr': 0.00441466507848701, 'batch_size': 128, 'alpha': 0.2040455872768075}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:17,075] Trial 138 finished with value: 0.1559864580631256 and parameters: {'hidden_dim': 96, 'id_embed_dim': 14, 'lr': 0.002753587349707364, 'batch_size': 128, 'alpha': 0.02892461717528904}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:17,218] Trial 139 finished with value: 0.12458440661430359 and parameters: {'hidden_dim': 119, 'id_embed_dim': 7, 'lr': 0.005042150014617252, 'batch_size': 128, 'alpha': 0.022561987480372657}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:17,355] Trial 140 finished with value: 0.30905795097351074 and parameters: {'hidden_dim': 81, 'id_embed_dim': 13, 'lr': 0.0004365311272723764, 'batch_size': 128, 'alpha': 0.010000872269214247}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:17,503] Trial 141 finished with value: 0.05093292146921158 and parameters: {'hidden_dim': 114, 'id_embed_dim': 13, 'lr': 0.003710200787344617, 'batch_size': 128, 'alpha': 0.010322736129660835}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:17,736] Trial 142 finished with value: 0.08437729626893997 and parameters: {'hidden_dim': 105, 'id_embed_dim': 12, 'lr': 0.003554572513572562, 'batch_size': 128, 'alpha': 0.015837294504190517}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:17,889] Trial 143 finished with value: 0.09317556768655777 and parameters: {'hidden_dim': 114, 'id_embed_dim': 13, 'lr': 0.003023173147714, 'batch_size': 128, 'alpha': 0.017104206434345577}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:18,036] Trial 144 finished with value: 0.05261116102337837 and parameters: {'hidden_dim': 113, 'id_embed_dim': 14, 'lr': 0.004481846567701592, 'batch_size': 128, 'alpha': 0.010232460688345019}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:18,177] Trial 145 finished with value: 0.11857177689671516 and parameters: {'hidden_dim': 112, 'id_embed_dim': 14, 'lr': 0.0045229759807538655, 'batch_size': 128, 'alpha': 0.02217824265860465}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:18,318] Trial 146 finished with value: 0.6864699721336365 and parameters: {'hidden_dim': 108, 'id_embed_dim': 14, 'lr': 0.006794938350194052, 'batch_size': 128, 'alpha': 0.2419652189334338}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:18,588] Trial 147 finished with value: 0.15247918292880058 and parameters: {'hidden_dim': 120, 'id_embed_dim': 14, 'lr': 0.003770808259954702, 'batch_size': 32, 'alpha': 0.029962548362182498}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:18,732] Trial 148 finished with value: 0.05152612738311291 and parameters: {'hidden_dim': 124, 'id_embed_dim': 13, 'lr': 0.004032182045297548, 'batch_size': 128, 'alpha': 0.010150424386032742}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:19,005] Trial 149 finished with value: 0.09177482686936855 and parameters: {'hidden_dim': 124, 'id_embed_dim': 14, 'lr': 0.00316276423488023, 'batch_size': 32, 'alpha': 0.017855826239173762}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:19,249] Trial 150 finished with value: 0.13166537135839462 and parameters: {'hidden_dim': 126, 'id_embed_dim': 11, 'lr': 0.003875789382035138, 'batch_size': 128, 'alpha': 0.02493283818967116}. Best is trial 101 with value: 0.04876003414392471.\n",
      "[I 2025-04-17 19:56:19,397] Trial 151 finished with value: 0.04762543551623821 and parameters: {'hidden_dim': 117, 'id_embed_dim': 13, 'lr': 0.004299061288182261, 'batch_size': 128, 'alpha': 0.010137349176671492}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:19,546] Trial 152 finished with value: 0.0853319875895977 and parameters: {'hidden_dim': 122, 'id_embed_dim': 13, 'lr': 0.004051136146639208, 'batch_size': 128, 'alpha': 0.01652442803164123}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:19,693] Trial 153 finished with value: 0.08745547384023666 and parameters: {'hidden_dim': 114, 'id_embed_dim': 13, 'lr': 0.0026063381023857164, 'batch_size': 128, 'alpha': 0.015984157117777766}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:19,838] Trial 154 finished with value: 0.13060633093118668 and parameters: {'hidden_dim': 117, 'id_embed_dim': 12, 'lr': 0.005840223446857052, 'batch_size': 128, 'alpha': 0.024021653666070453}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:19,982] Trial 155 finished with value: 0.40775102376937866 and parameters: {'hidden_dim': 116, 'id_embed_dim': 14, 'lr': 0.000241006270767038, 'batch_size': 128, 'alpha': 0.010424789330121665}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:20,130] Trial 156 finished with value: 0.11334459483623505 and parameters: {'hidden_dim': 122, 'id_embed_dim': 13, 'lr': 0.004837535312159424, 'batch_size': 128, 'alpha': 0.020578947636534083}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:20,403] Trial 157 finished with value: 0.18633411452174187 and parameters: {'hidden_dim': 118, 'id_embed_dim': 13, 'lr': 0.007968296818451709, 'batch_size': 32, 'alpha': 0.03344571136247667}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:20,549] Trial 158 finished with value: 0.04859286732971668 and parameters: {'hidden_dim': 114, 'id_embed_dim': 14, 'lr': 0.0035239033779217535, 'batch_size': 128, 'alpha': 0.010026872308469785}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:20,778] Trial 159 finished with value: 0.04987642262130976 and parameters: {'hidden_dim': 113, 'id_embed_dim': 12, 'lr': 0.003514546419707598, 'batch_size': 64, 'alpha': 0.010038316817404034}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:20,963] Trial 160 finished with value: 0.13878808356821537 and parameters: {'hidden_dim': 110, 'id_embed_dim': 12, 'lr': 0.0032854494162272855, 'batch_size': 64, 'alpha': 0.02726294228778809}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:21,146] Trial 161 finished with value: 0.08291307464241982 and parameters: {'hidden_dim': 112, 'id_embed_dim': 12, 'lr': 0.0035534838080583443, 'batch_size': 64, 'alpha': 0.01660205525726693}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:21,333] Trial 162 finished with value: 0.08146468363702297 and parameters: {'hidden_dim': 113, 'id_embed_dim': 14, 'lr': 0.002852056459403907, 'batch_size': 64, 'alpha': 0.015451563397329053}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:21,478] Trial 163 finished with value: 0.05211425945162773 and parameters: {'hidden_dim': 120, 'id_embed_dim': 13, 'lr': 0.004514323483645179, 'batch_size': 128, 'alpha': 0.010350579368765324}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:21,665] Trial 164 finished with value: 0.11271978914737701 and parameters: {'hidden_dim': 120, 'id_embed_dim': 13, 'lr': 0.0054742516718524065, 'batch_size': 64, 'alpha': 0.021282982211926267}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:21,850] Trial 165 finished with value: 0.10973416827619076 and parameters: {'hidden_dim': 116, 'id_embed_dim': 13, 'lr': 0.002250728467279225, 'batch_size': 64, 'alpha': 0.021055286800603003}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:22,125] Trial 166 finished with value: 0.08105485932901502 and parameters: {'hidden_dim': 123, 'id_embed_dim': 12, 'lr': 0.006226819150128398, 'batch_size': 32, 'alpha': 0.015282599549337306}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:22,270] Trial 167 finished with value: 0.1491609700024128 and parameters: {'hidden_dim': 120, 'id_embed_dim': 11, 'lr': 0.0036779884718443298, 'batch_size': 128, 'alpha': 0.02809870911633574}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:22,416] Trial 168 finished with value: 0.05403222143650055 and parameters: {'hidden_dim': 126, 'id_embed_dim': 13, 'lr': 0.003133076843179371, 'batch_size': 128, 'alpha': 0.010733675334872872}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:22,658] Trial 169 finished with value: 0.0792689505033195 and parameters: {'hidden_dim': 42, 'id_embed_dim': 12, 'lr': 0.007066804088718464, 'batch_size': 32, 'alpha': 0.01533254096260761}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:22,806] Trial 170 finished with value: 0.11318042501807213 and parameters: {'hidden_dim': 117, 'id_embed_dim': 13, 'lr': 0.004942187964948883, 'batch_size': 128, 'alpha': 0.02124323837097305}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:22,950] Trial 171 finished with value: 0.051313064992427826 and parameters: {'hidden_dim': 115, 'id_embed_dim': 14, 'lr': 0.004379011930256126, 'batch_size': 128, 'alpha': 0.010806710341465044}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:23,096] Trial 172 finished with value: 0.05221460200846195 and parameters: {'hidden_dim': 115, 'id_embed_dim': 14, 'lr': 0.004062217271386466, 'batch_size': 128, 'alpha': 0.01012813922808653}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:23,242] Trial 173 finished with value: 0.08371059969067574 and parameters: {'hidden_dim': 115, 'id_embed_dim': 14, 'lr': 0.004341950339510951, 'batch_size': 128, 'alpha': 0.016250050309185}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:23,387] Trial 174 finished with value: 0.07931052148342133 and parameters: {'hidden_dim': 118, 'id_embed_dim': 14, 'lr': 0.0038529988740170734, 'batch_size': 128, 'alpha': 0.015076611469701844}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:23,529] Trial 175 finished with value: 0.04928736947476864 and parameters: {'hidden_dim': 108, 'id_embed_dim': 14, 'lr': 0.004219356772658632, 'batch_size': 128, 'alpha': 0.010128736989705135}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:23,675] Trial 176 finished with value: 0.1177130863070488 and parameters: {'hidden_dim': 115, 'id_embed_dim': 14, 'lr': 0.003528911452370536, 'batch_size': 128, 'alpha': 0.021481491620518236}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:23,821] Trial 177 finished with value: 0.050657667219638824 and parameters: {'hidden_dim': 111, 'id_embed_dim': 14, 'lr': 0.004147757047292716, 'batch_size': 128, 'alpha': 0.01017329233916003}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:23,972] Trial 178 finished with value: 0.13891392946243286 and parameters: {'hidden_dim': 128, 'id_embed_dim': 14, 'lr': 0.004245616823360899, 'batch_size': 128, 'alpha': 0.027459385016320857}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:24,118] Trial 179 finished with value: 0.10251989215612411 and parameters: {'hidden_dim': 109, 'id_embed_dim': 13, 'lr': 0.00459050196868824, 'batch_size': 128, 'alpha': 0.019926243259139772}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:24,262] Trial 180 finished with value: 0.19006607681512833 and parameters: {'hidden_dim': 106, 'id_embed_dim': 13, 'lr': 0.003281662250831672, 'batch_size': 128, 'alpha': 0.035911841249690844}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:24,424] Trial 181 finished with value: 0.056060804054141045 and parameters: {'hidden_dim': 113, 'id_embed_dim': 14, 'lr': 0.003934101723779324, 'batch_size': 128, 'alpha': 0.011120940233068472}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:24,587] Trial 182 finished with value: 0.08407644554972649 and parameters: {'hidden_dim': 111, 'id_embed_dim': 14, 'lr': 0.004121075060859932, 'batch_size': 128, 'alpha': 0.01661356671776397}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:24,735] Trial 183 finished with value: 0.07612238638103008 and parameters: {'hidden_dim': 121, 'id_embed_dim': 14, 'lr': 0.0035448687202707086, 'batch_size': 128, 'alpha': 0.014096712980407121}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:24,882] Trial 184 finished with value: 0.12349414825439453 and parameters: {'hidden_dim': 117, 'id_embed_dim': 14, 'lr': 0.004731951703831694, 'batch_size': 128, 'alpha': 0.023458852786750393}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:25,030] Trial 185 finished with value: 0.07791348919272423 and parameters: {'hidden_dim': 125, 'id_embed_dim': 13, 'lr': 0.005227857048155022, 'batch_size': 128, 'alpha': 0.015457082321372738}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:25,178] Trial 186 finished with value: 0.055271875113248825 and parameters: {'hidden_dim': 119, 'id_embed_dim': 14, 'lr': 0.0029520662324946842, 'batch_size': 128, 'alpha': 0.01071636462224352}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:25,324] Trial 187 finished with value: 0.05231957323849201 and parameters: {'hidden_dim': 115, 'id_embed_dim': 14, 'lr': 0.003978691564774446, 'batch_size': 128, 'alpha': 0.010273202305482666}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:25,499] Trial 188 finished with value: 0.11172068491578102 and parameters: {'hidden_dim': 110, 'id_embed_dim': 13, 'lr': 0.004454302843187532, 'batch_size': 128, 'alpha': 0.020524217156775236}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:25,638] Trial 189 finished with value: 0.048853885382413864 and parameters: {'hidden_dim': 75, 'id_embed_dim': 13, 'lr': 0.0036739733934293825, 'batch_size': 128, 'alpha': 0.010046449739129592}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:25,776] Trial 190 finished with value: 0.13844400644302368 and parameters: {'hidden_dim': 74, 'id_embed_dim': 13, 'lr': 0.0033755363525763155, 'batch_size': 128, 'alpha': 0.025850273895946584}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:25,926] Trial 191 finished with value: 0.08006710186600685 and parameters: {'hidden_dim': 114, 'id_embed_dim': 13, 'lr': 0.0038176171212271457, 'batch_size': 128, 'alpha': 0.01559103367743742}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:26,064] Trial 192 finished with value: 0.05359094589948654 and parameters: {'hidden_dim': 67, 'id_embed_dim': 14, 'lr': 0.004252657323740552, 'batch_size': 128, 'alpha': 0.010694664088643757}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:26,247] Trial 193 finished with value: 0.09736547619104385 and parameters: {'hidden_dim': 117, 'id_embed_dim': 13, 'lr': 0.0036305367569619876, 'batch_size': 128, 'alpha': 0.01877799856362537}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:26,392] Trial 194 finished with value: 0.07810129970312119 and parameters: {'hidden_dim': 112, 'id_embed_dim': 14, 'lr': 0.004784309690375953, 'batch_size': 128, 'alpha': 0.015123721725275856}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:26,537] Trial 195 finished with value: 0.06223543733358383 and parameters: {'hidden_dim': 108, 'id_embed_dim': 13, 'lr': 0.001679548991520069, 'batch_size': 128, 'alpha': 0.010440411384406868}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:26,722] Trial 196 finished with value: 0.1098709125071764 and parameters: {'hidden_dim': 104, 'id_embed_dim': 14, 'lr': 0.003053931633208676, 'batch_size': 64, 'alpha': 0.02140372199935325}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:26,868] Trial 197 finished with value: 0.37014733254909515 and parameters: {'hidden_dim': 119, 'id_embed_dim': 13, 'lr': 0.0025784042664224044, 'batch_size': 128, 'alpha': 0.08553549281696299}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:27,007] Trial 198 finished with value: 0.05206315591931343 and parameters: {'hidden_dim': 80, 'id_embed_dim': 14, 'lr': 0.004120898402518798, 'batch_size': 128, 'alpha': 0.01054091280632121}. Best is trial 151 with value: 0.04762543551623821.\n",
      "[I 2025-04-17 19:56:27,145] Trial 199 finished with value: 0.049734245985746384 and parameters: {'hidden_dim': 81, 'id_embed_dim': 15, 'lr': 0.0034016975494951473, 'batch_size': 128, 'alpha': 0.010150665584188706}. Best is trial 151 with value: 0.04762543551623821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'hidden_dim': 117, 'id_embed_dim': 13, 'lr': 0.004299061288182261, 'batch_size': 128, 'alpha': 0.010137349176671492}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, train_df_split, val_df_split, id_map, input_dim, id_count, output_dim, device), n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparams:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c89fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "model = RNNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    id_count=id_count,\n",
    "    id_embed_dim=best_params['id_embed_dim'],\n",
    "    output_dim=output_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = OrdinalLabelSmoothingLoss(num_classes=output_dim, alpha=best_params['alpha'])\n",
    "\n",
    "train_loader = DataLoader(MoodDataset(train_df_split, id_map), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(MoodDataset(val_df_split, id_map), batch_size=best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0bfbe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.4224, val loss = 0.3331\n",
      "Epoch 2: train loss = 0.2143, val loss = 0.1688\n",
      "Epoch 3: train loss = 0.1073, val loss = 0.0897\n",
      "Epoch 4: train loss = 0.0676, val loss = 0.0633\n",
      "Epoch 5: train loss = 0.0546, val loss = 0.0543\n",
      "Epoch 6: train loss = 0.0494, val loss = 0.0519\n",
      "Epoch 7: train loss = 0.0459, val loss = 0.0493\n",
      "Epoch 8: train loss = 0.0442, val loss = 0.0492\n",
      "Epoch 9: train loss = 0.0426, val loss = 0.0481\n",
      "Epoch 10: train loss = 0.0419, val loss = 0.0492\n",
      "Epoch 11: train loss = 0.0408, val loss = 0.0483\n",
      "Epoch 12: train loss = 0.0404, val loss = 0.0491\n",
      "Epoch 13: train loss = 0.0398, val loss = 0.0485\n",
      "Epoch 14: train loss = 0.0386, val loss = 0.0497\n",
      "Epoch 15: train loss = 0.0384, val loss = 0.0497\n",
      "Epoch 16: train loss = 0.0377, val loss = 0.0502\n",
      "Epoch 17: train loss = 0.0370, val loss = 0.0490\n",
      "Epoch 18: train loss = 0.0368, val loss = 0.0512\n",
      "Epoch 19: train loss = 0.0361, val loss = 0.0495\n",
      "Epoch 20: train loss = 0.0355, val loss = 0.0513\n",
      "Epoch 21: train loss = 0.0355, val loss = 0.0516\n",
      "Epoch 22: train loss = 0.0346, val loss = 0.0511\n",
      "Epoch 23: train loss = 0.0337, val loss = 0.0509\n",
      "Epoch 24: train loss = 0.0331, val loss = 0.0495\n",
      "Epoch 25: train loss = 0.0331, val loss = 0.0518\n",
      "Epoch 26: train loss = 0.0322, val loss = 0.0500\n",
      "Epoch 27: train loss = 0.0317, val loss = 0.0510\n",
      "Epoch 28: train loss = 0.0311, val loss = 0.0502\n",
      "Epoch 29: train loss = 0.0308, val loss = 0.0523\n",
      "Epoch 30: train loss = 0.0301, val loss = 0.0510\n",
      "Epoch 31: train loss = 0.0295, val loss = 0.0525\n",
      "Epoch 32: train loss = 0.0293, val loss = 0.0533\n",
      "Epoch 33: train loss = 0.0287, val loss = 0.0539\n",
      "Epoch 34: train loss = 0.0287, val loss = 0.0532\n",
      "Epoch 35: train loss = 0.0278, val loss = 0.0523\n",
      "Epoch 36: train loss = 0.0274, val loss = 0.0552\n",
      "Epoch 37: train loss = 0.0265, val loss = 0.0547\n",
      "Epoch 38: train loss = 0.0255, val loss = 0.0544\n",
      "Epoch 39: train loss = 0.0249, val loss = 0.0570\n",
      "Epoch 40: train loss = 0.0243, val loss = 0.0541\n",
      "Epoch 41: train loss = 0.0240, val loss = 0.0578\n",
      "Epoch 42: train loss = 0.0237, val loss = 0.0566\n",
      "Epoch 43: train loss = 0.0234, val loss = 0.0571\n",
      "Epoch 44: train loss = 0.0232, val loss = 0.0602\n",
      "Epoch 45: train loss = 0.0221, val loss = 0.0615\n",
      "Epoch 46: train loss = 0.0218, val loss = 0.0618\n",
      "Epoch 47: train loss = 0.0213, val loss = 0.0629\n",
      "Epoch 48: train loss = 0.0205, val loss = 0.0619\n",
      "Epoch 49: train loss = 0.0200, val loss = 0.0629\n",
      "Epoch 50: train loss = 0.0196, val loss = 0.0645\n",
      "Epoch 51: train loss = 0.0195, val loss = 0.0673\n",
      "Epoch 52: train loss = 0.0188, val loss = 0.0664\n",
      "Epoch 53: train loss = 0.0179, val loss = 0.0667\n",
      "Epoch 54: train loss = 0.0177, val loss = 0.0693\n",
      "Epoch 55: train loss = 0.0173, val loss = 0.0684\n",
      "Epoch 56: train loss = 0.0169, val loss = 0.0692\n",
      "Epoch 57: train loss = 0.0165, val loss = 0.0677\n",
      "Epoch 58: train loss = 0.0156, val loss = 0.0737\n",
      "Epoch 59: train loss = 0.0156, val loss = 0.0719\n",
      "Epoch 60: train loss = 0.0153, val loss = 0.0714\n",
      "Epoch 61: train loss = 0.0151, val loss = 0.0745\n",
      "Epoch 62: train loss = 0.0142, val loss = 0.0721\n",
      "Epoch 63: train loss = 0.0141, val loss = 0.0748\n",
      "Epoch 64: train loss = 0.0138, val loss = 0.0794\n",
      "Epoch 65: train loss = 0.0142, val loss = 0.0770\n",
      "Epoch 66: train loss = 0.0134, val loss = 0.0780\n",
      "Epoch 67: train loss = 0.0128, val loss = 0.0802\n",
      "Epoch 68: train loss = 0.0120, val loss = 0.0775\n",
      "Epoch 69: train loss = 0.0117, val loss = 0.0815\n",
      "Epoch 70: train loss = 0.0114, val loss = 0.0795\n",
      "Epoch 71: train loss = 0.0112, val loss = 0.0805\n",
      "Epoch 72: train loss = 0.0109, val loss = 0.0810\n",
      "Epoch 73: train loss = 0.0104, val loss = 0.0839\n",
      "Epoch 74: train loss = 0.0104, val loss = 0.0809\n",
      "Epoch 75: train loss = 0.0101, val loss = 0.0841\n",
      "Epoch 76: train loss = 0.0098, val loss = 0.0799\n",
      "Epoch 77: train loss = 0.0097, val loss = 0.0855\n",
      "Epoch 78: train loss = 0.0094, val loss = 0.0844\n",
      "Epoch 79: train loss = 0.0089, val loss = 0.0833\n",
      "Epoch 80: train loss = 0.0087, val loss = 0.0867\n",
      "Epoch 81: train loss = 0.0086, val loss = 0.0881\n",
      "Epoch 82: train loss = 0.0084, val loss = 0.0852\n",
      "Epoch 83: train loss = 0.0082, val loss = 0.0880\n",
      "Epoch 84: train loss = 0.0080, val loss = 0.0852\n",
      "Epoch 85: train loss = 0.0078, val loss = 0.0865\n",
      "Epoch 86: train loss = 0.0080, val loss = 0.0888\n",
      "Epoch 87: train loss = 0.0077, val loss = 0.0899\n",
      "Epoch 88: train loss = 0.0071, val loss = 0.0844\n",
      "Epoch 89: train loss = 0.0072, val loss = 0.0897\n",
      "Epoch 90: train loss = 0.0070, val loss = 0.0924\n",
      "Epoch 91: train loss = 0.0068, val loss = 0.0890\n",
      "Epoch 92: train loss = 0.0068, val loss = 0.0901\n",
      "Epoch 93: train loss = 0.0066, val loss = 0.0933\n",
      "Epoch 94: train loss = 0.0062, val loss = 0.0894\n",
      "Epoch 95: train loss = 0.0060, val loss = 0.0900\n",
      "Epoch 96: train loss = 0.0059, val loss = 0.0905\n",
      "Epoch 97: train loss = 0.0059, val loss = 0.0911\n",
      "Epoch 98: train loss = 0.0060, val loss = 0.0926\n",
      "Epoch 99: train loss = 0.0058, val loss = 0.0927\n",
      "Epoch 100: train loss = 0.0054, val loss = 0.0934\n",
      "Epoch 101: train loss = 0.0054, val loss = 0.0942\n",
      "Epoch 102: train loss = 0.0050, val loss = 0.0968\n",
      "Epoch 103: train loss = 0.0049, val loss = 0.0925\n",
      "Epoch 104: train loss = 0.0045, val loss = 0.0969\n",
      "Epoch 105: train loss = 0.0046, val loss = 0.0941\n",
      "Epoch 106: train loss = 0.0044, val loss = 0.0959\n",
      "Epoch 107: train loss = 0.0046, val loss = 0.0970\n",
      "Epoch 108: train loss = 0.0045, val loss = 0.0983\n",
      "Epoch 109: train loss = 0.0044, val loss = 0.0991\n",
      "Epoch 110: train loss = 0.0042, val loss = 0.0974\n",
      "Epoch 111: train loss = 0.0044, val loss = 0.0973\n",
      "Epoch 112: train loss = 0.0039, val loss = 0.0976\n",
      "Epoch 113: train loss = 0.0039, val loss = 0.0979\n",
      "Epoch 114: train loss = 0.0037, val loss = 0.0982\n",
      "Epoch 115: train loss = 0.0036, val loss = 0.0990\n",
      "Epoch 116: train loss = 0.0037, val loss = 0.0989\n",
      "Epoch 117: train loss = 0.0036, val loss = 0.1011\n",
      "Epoch 118: train loss = 0.0033, val loss = 0.0976\n",
      "Epoch 119: train loss = 0.0033, val loss = 0.1012\n",
      "Epoch 120: train loss = 0.0032, val loss = 0.1023\n",
      "Epoch 121: train loss = 0.0033, val loss = 0.1020\n",
      "Epoch 122: train loss = 0.0033, val loss = 0.1000\n",
      "Epoch 123: train loss = 0.0031, val loss = 0.1002\n",
      "Epoch 124: train loss = 0.0030, val loss = 0.1023\n",
      "Epoch 125: train loss = 0.0030, val loss = 0.1035\n",
      "Epoch 126: train loss = 0.0028, val loss = 0.1034\n",
      "Epoch 127: train loss = 0.0028, val loss = 0.1052\n",
      "Epoch 128: train loss = 0.0027, val loss = 0.1051\n",
      "Epoch 129: train loss = 0.0027, val loss = 0.1014\n",
      "Epoch 130: train loss = 0.0027, val loss = 0.1061\n",
      "Epoch 131: train loss = 0.0028, val loss = 0.1080\n",
      "Epoch 132: train loss = 0.0028, val loss = 0.1039\n",
      "Epoch 133: train loss = 0.0024, val loss = 0.1067\n",
      "Epoch 134: train loss = 0.0024, val loss = 0.1070\n",
      "Epoch 135: train loss = 0.0023, val loss = 0.1045\n",
      "Epoch 136: train loss = 0.0025, val loss = 0.1093\n",
      "Epoch 137: train loss = 0.0024, val loss = 0.1072\n",
      "Epoch 138: train loss = 0.0024, val loss = 0.1058\n",
      "Epoch 139: train loss = 0.0022, val loss = 0.1092\n",
      "Epoch 140: train loss = 0.0023, val loss = 0.1090\n",
      "Epoch 141: train loss = 0.0025, val loss = 0.1071\n",
      "Epoch 142: train loss = 0.0025, val loss = 0.1070\n",
      "Epoch 143: train loss = 0.0022, val loss = 0.1095\n",
      "Epoch 144: train loss = 0.0021, val loss = 0.1110\n",
      "Epoch 145: train loss = 0.0020, val loss = 0.1085\n",
      "Epoch 146: train loss = 0.0020, val loss = 0.1088\n",
      "Epoch 147: train loss = 0.0022, val loss = 0.1112\n",
      "Epoch 148: train loss = 0.0022, val loss = 0.1101\n",
      "Epoch 149: train loss = 0.0020, val loss = 0.1130\n",
      "Epoch 150: train loss = 0.0020, val loss = 0.1121\n",
      "Epoch 151: train loss = 0.0019, val loss = 0.1097\n",
      "Epoch 152: train loss = 0.0019, val loss = 0.1133\n",
      "Epoch 153: train loss = 0.0018, val loss = 0.1122\n",
      "Epoch 154: train loss = 0.0017, val loss = 0.1134\n",
      "Epoch 155: train loss = 0.0017, val loss = 0.1148\n",
      "Epoch 156: train loss = 0.0017, val loss = 0.1115\n",
      "Epoch 157: train loss = 0.0017, val loss = 0.1150\n",
      "Epoch 158: train loss = 0.0017, val loss = 0.1127\n",
      "Epoch 159: train loss = 0.0016, val loss = 0.1149\n",
      "Epoch 160: train loss = 0.0016, val loss = 0.1147\n",
      "Epoch 161: train loss = 0.0015, val loss = 0.1170\n",
      "Epoch 162: train loss = 0.0015, val loss = 0.1141\n",
      "Epoch 163: train loss = 0.0015, val loss = 0.1161\n",
      "Epoch 164: train loss = 0.0015, val loss = 0.1165\n",
      "Epoch 165: train loss = 0.0016, val loss = 0.1162\n",
      "Epoch 166: train loss = 0.0015, val loss = 0.1162\n",
      "Epoch 167: train loss = 0.0014, val loss = 0.1160\n",
      "Epoch 168: train loss = 0.0014, val loss = 0.1183\n",
      "Epoch 169: train loss = 0.0015, val loss = 0.1164\n",
      "Epoch 170: train loss = 0.0015, val loss = 0.1173\n",
      "Epoch 171: train loss = 0.0014, val loss = 0.1194\n",
      "Epoch 172: train loss = 0.0013, val loss = 0.1175\n",
      "Epoch 173: train loss = 0.0012, val loss = 0.1188\n",
      "Epoch 174: train loss = 0.0012, val loss = 0.1181\n",
      "Epoch 175: train loss = 0.0011, val loss = 0.1188\n",
      "Epoch 176: train loss = 0.0011, val loss = 0.1193\n",
      "Epoch 177: train loss = 0.0012, val loss = 0.1202\n",
      "Epoch 178: train loss = 0.0013, val loss = 0.1194\n",
      "Epoch 179: train loss = 0.0011, val loss = 0.1180\n",
      "Epoch 180: train loss = 0.0011, val loss = 0.1189\n",
      "Epoch 181: train loss = 0.0010, val loss = 0.1210\n",
      "Epoch 182: train loss = 0.0010, val loss = 0.1211\n",
      "Epoch 183: train loss = 0.0011, val loss = 0.1204\n",
      "Epoch 184: train loss = 0.0010, val loss = 0.1201\n",
      "Epoch 185: train loss = 0.0010, val loss = 0.1202\n",
      "Epoch 186: train loss = 0.0010, val loss = 0.1211\n",
      "Epoch 187: train loss = 0.0010, val loss = 0.1206\n",
      "Epoch 188: train loss = 0.0010, val loss = 0.1225\n",
      "Epoch 189: train loss = 0.0009, val loss = 0.1216\n",
      "Epoch 190: train loss = 0.0010, val loss = 0.1201\n",
      "Epoch 191: train loss = 0.0009, val loss = 0.1217\n",
      "Epoch 192: train loss = 0.0009, val loss = 0.1211\n",
      "Epoch 193: train loss = 0.0009, val loss = 0.1209\n",
      "Epoch 194: train loss = 0.0010, val loss = 0.1227\n",
      "Epoch 195: train loss = 0.0010, val loss = 0.1222\n",
      "Epoch 196: train loss = 0.0010, val loss = 0.1266\n",
      "Epoch 197: train loss = 0.0010, val loss = 0.1240\n",
      "Epoch 198: train loss = 0.0010, val loss = 0.1245\n",
      "Epoch 199: train loss = 0.0009, val loss = 0.1239\n",
      "Epoch 200: train loss = 0.0009, val loss = 0.1242\n",
      "Epoch 201: train loss = 0.0008, val loss = 0.1240\n",
      "Epoch 202: train loss = 0.0008, val loss = 0.1230\n",
      "Epoch 203: train loss = 0.0009, val loss = 0.1245\n",
      "Epoch 204: train loss = 0.0008, val loss = 0.1241\n",
      "Epoch 205: train loss = 0.0008, val loss = 0.1264\n",
      "Epoch 206: train loss = 0.0008, val loss = 0.1247\n",
      "Epoch 207: train loss = 0.0008, val loss = 0.1253\n",
      "Epoch 208: train loss = 0.0008, val loss = 0.1248\n",
      "Epoch 209: train loss = 0.0008, val loss = 0.1252\n",
      "Epoch 210: train loss = 0.0007, val loss = 0.1262\n",
      "Epoch 211: train loss = 0.0006, val loss = 0.1265\n",
      "Epoch 212: train loss = 0.0006, val loss = 0.1269\n",
      "Epoch 213: train loss = 0.0007, val loss = 0.1266\n",
      "Epoch 214: train loss = 0.0007, val loss = 0.1267\n",
      "Epoch 215: train loss = 0.0008, val loss = 0.1266\n",
      "Epoch 216: train loss = 0.0008, val loss = 0.1267\n",
      "Epoch 217: train loss = 0.0008, val loss = 0.1272\n",
      "Epoch 218: train loss = 0.0007, val loss = 0.1282\n",
      "Epoch 219: train loss = 0.0008, val loss = 0.1277\n",
      "Epoch 220: train loss = 0.0007, val loss = 0.1265\n",
      "Epoch 221: train loss = 0.0007, val loss = 0.1267\n",
      "Epoch 222: train loss = 0.0007, val loss = 0.1285\n",
      "Epoch 223: train loss = 0.0007, val loss = 0.1277\n",
      "Epoch 224: train loss = 0.0009, val loss = 0.1279\n",
      "Epoch 225: train loss = 0.0009, val loss = 0.1278\n",
      "Epoch 226: train loss = 0.0008, val loss = 0.1282\n",
      "Epoch 227: train loss = 0.0008, val loss = 0.1262\n",
      "Epoch 228: train loss = 0.0008, val loss = 0.1293\n",
      "Epoch 229: train loss = 0.0008, val loss = 0.1268\n",
      "Epoch 230: train loss = 0.0007, val loss = 0.1302\n",
      "Epoch 231: train loss = 0.0007, val loss = 0.1283\n",
      "Epoch 232: train loss = 0.0006, val loss = 0.1293\n",
      "Epoch 233: train loss = 0.0006, val loss = 0.1298\n",
      "Epoch 234: train loss = 0.0006, val loss = 0.1295\n",
      "Epoch 235: train loss = 0.0006, val loss = 0.1282\n",
      "Epoch 236: train loss = 0.0005, val loss = 0.1300\n",
      "Epoch 237: train loss = 0.0005, val loss = 0.1302\n",
      "Epoch 238: train loss = 0.0005, val loss = 0.1283\n",
      "Epoch 239: train loss = 0.0006, val loss = 0.1298\n",
      "Epoch 240: train loss = 0.0006, val loss = 0.1303\n",
      "Epoch 241: train loss = 0.0006, val loss = 0.1313\n",
      "Epoch 242: train loss = 0.0005, val loss = 0.1283\n",
      "Epoch 243: train loss = 0.0006, val loss = 0.1321\n",
      "Epoch 244: train loss = 0.0006, val loss = 0.1282\n",
      "Epoch 245: train loss = 0.0005, val loss = 0.1306\n",
      "Epoch 246: train loss = 0.0005, val loss = 0.1304\n",
      "Epoch 247: train loss = 0.0005, val loss = 0.1301\n",
      "Epoch 248: train loss = 0.0005, val loss = 0.1294\n",
      "Epoch 249: train loss = 0.0005, val loss = 0.1303\n",
      "Epoch 250: train loss = 0.0005, val loss = 0.1321\n",
      "Epoch 251: train loss = 0.0005, val loss = 0.1294\n",
      "Epoch 252: train loss = 0.0005, val loss = 0.1323\n",
      "Epoch 253: train loss = 0.0005, val loss = 0.1318\n",
      "Epoch 254: train loss = 0.0005, val loss = 0.1312\n",
      "Epoch 255: train loss = 0.0006, val loss = 0.1301\n",
      "Epoch 256: train loss = 0.0006, val loss = 0.1332\n",
      "Epoch 257: train loss = 0.0005, val loss = 0.1308\n",
      "Epoch 258: train loss = 0.0005, val loss = 0.1310\n",
      "Epoch 259: train loss = 0.0005, val loss = 0.1316\n",
      "Epoch 260: train loss = 0.0005, val loss = 0.1306\n",
      "Epoch 261: train loss = 0.0005, val loss = 0.1316\n",
      "Epoch 262: train loss = 0.0005, val loss = 0.1306\n",
      "Epoch 263: train loss = 0.0005, val loss = 0.1308\n",
      "Epoch 264: train loss = 0.0006, val loss = 0.1325\n",
      "Epoch 265: train loss = 0.0005, val loss = 0.1299\n",
      "Epoch 266: train loss = 0.0005, val loss = 0.1301\n",
      "Epoch 267: train loss = 0.0005, val loss = 0.1315\n",
      "Epoch 268: train loss = 0.0004, val loss = 0.1327\n",
      "Epoch 269: train loss = 0.0005, val loss = 0.1315\n",
      "Epoch 270: train loss = 0.0004, val loss = 0.1325\n",
      "Epoch 271: train loss = 0.0004, val loss = 0.1308\n",
      "Epoch 272: train loss = 0.0004, val loss = 0.1319\n",
      "Epoch 273: train loss = 0.0004, val loss = 0.1323\n",
      "Epoch 274: train loss = 0.0004, val loss = 0.1336\n",
      "Epoch 275: train loss = 0.0004, val loss = 0.1323\n",
      "Epoch 276: train loss = 0.0004, val loss = 0.1328\n",
      "Epoch 277: train loss = 0.0004, val loss = 0.1316\n",
      "Epoch 278: train loss = 0.0004, val loss = 0.1330\n",
      "Epoch 279: train loss = 0.0003, val loss = 0.1322\n",
      "Epoch 280: train loss = 0.0004, val loss = 0.1338\n",
      "Epoch 281: train loss = 0.0004, val loss = 0.1332\n",
      "Epoch 282: train loss = 0.0004, val loss = 0.1332\n",
      "Epoch 283: train loss = 0.0004, val loss = 0.1331\n",
      "Epoch 284: train loss = 0.0004, val loss = 0.1316\n",
      "Epoch 285: train loss = 0.0004, val loss = 0.1329\n",
      "Epoch 286: train loss = 0.0004, val loss = 0.1332\n",
      "Epoch 287: train loss = 0.0003, val loss = 0.1326\n",
      "Epoch 288: train loss = 0.0003, val loss = 0.1334\n",
      "Epoch 289: train loss = 0.0003, val loss = 0.1331\n",
      "Epoch 290: train loss = 0.0003, val loss = 0.1332\n",
      "Epoch 291: train loss = 0.0003, val loss = 0.1323\n",
      "Epoch 292: train loss = 0.0003, val loss = 0.1336\n",
      "Epoch 293: train loss = 0.0003, val loss = 0.1317\n",
      "Epoch 294: train loss = 0.0003, val loss = 0.1330\n",
      "Epoch 295: train loss = 0.0003, val loss = 0.1333\n",
      "Epoch 296: train loss = 0.0003, val loss = 0.1320\n",
      "Epoch 297: train loss = 0.0003, val loss = 0.1339\n",
      "Epoch 298: train loss = 0.0003, val loss = 0.1327\n",
      "Epoch 299: train loss = 0.0003, val loss = 0.1330\n",
      "Epoch 300: train loss = 0.0003, val loss = 0.1327\n",
      "Epoch 301: train loss = 0.0003, val loss = 0.1340\n",
      "Epoch 302: train loss = 0.0003, val loss = 0.1337\n",
      "Epoch 303: train loss = 0.0003, val loss = 0.1331\n",
      "Epoch 304: train loss = 0.0003, val loss = 0.1338\n",
      "Epoch 305: train loss = 0.0003, val loss = 0.1330\n",
      "Epoch 306: train loss = 0.0002, val loss = 0.1335\n",
      "Epoch 307: train loss = 0.0003, val loss = 0.1337\n",
      "Epoch 308: train loss = 0.0003, val loss = 0.1345\n",
      "Epoch 309: train loss = 0.0003, val loss = 0.1345\n",
      "Epoch 310: train loss = 0.0003, val loss = 0.1346\n",
      "Epoch 311: train loss = 0.0003, val loss = 0.1340\n",
      "Epoch 312: train loss = 0.0003, val loss = 0.1341\n",
      "Epoch 313: train loss = 0.0003, val loss = 0.1343\n",
      "Epoch 314: train loss = 0.0002, val loss = 0.1347\n",
      "Epoch 315: train loss = 0.0002, val loss = 0.1341\n",
      "Epoch 316: train loss = 0.0002, val loss = 0.1341\n",
      "Epoch 317: train loss = 0.0002, val loss = 0.1342\n",
      "Epoch 318: train loss = 0.0002, val loss = 0.1345\n",
      "Epoch 319: train loss = 0.0002, val loss = 0.1339\n",
      "Epoch 320: train loss = 0.0002, val loss = 0.1346\n",
      "Epoch 321: train loss = 0.0003, val loss = 0.1354\n",
      "Epoch 322: train loss = 0.0003, val loss = 0.1345\n",
      "Epoch 323: train loss = 0.0003, val loss = 0.1336\n",
      "Epoch 324: train loss = 0.0003, val loss = 0.1355\n",
      "Epoch 325: train loss = 0.0003, val loss = 0.1350\n",
      "Epoch 326: train loss = 0.0004, val loss = 0.1360\n",
      "Epoch 327: train loss = 0.0004, val loss = 0.1325\n",
      "Epoch 328: train loss = 0.0004, val loss = 0.1340\n",
      "Epoch 329: train loss = 0.0005, val loss = 0.1346\n",
      "Epoch 330: train loss = 0.0005, val loss = 0.1349\n",
      "Epoch 331: train loss = 0.0006, val loss = 0.1354\n",
      "Epoch 332: train loss = 0.0006, val loss = 0.1315\n",
      "Epoch 333: train loss = 0.0005, val loss = 0.1344\n",
      "Epoch 334: train loss = 0.0006, val loss = 0.1330\n",
      "Epoch 335: train loss = 0.0005, val loss = 0.1350\n",
      "Epoch 336: train loss = 0.0005, val loss = 0.1331\n",
      "Epoch 337: train loss = 0.0004, val loss = 0.1347\n",
      "Epoch 338: train loss = 0.0004, val loss = 0.1358\n",
      "Epoch 339: train loss = 0.0004, val loss = 0.1348\n",
      "Epoch 340: train loss = 0.0004, val loss = 0.1356\n",
      "Epoch 341: train loss = 0.0003, val loss = 0.1335\n",
      "Epoch 342: train loss = 0.0003, val loss = 0.1354\n",
      "Epoch 343: train loss = 0.0003, val loss = 0.1339\n",
      "Epoch 344: train loss = 0.0002, val loss = 0.1340\n",
      "Epoch 345: train loss = 0.0002, val loss = 0.1341\n",
      "Epoch 346: train loss = 0.0002, val loss = 0.1346\n",
      "Epoch 347: train loss = 0.0002, val loss = 0.1347\n",
      "Epoch 348: train loss = 0.0002, val loss = 0.1346\n",
      "Epoch 349: train loss = 0.0002, val loss = 0.1335\n",
      "Epoch 350: train loss = 0.0002, val loss = 0.1347\n",
      "Epoch 351: train loss = 0.0002, val loss = 0.1344\n",
      "Epoch 352: train loss = 0.0002, val loss = 0.1351\n",
      "Epoch 353: train loss = 0.0002, val loss = 0.1346\n",
      "Epoch 354: train loss = 0.0002, val loss = 0.1357\n",
      "Epoch 355: train loss = 0.0002, val loss = 0.1342\n",
      "Epoch 356: train loss = 0.0002, val loss = 0.1347\n",
      "Epoch 357: train loss = 0.0002, val loss = 0.1344\n",
      "Epoch 358: train loss = 0.0002, val loss = 0.1357\n",
      "Epoch 359: train loss = 0.0003, val loss = 0.1333\n",
      "Epoch 360: train loss = 0.0003, val loss = 0.1344\n",
      "Epoch 361: train loss = 0.0004, val loss = 0.1344\n",
      "Epoch 362: train loss = 0.0004, val loss = 0.1342\n",
      "Epoch 363: train loss = 0.0003, val loss = 0.1360\n",
      "Epoch 364: train loss = 0.0003, val loss = 0.1344\n",
      "Epoch 365: train loss = 0.0003, val loss = 0.1328\n",
      "Epoch 366: train loss = 0.0003, val loss = 0.1344\n",
      "Epoch 367: train loss = 0.0003, val loss = 0.1346\n",
      "Epoch 368: train loss = 0.0003, val loss = 0.1343\n",
      "Epoch 369: train loss = 0.0003, val loss = 0.1348\n",
      "Epoch 370: train loss = 0.0004, val loss = 0.1351\n",
      "Epoch 371: train loss = 0.0003, val loss = 0.1370\n",
      "Epoch 372: train loss = 0.0003, val loss = 0.1361\n",
      "Epoch 373: train loss = 0.0003, val loss = 0.1342\n",
      "Epoch 374: train loss = 0.0003, val loss = 0.1345\n",
      "Epoch 375: train loss = 0.0003, val loss = 0.1339\n",
      "Epoch 376: train loss = 0.0003, val loss = 0.1349\n",
      "Epoch 377: train loss = 0.0003, val loss = 0.1343\n",
      "Epoch 378: train loss = 0.0003, val loss = 0.1354\n",
      "Epoch 379: train loss = 0.0002, val loss = 0.1324\n",
      "Epoch 380: train loss = 0.0003, val loss = 0.1342\n",
      "Epoch 381: train loss = 0.0003, val loss = 0.1314\n",
      "Epoch 382: train loss = 0.0003, val loss = 0.1357\n",
      "Epoch 383: train loss = 0.0003, val loss = 0.1331\n",
      "Epoch 384: train loss = 0.0003, val loss = 0.1344\n",
      "Epoch 385: train loss = 0.0003, val loss = 0.1335\n",
      "Epoch 386: train loss = 0.0003, val loss = 0.1347\n",
      "Epoch 387: train loss = 0.0003, val loss = 0.1344\n",
      "Epoch 388: train loss = 0.0003, val loss = 0.1328\n",
      "Epoch 389: train loss = 0.0002, val loss = 0.1349\n",
      "Epoch 390: train loss = 0.0002, val loss = 0.1321\n",
      "Epoch 391: train loss = 0.0003, val loss = 0.1361\n",
      "Epoch 392: train loss = 0.0002, val loss = 0.1317\n",
      "Epoch 393: train loss = 0.0002, val loss = 0.1341\n",
      "Epoch 394: train loss = 0.0002, val loss = 0.1339\n",
      "Epoch 395: train loss = 0.0002, val loss = 0.1324\n",
      "Epoch 396: train loss = 0.0002, val loss = 0.1349\n",
      "Epoch 397: train loss = 0.0003, val loss = 0.1334\n",
      "Epoch 398: train loss = 0.0003, val loss = 0.1356\n",
      "Epoch 399: train loss = 0.0004, val loss = 0.1337\n",
      "Epoch 400: train loss = 0.0005, val loss = 0.1335\n",
      "Epoch 401: train loss = 0.0005, val loss = 0.1347\n",
      "Epoch 402: train loss = 0.0004, val loss = 0.1332\n",
      "Epoch 403: train loss = 0.0004, val loss = 0.1321\n",
      "Epoch 404: train loss = 0.0004, val loss = 0.1342\n",
      "Epoch 405: train loss = 0.0004, val loss = 0.1340\n",
      "Epoch 406: train loss = 0.0004, val loss = 0.1319\n",
      "Epoch 407: train loss = 0.0004, val loss = 0.1317\n",
      "Epoch 408: train loss = 0.0004, val loss = 0.1329\n",
      "Epoch 409: train loss = 0.0004, val loss = 0.1325\n",
      "Epoch 410: train loss = 0.0003, val loss = 0.1330\n",
      "Epoch 411: train loss = 0.0003, val loss = 0.1321\n",
      "Epoch 412: train loss = 0.0002, val loss = 0.1343\n",
      "Epoch 413: train loss = 0.0002, val loss = 0.1335\n",
      "Epoch 414: train loss = 0.0002, val loss = 0.1328\n",
      "Epoch 415: train loss = 0.0002, val loss = 0.1340\n",
      "Epoch 416: train loss = 0.0002, val loss = 0.1325\n",
      "Epoch 417: train loss = 0.0002, val loss = 0.1336\n",
      "Epoch 418: train loss = 0.0002, val loss = 0.1336\n",
      "Epoch 419: train loss = 0.0002, val loss = 0.1330\n",
      "Epoch 420: train loss = 0.0001, val loss = 0.1336\n",
      "Epoch 421: train loss = 0.0001, val loss = 0.1333\n",
      "Epoch 422: train loss = 0.0001, val loss = 0.1332\n",
      "Epoch 423: train loss = 0.0001, val loss = 0.1332\n",
      "Epoch 424: train loss = 0.0001, val loss = 0.1341\n",
      "Epoch 425: train loss = 0.0001, val loss = 0.1345\n",
      "Epoch 426: train loss = 0.0001, val loss = 0.1340\n",
      "Epoch 427: train loss = 0.0001, val loss = 0.1345\n",
      "Epoch 428: train loss = 0.0001, val loss = 0.1328\n",
      "Epoch 429: train loss = 0.0001, val loss = 0.1335\n",
      "Epoch 430: train loss = 0.0001, val loss = 0.1341\n",
      "Epoch 431: train loss = 0.0001, val loss = 0.1347\n",
      "Epoch 432: train loss = 0.0001, val loss = 0.1345\n",
      "Epoch 433: train loss = 0.0001, val loss = 0.1335\n",
      "Epoch 434: train loss = 0.0001, val loss = 0.1350\n",
      "Epoch 435: train loss = 0.0001, val loss = 0.1341\n",
      "Epoch 436: train loss = 0.0001, val loss = 0.1350\n",
      "Epoch 437: train loss = 0.0001, val loss = 0.1340\n",
      "Epoch 438: train loss = 0.0001, val loss = 0.1338\n",
      "Epoch 439: train loss = 0.0001, val loss = 0.1345\n",
      "Epoch 440: train loss = 0.0001, val loss = 0.1339\n",
      "Epoch 441: train loss = 0.0001, val loss = 0.1349\n",
      "Epoch 442: train loss = 0.0001, val loss = 0.1329\n",
      "Epoch 443: train loss = 0.0001, val loss = 0.1340\n",
      "Epoch 444: train loss = 0.0001, val loss = 0.1338\n",
      "Epoch 445: train loss = 0.0001, val loss = 0.1345\n",
      "Epoch 446: train loss = 0.0001, val loss = 0.1340\n",
      "Epoch 447: train loss = 0.0001, val loss = 0.1350\n",
      "Epoch 448: train loss = 0.0001, val loss = 0.1343\n",
      "Epoch 449: train loss = 0.0001, val loss = 0.1343\n",
      "Epoch 450: train loss = 0.0001, val loss = 0.1333\n",
      "Epoch 451: train loss = 0.0001, val loss = 0.1350\n",
      "Epoch 452: train loss = 0.0001, val loss = 0.1337\n",
      "Epoch 453: train loss = 0.0001, val loss = 0.1354\n",
      "Epoch 454: train loss = 0.0002, val loss = 0.1336\n",
      "Epoch 455: train loss = 0.0002, val loss = 0.1349\n",
      "Epoch 456: train loss = 0.0002, val loss = 0.1355\n",
      "Epoch 457: train loss = 0.0003, val loss = 0.1351\n",
      "Epoch 458: train loss = 0.0003, val loss = 0.1352\n",
      "Epoch 459: train loss = 0.0003, val loss = 0.1347\n",
      "Epoch 460: train loss = 0.0004, val loss = 0.1335\n",
      "Epoch 461: train loss = 0.0004, val loss = 0.1318\n",
      "Epoch 462: train loss = 0.0004, val loss = 0.1356\n",
      "Epoch 463: train loss = 0.0005, val loss = 0.1344\n",
      "Epoch 464: train loss = 0.0005, val loss = 0.1314\n",
      "Epoch 465: train loss = 0.0005, val loss = 0.1360\n",
      "Epoch 466: train loss = 0.0006, val loss = 0.1320\n",
      "Epoch 467: train loss = 0.0007, val loss = 0.1363\n",
      "Epoch 468: train loss = 0.0007, val loss = 0.1330\n",
      "Epoch 469: train loss = 0.0006, val loss = 0.1342\n",
      "Epoch 470: train loss = 0.0006, val loss = 0.1339\n",
      "Epoch 471: train loss = 0.0005, val loss = 0.1348\n",
      "Epoch 472: train loss = 0.0005, val loss = 0.1324\n",
      "Epoch 473: train loss = 0.0005, val loss = 0.1310\n",
      "Epoch 474: train loss = 0.0006, val loss = 0.1322\n",
      "Epoch 475: train loss = 0.0008, val loss = 0.1331\n",
      "Epoch 476: train loss = 0.0009, val loss = 0.1305\n",
      "Epoch 477: train loss = 0.0008, val loss = 0.1339\n",
      "Epoch 478: train loss = 0.0008, val loss = 0.1327\n",
      "Epoch 479: train loss = 0.0007, val loss = 0.1333\n",
      "Epoch 480: train loss = 0.0005, val loss = 0.1300\n",
      "Epoch 481: train loss = 0.0006, val loss = 0.1308\n",
      "Epoch 482: train loss = 0.0005, val loss = 0.1301\n",
      "Epoch 483: train loss = 0.0004, val loss = 0.1308\n",
      "Epoch 484: train loss = 0.0004, val loss = 0.1307\n",
      "Epoch 485: train loss = 0.0004, val loss = 0.1284\n",
      "Epoch 486: train loss = 0.0004, val loss = 0.1306\n",
      "Epoch 487: train loss = 0.0003, val loss = 0.1311\n",
      "Epoch 488: train loss = 0.0003, val loss = 0.1312\n",
      "Epoch 489: train loss = 0.0003, val loss = 0.1313\n",
      "Epoch 490: train loss = 0.0003, val loss = 0.1307\n",
      "Epoch 491: train loss = 0.0003, val loss = 0.1300\n",
      "Epoch 492: train loss = 0.0003, val loss = 0.1319\n",
      "Epoch 493: train loss = 0.0003, val loss = 0.1301\n",
      "Epoch 494: train loss = 0.0003, val loss = 0.1306\n",
      "Epoch 495: train loss = 0.0003, val loss = 0.1318\n",
      "Epoch 496: train loss = 0.0002, val loss = 0.1305\n",
      "Epoch 497: train loss = 0.0002, val loss = 0.1291\n",
      "Epoch 498: train loss = 0.0002, val loss = 0.1305\n",
      "Epoch 499: train loss = 0.0002, val loss = 0.1302\n",
      "Epoch 500: train loss = 0.0002, val loss = 0.1313\n",
      "Epoch 501: train loss = 0.0002, val loss = 0.1308\n",
      "Epoch 502: train loss = 0.0002, val loss = 0.1312\n",
      "Epoch 503: train loss = 0.0002, val loss = 0.1310\n",
      "Epoch 504: train loss = 0.0002, val loss = 0.1299\n",
      "Epoch 505: train loss = 0.0001, val loss = 0.1303\n",
      "Epoch 506: train loss = 0.0001, val loss = 0.1313\n",
      "Epoch 507: train loss = 0.0001, val loss = 0.1305\n",
      "Epoch 508: train loss = 0.0001, val loss = 0.1308\n",
      "Epoch 509: train loss = 0.0001, val loss = 0.1310\n",
      "Epoch 510: train loss = 0.0001, val loss = 0.1309\n",
      "Epoch 511: train loss = 0.0001, val loss = 0.1310\n",
      "Epoch 512: train loss = 0.0001, val loss = 0.1309\n",
      "Epoch 513: train loss = 0.0001, val loss = 0.1305\n",
      "Epoch 514: train loss = 0.0001, val loss = 0.1316\n",
      "Epoch 515: train loss = 0.0001, val loss = 0.1318\n",
      "Epoch 516: train loss = 0.0000, val loss = 0.1315\n",
      "Epoch 517: train loss = 0.0001, val loss = 0.1312\n",
      "Epoch 518: train loss = 0.0001, val loss = 0.1321\n",
      "Epoch 519: train loss = 0.0001, val loss = 0.1320\n",
      "Epoch 520: train loss = 0.0001, val loss = 0.1319\n",
      "Epoch 521: train loss = 0.0001, val loss = 0.1316\n",
      "Epoch 522: train loss = 0.0001, val loss = 0.1320\n",
      "Epoch 523: train loss = 0.0001, val loss = 0.1322\n",
      "Epoch 524: train loss = 0.0001, val loss = 0.1302\n",
      "Epoch 525: train loss = 0.0001, val loss = 0.1317\n",
      "Epoch 526: train loss = 0.0001, val loss = 0.1321\n",
      "Epoch 527: train loss = 0.0001, val loss = 0.1321\n",
      "Epoch 528: train loss = 0.0001, val loss = 0.1319\n",
      "Epoch 529: train loss = 0.0001, val loss = 0.1323\n",
      "Epoch 530: train loss = 0.0001, val loss = 0.1327\n",
      "Epoch 531: train loss = 0.0001, val loss = 0.1310\n",
      "Epoch 532: train loss = 0.0001, val loss = 0.1314\n",
      "Epoch 533: train loss = 0.0001, val loss = 0.1320\n",
      "Epoch 534: train loss = 0.0001, val loss = 0.1317\n",
      "Epoch 535: train loss = 0.0001, val loss = 0.1320\n",
      "Epoch 536: train loss = 0.0001, val loss = 0.1308\n",
      "Epoch 537: train loss = 0.0002, val loss = 0.1324\n",
      "Epoch 538: train loss = 0.0002, val loss = 0.1305\n",
      "Epoch 539: train loss = 0.0001, val loss = 0.1316\n",
      "Epoch 540: train loss = 0.0002, val loss = 0.1332\n",
      "Epoch 541: train loss = 0.0001, val loss = 0.1307\n",
      "Epoch 542: train loss = 0.0001, val loss = 0.1324\n",
      "Epoch 543: train loss = 0.0001, val loss = 0.1311\n",
      "Epoch 544: train loss = 0.0002, val loss = 0.1321\n",
      "Epoch 545: train loss = 0.0002, val loss = 0.1307\n",
      "Epoch 546: train loss = 0.0001, val loss = 0.1309\n",
      "Epoch 547: train loss = 0.0001, val loss = 0.1312\n",
      "Epoch 548: train loss = 0.0001, val loss = 0.1319\n",
      "Epoch 549: train loss = 0.0001, val loss = 0.1317\n",
      "Epoch 550: train loss = 0.0001, val loss = 0.1318\n",
      "Epoch 551: train loss = 0.0002, val loss = 0.1320\n",
      "Epoch 552: train loss = 0.0002, val loss = 0.1323\n",
      "Epoch 553: train loss = 0.0002, val loss = 0.1305\n",
      "Epoch 554: train loss = 0.0002, val loss = 0.1302\n",
      "Epoch 555: train loss = 0.0002, val loss = 0.1310\n",
      "Epoch 556: train loss = 0.0002, val loss = 0.1320\n",
      "Epoch 557: train loss = 0.0002, val loss = 0.1291\n",
      "Epoch 558: train loss = 0.0003, val loss = 0.1331\n",
      "Epoch 559: train loss = 0.0004, val loss = 0.1314\n",
      "Epoch 560: train loss = 0.0005, val loss = 0.1332\n",
      "Epoch 561: train loss = 0.0005, val loss = 0.1274\n",
      "Epoch 562: train loss = 0.0005, val loss = 0.1310\n",
      "Epoch 563: train loss = 0.0004, val loss = 0.1304\n",
      "Epoch 564: train loss = 0.0004, val loss = 0.1329\n",
      "Epoch 565: train loss = 0.0004, val loss = 0.1289\n",
      "Epoch 566: train loss = 0.0004, val loss = 0.1296\n",
      "Epoch 567: train loss = 0.0005, val loss = 0.1298\n",
      "Epoch 568: train loss = 0.0004, val loss = 0.1296\n",
      "Epoch 569: train loss = 0.0004, val loss = 0.1304\n",
      "Epoch 570: train loss = 0.0004, val loss = 0.1297\n",
      "Epoch 571: train loss = 0.0003, val loss = 0.1304\n",
      "Epoch 572: train loss = 0.0003, val loss = 0.1315\n",
      "Epoch 573: train loss = 0.0003, val loss = 0.1306\n",
      "Epoch 574: train loss = 0.0004, val loss = 0.1292\n",
      "Epoch 575: train loss = 0.0003, val loss = 0.1315\n",
      "Epoch 576: train loss = 0.0003, val loss = 0.1306\n",
      "Epoch 577: train loss = 0.0004, val loss = 0.1318\n",
      "Epoch 578: train loss = 0.0003, val loss = 0.1298\n",
      "Epoch 579: train loss = 0.0003, val loss = 0.1265\n",
      "Epoch 580: train loss = 0.0002, val loss = 0.1293\n",
      "Epoch 581: train loss = 0.0002, val loss = 0.1294\n",
      "Epoch 582: train loss = 0.0002, val loss = 0.1302\n",
      "Epoch 583: train loss = 0.0002, val loss = 0.1283\n",
      "Epoch 584: train loss = 0.0002, val loss = 0.1294\n",
      "Epoch 585: train loss = 0.0002, val loss = 0.1290\n",
      "Epoch 586: train loss = 0.0003, val loss = 0.1321\n",
      "Epoch 587: train loss = 0.0002, val loss = 0.1285\n",
      "Epoch 588: train loss = 0.0003, val loss = 0.1291\n",
      "Epoch 589: train loss = 0.0003, val loss = 0.1299\n",
      "Epoch 590: train loss = 0.0003, val loss = 0.1297\n",
      "Epoch 591: train loss = 0.0003, val loss = 0.1293\n",
      "Epoch 592: train loss = 0.0003, val loss = 0.1292\n",
      "Epoch 593: train loss = 0.0002, val loss = 0.1285\n",
      "Epoch 594: train loss = 0.0002, val loss = 0.1266\n",
      "Epoch 595: train loss = 0.0002, val loss = 0.1289\n",
      "Epoch 596: train loss = 0.0002, val loss = 0.1296\n",
      "Epoch 597: train loss = 0.0002, val loss = 0.1290\n",
      "Epoch 598: train loss = 0.0002, val loss = 0.1287\n",
      "Epoch 599: train loss = 0.0002, val loss = 0.1274\n",
      "Epoch 600: train loss = 0.0003, val loss = 0.1284\n",
      "Epoch 601: train loss = 0.0003, val loss = 0.1319\n",
      "Epoch 602: train loss = 0.0003, val loss = 0.1279\n",
      "Epoch 603: train loss = 0.0003, val loss = 0.1286\n",
      "Epoch 604: train loss = 0.0003, val loss = 0.1305\n",
      "Epoch 605: train loss = 0.0002, val loss = 0.1270\n",
      "Epoch 606: train loss = 0.0002, val loss = 0.1309\n",
      "Epoch 607: train loss = 0.0002, val loss = 0.1290\n",
      "Epoch 608: train loss = 0.0002, val loss = 0.1304\n",
      "Epoch 609: train loss = 0.0002, val loss = 0.1302\n",
      "Epoch 610: train loss = 0.0002, val loss = 0.1289\n",
      "Epoch 611: train loss = 0.0002, val loss = 0.1295\n",
      "Epoch 612: train loss = 0.0002, val loss = 0.1287\n",
      "Epoch 613: train loss = 0.0002, val loss = 0.1283\n",
      "Epoch 614: train loss = 0.0002, val loss = 0.1286\n",
      "Epoch 615: train loss = 0.0003, val loss = 0.1279\n",
      "Epoch 616: train loss = 0.0002, val loss = 0.1285\n",
      "Epoch 617: train loss = 0.0002, val loss = 0.1300\n",
      "Epoch 618: train loss = 0.0002, val loss = 0.1300\n",
      "Epoch 619: train loss = 0.0002, val loss = 0.1282\n",
      "Epoch 620: train loss = 0.0002, val loss = 0.1299\n",
      "Epoch 621: train loss = 0.0002, val loss = 0.1293\n",
      "Epoch 622: train loss = 0.0002, val loss = 0.1285\n",
      "Epoch 623: train loss = 0.0003, val loss = 0.1280\n",
      "Epoch 624: train loss = 0.0002, val loss = 0.1289\n",
      "Epoch 625: train loss = 0.0002, val loss = 0.1267\n",
      "Epoch 626: train loss = 0.0003, val loss = 0.1299\n",
      "Epoch 627: train loss = 0.0002, val loss = 0.1273\n",
      "Epoch 628: train loss = 0.0003, val loss = 0.1288\n",
      "Epoch 629: train loss = 0.0003, val loss = 0.1270\n",
      "Epoch 630: train loss = 0.0003, val loss = 0.1306\n",
      "Epoch 631: train loss = 0.0002, val loss = 0.1272\n",
      "Epoch 632: train loss = 0.0002, val loss = 0.1283\n",
      "Epoch 633: train loss = 0.0002, val loss = 0.1278\n",
      "Epoch 634: train loss = 0.0002, val loss = 0.1291\n",
      "Epoch 635: train loss = 0.0002, val loss = 0.1278\n",
      "Epoch 636: train loss = 0.0002, val loss = 0.1273\n",
      "Epoch 637: train loss = 0.0002, val loss = 0.1281\n",
      "Epoch 638: train loss = 0.0001, val loss = 0.1270\n",
      "Epoch 639: train loss = 0.0001, val loss = 0.1265\n",
      "Epoch 640: train loss = 0.0001, val loss = 0.1277\n",
      "Epoch 641: train loss = 0.0001, val loss = 0.1282\n",
      "Epoch 642: train loss = 0.0001, val loss = 0.1288\n",
      "Epoch 643: train loss = 0.0001, val loss = 0.1277\n",
      "Epoch 644: train loss = 0.0001, val loss = 0.1291\n",
      "Epoch 645: train loss = 0.0001, val loss = 0.1287\n",
      "Epoch 646: train loss = 0.0001, val loss = 0.1279\n",
      "Epoch 647: train loss = 0.0001, val loss = 0.1283\n",
      "Epoch 648: train loss = 0.0001, val loss = 0.1286\n",
      "Epoch 649: train loss = 0.0001, val loss = 0.1287\n",
      "Epoch 650: train loss = 0.0001, val loss = 0.1292\n",
      "Epoch 651: train loss = 0.0001, val loss = 0.1284\n",
      "Epoch 652: train loss = 0.0001, val loss = 0.1276\n",
      "Epoch 653: train loss = 0.0001, val loss = 0.1288\n",
      "Epoch 654: train loss = 0.0001, val loss = 0.1281\n",
      "Epoch 655: train loss = 0.0001, val loss = 0.1268\n",
      "Epoch 656: train loss = 0.0001, val loss = 0.1276\n",
      "Epoch 657: train loss = 0.0001, val loss = 0.1275\n",
      "Epoch 658: train loss = 0.0001, val loss = 0.1283\n",
      "Epoch 659: train loss = 0.0001, val loss = 0.1301\n",
      "Epoch 660: train loss = 0.0001, val loss = 0.1292\n",
      "Epoch 661: train loss = 0.0001, val loss = 0.1284\n",
      "Epoch 662: train loss = 0.0001, val loss = 0.1284\n",
      "Epoch 663: train loss = 0.0001, val loss = 0.1280\n",
      "Epoch 664: train loss = 0.0001, val loss = 0.1285\n",
      "Epoch 665: train loss = 0.0001, val loss = 0.1288\n",
      "Epoch 666: train loss = 0.0001, val loss = 0.1280\n",
      "Epoch 667: train loss = 0.0001, val loss = 0.1280\n",
      "Epoch 668: train loss = 0.0002, val loss = 0.1288\n",
      "Epoch 669: train loss = 0.0002, val loss = 0.1285\n",
      "Epoch 670: train loss = 0.0002, val loss = 0.1288\n",
      "Epoch 671: train loss = 0.0002, val loss = 0.1259\n",
      "Epoch 672: train loss = 0.0002, val loss = 0.1275\n",
      "Epoch 673: train loss = 0.0002, val loss = 0.1271\n",
      "Epoch 674: train loss = 0.0002, val loss = 0.1299\n",
      "Epoch 675: train loss = 0.0003, val loss = 0.1277\n",
      "Epoch 676: train loss = 0.0003, val loss = 0.1270\n",
      "Epoch 677: train loss = 0.0003, val loss = 0.1267\n",
      "Epoch 678: train loss = 0.0003, val loss = 0.1275\n",
      "Epoch 679: train loss = 0.0003, val loss = 0.1283\n",
      "Epoch 680: train loss = 0.0003, val loss = 0.1260\n",
      "Epoch 681: train loss = 0.0003, val loss = 0.1265\n",
      "Epoch 682: train loss = 0.0003, val loss = 0.1259\n",
      "Epoch 683: train loss = 0.0002, val loss = 0.1293\n",
      "Epoch 684: train loss = 0.0002, val loss = 0.1274\n",
      "Epoch 685: train loss = 0.0002, val loss = 0.1282\n",
      "Epoch 686: train loss = 0.0003, val loss = 0.1278\n",
      "Epoch 687: train loss = 0.0003, val loss = 0.1266\n",
      "Epoch 688: train loss = 0.0003, val loss = 0.1272\n",
      "Epoch 689: train loss = 0.0003, val loss = 0.1263\n",
      "Epoch 690: train loss = 0.0003, val loss = 0.1292\n",
      "Epoch 691: train loss = 0.0003, val loss = 0.1249\n",
      "Epoch 692: train loss = 0.0003, val loss = 0.1277\n",
      "Epoch 693: train loss = 0.0003, val loss = 0.1257\n",
      "Epoch 694: train loss = 0.0003, val loss = 0.1281\n",
      "Epoch 695: train loss = 0.0003, val loss = 0.1286\n",
      "Epoch 696: train loss = 0.0003, val loss = 0.1268\n",
      "Epoch 697: train loss = 0.0002, val loss = 0.1267\n",
      "Epoch 698: train loss = 0.0003, val loss = 0.1269\n",
      "Epoch 699: train loss = 0.0002, val loss = 0.1274\n",
      "Epoch 700: train loss = 0.0002, val loss = 0.1258\n",
      "Epoch 701: train loss = 0.0002, val loss = 0.1266\n",
      "Epoch 702: train loss = 0.0002, val loss = 0.1272\n",
      "Epoch 703: train loss = 0.0002, val loss = 0.1267\n",
      "Epoch 704: train loss = 0.0002, val loss = 0.1264\n",
      "Epoch 705: train loss = 0.0002, val loss = 0.1267\n",
      "Epoch 706: train loss = 0.0002, val loss = 0.1270\n",
      "Epoch 707: train loss = 0.0002, val loss = 0.1268\n",
      "Epoch 708: train loss = 0.0002, val loss = 0.1262\n",
      "Epoch 709: train loss = 0.0002, val loss = 0.1274\n",
      "Epoch 710: train loss = 0.0002, val loss = 0.1253\n",
      "Epoch 711: train loss = 0.0002, val loss = 0.1270\n",
      "Epoch 712: train loss = 0.0002, val loss = 0.1274\n",
      "Epoch 713: train loss = 0.0002, val loss = 0.1260\n",
      "Epoch 714: train loss = 0.0002, val loss = 0.1285\n",
      "Epoch 715: train loss = 0.0002, val loss = 0.1254\n",
      "Epoch 716: train loss = 0.0002, val loss = 0.1264\n",
      "Epoch 717: train loss = 0.0002, val loss = 0.1255\n",
      "Epoch 718: train loss = 0.0002, val loss = 0.1261\n",
      "Epoch 719: train loss = 0.0002, val loss = 0.1265\n",
      "Epoch 720: train loss = 0.0003, val loss = 0.1272\n",
      "Epoch 721: train loss = 0.0003, val loss = 0.1255\n",
      "Epoch 722: train loss = 0.0004, val loss = 0.1263\n",
      "Epoch 723: train loss = 0.0004, val loss = 0.1218\n",
      "Epoch 724: train loss = 0.0004, val loss = 0.1282\n",
      "Epoch 725: train loss = 0.0004, val loss = 0.1253\n",
      "Epoch 726: train loss = 0.0003, val loss = 0.1253\n",
      "Epoch 727: train loss = 0.0004, val loss = 0.1244\n",
      "Epoch 728: train loss = 0.0004, val loss = 0.1233\n",
      "Epoch 729: train loss = 0.0003, val loss = 0.1254\n",
      "Epoch 730: train loss = 0.0002, val loss = 0.1250\n",
      "Epoch 731: train loss = 0.0003, val loss = 0.1273\n",
      "Epoch 732: train loss = 0.0002, val loss = 0.1241\n",
      "Epoch 733: train loss = 0.0002, val loss = 0.1250\n",
      "Epoch 734: train loss = 0.0003, val loss = 0.1270\n",
      "Epoch 735: train loss = 0.0003, val loss = 0.1257\n",
      "Epoch 736: train loss = 0.0002, val loss = 0.1256\n",
      "Epoch 737: train loss = 0.0002, val loss = 0.1241\n",
      "Epoch 738: train loss = 0.0002, val loss = 0.1264\n",
      "Epoch 739: train loss = 0.0002, val loss = 0.1261\n",
      "Epoch 740: train loss = 0.0002, val loss = 0.1265\n",
      "Epoch 741: train loss = 0.0002, val loss = 0.1265\n",
      "Epoch 742: train loss = 0.0002, val loss = 0.1250\n",
      "Epoch 743: train loss = 0.0003, val loss = 0.1248\n",
      "Epoch 744: train loss = 0.0002, val loss = 0.1255\n",
      "Epoch 745: train loss = 0.0002, val loss = 0.1239\n",
      "Epoch 746: train loss = 0.0002, val loss = 0.1248\n",
      "Epoch 747: train loss = 0.0002, val loss = 0.1231\n",
      "Epoch 748: train loss = 0.0002, val loss = 0.1253\n",
      "Epoch 749: train loss = 0.0002, val loss = 0.1242\n",
      "Epoch 750: train loss = 0.0002, val loss = 0.1242\n",
      "Epoch 751: train loss = 0.0002, val loss = 0.1246\n",
      "Epoch 752: train loss = 0.0001, val loss = 0.1247\n",
      "Epoch 753: train loss = 0.0001, val loss = 0.1248\n",
      "Epoch 754: train loss = 0.0001, val loss = 0.1253\n",
      "Epoch 755: train loss = 0.0001, val loss = 0.1251\n",
      "Epoch 756: train loss = 0.0001, val loss = 0.1250\n",
      "Epoch 757: train loss = 0.0001, val loss = 0.1238\n",
      "Epoch 758: train loss = 0.0001, val loss = 0.1251\n",
      "Epoch 759: train loss = 0.0001, val loss = 0.1239\n",
      "Epoch 760: train loss = 0.0001, val loss = 0.1254\n",
      "Epoch 761: train loss = 0.0001, val loss = 0.1238\n",
      "Epoch 762: train loss = 0.0001, val loss = 0.1248\n",
      "Epoch 763: train loss = 0.0001, val loss = 0.1245\n",
      "Epoch 764: train loss = 0.0001, val loss = 0.1258\n",
      "Epoch 765: train loss = 0.0001, val loss = 0.1250\n",
      "Epoch 766: train loss = 0.0001, val loss = 0.1244\n",
      "Epoch 767: train loss = 0.0001, val loss = 0.1249\n",
      "Epoch 768: train loss = 0.0001, val loss = 0.1246\n",
      "Epoch 769: train loss = 0.0001, val loss = 0.1259\n",
      "Epoch 770: train loss = 0.0001, val loss = 0.1240\n",
      "Epoch 771: train loss = 0.0001, val loss = 0.1238\n",
      "Epoch 772: train loss = 0.0001, val loss = 0.1250\n",
      "Epoch 773: train loss = 0.0001, val loss = 0.1246\n",
      "Epoch 774: train loss = 0.0001, val loss = 0.1248\n",
      "Epoch 775: train loss = 0.0001, val loss = 0.1243\n",
      "Epoch 776: train loss = 0.0001, val loss = 0.1245\n",
      "Epoch 777: train loss = 0.0001, val loss = 0.1239\n",
      "Epoch 778: train loss = 0.0001, val loss = 0.1247\n",
      "Epoch 779: train loss = 0.0001, val loss = 0.1249\n",
      "Epoch 780: train loss = 0.0001, val loss = 0.1240\n",
      "Epoch 781: train loss = 0.0001, val loss = 0.1242\n",
      "Epoch 782: train loss = 0.0001, val loss = 0.1244\n",
      "Epoch 783: train loss = 0.0002, val loss = 0.1233\n",
      "Epoch 784: train loss = 0.0002, val loss = 0.1245\n",
      "Epoch 785: train loss = 0.0001, val loss = 0.1256\n",
      "Epoch 786: train loss = 0.0001, val loss = 0.1236\n",
      "Epoch 787: train loss = 0.0001, val loss = 0.1250\n",
      "Epoch 788: train loss = 0.0001, val loss = 0.1236\n",
      "Epoch 789: train loss = 0.0001, val loss = 0.1250\n",
      "Epoch 790: train loss = 0.0001, val loss = 0.1238\n",
      "Epoch 791: train loss = 0.0001, val loss = 0.1243\n",
      "Epoch 792: train loss = 0.0001, val loss = 0.1253\n",
      "Epoch 793: train loss = 0.0001, val loss = 0.1246\n",
      "Epoch 794: train loss = 0.0001, val loss = 0.1250\n",
      "Epoch 795: train loss = 0.0001, val loss = 0.1243\n",
      "Epoch 796: train loss = 0.0002, val loss = 0.1244\n",
      "Epoch 797: train loss = 0.0002, val loss = 0.1239\n",
      "Epoch 798: train loss = 0.0002, val loss = 0.1256\n",
      "Epoch 799: train loss = 0.0002, val loss = 0.1245\n",
      "Epoch 800: train loss = 0.0002, val loss = 0.1247\n",
      "Epoch 801: train loss = 0.0002, val loss = 0.1234\n",
      "Epoch 802: train loss = 0.0003, val loss = 0.1218\n",
      "Epoch 803: train loss = 0.0003, val loss = 0.1240\n",
      "Epoch 804: train loss = 0.0003, val loss = 0.1245\n",
      "Epoch 805: train loss = 0.0003, val loss = 0.1217\n",
      "Epoch 806: train loss = 0.0003, val loss = 0.1248\n",
      "Epoch 807: train loss = 0.0003, val loss = 0.1252\n",
      "Epoch 808: train loss = 0.0003, val loss = 0.1217\n",
      "Epoch 809: train loss = 0.0004, val loss = 0.1239\n",
      "Epoch 810: train loss = 0.0004, val loss = 0.1247\n",
      "Epoch 811: train loss = 0.0004, val loss = 0.1226\n",
      "Epoch 812: train loss = 0.0004, val loss = 0.1227\n",
      "Epoch 813: train loss = 0.0003, val loss = 0.1237\n",
      "Epoch 814: train loss = 0.0003, val loss = 0.1246\n",
      "Epoch 815: train loss = 0.0004, val loss = 0.1249\n",
      "Epoch 816: train loss = 0.0004, val loss = 0.1258\n",
      "Epoch 817: train loss = 0.0005, val loss = 0.1241\n",
      "Epoch 818: train loss = 0.0005, val loss = 0.1218\n",
      "Epoch 819: train loss = 0.0006, val loss = 0.1225\n",
      "Epoch 820: train loss = 0.0006, val loss = 0.1213\n",
      "Epoch 821: train loss = 0.0006, val loss = 0.1269\n",
      "Epoch 822: train loss = 0.0006, val loss = 0.1211\n",
      "Epoch 823: train loss = 0.0006, val loss = 0.1235\n",
      "Epoch 824: train loss = 0.0005, val loss = 0.1196\n",
      "Epoch 825: train loss = 0.0006, val loss = 0.1221\n",
      "Epoch 826: train loss = 0.0006, val loss = 0.1229\n",
      "Epoch 827: train loss = 0.0005, val loss = 0.1231\n",
      "Epoch 828: train loss = 0.0004, val loss = 0.1205\n",
      "Epoch 829: train loss = 0.0004, val loss = 0.1227\n",
      "Epoch 830: train loss = 0.0003, val loss = 0.1234\n",
      "Epoch 831: train loss = 0.0003, val loss = 0.1216\n",
      "Epoch 832: train loss = 0.0003, val loss = 0.1202\n",
      "Epoch 833: train loss = 0.0003, val loss = 0.1235\n",
      "Epoch 834: train loss = 0.0003, val loss = 0.1227\n",
      "Epoch 835: train loss = 0.0003, val loss = 0.1214\n",
      "Epoch 836: train loss = 0.0002, val loss = 0.1217\n",
      "Epoch 837: train loss = 0.0002, val loss = 0.1225\n",
      "Epoch 838: train loss = 0.0001, val loss = 0.1226\n",
      "Epoch 839: train loss = 0.0001, val loss = 0.1207\n",
      "Epoch 840: train loss = 0.0001, val loss = 0.1226\n",
      "Epoch 841: train loss = 0.0001, val loss = 0.1214\n",
      "Epoch 842: train loss = 0.0001, val loss = 0.1213\n",
      "Epoch 843: train loss = 0.0002, val loss = 0.1226\n",
      "Epoch 844: train loss = 0.0001, val loss = 0.1219\n",
      "Epoch 845: train loss = 0.0001, val loss = 0.1224\n",
      "Epoch 846: train loss = 0.0001, val loss = 0.1209\n",
      "Epoch 847: train loss = 0.0001, val loss = 0.1228\n",
      "Epoch 848: train loss = 0.0001, val loss = 0.1209\n",
      "Epoch 849: train loss = 0.0001, val loss = 0.1218\n",
      "Epoch 850: train loss = 0.0001, val loss = 0.1228\n",
      "Epoch 851: train loss = 0.0001, val loss = 0.1219\n",
      "Epoch 852: train loss = 0.0001, val loss = 0.1227\n",
      "Epoch 853: train loss = 0.0001, val loss = 0.1212\n",
      "Epoch 854: train loss = 0.0001, val loss = 0.1219\n",
      "Epoch 855: train loss = 0.0001, val loss = 0.1218\n",
      "Epoch 856: train loss = 0.0000, val loss = 0.1215\n",
      "Epoch 857: train loss = 0.0000, val loss = 0.1222\n",
      "Epoch 858: train loss = 0.0000, val loss = 0.1211\n",
      "Epoch 859: train loss = 0.0000, val loss = 0.1221\n",
      "Epoch 860: train loss = 0.0000, val loss = 0.1215\n",
      "Epoch 861: train loss = 0.0000, val loss = 0.1219\n",
      "Epoch 862: train loss = 0.0000, val loss = 0.1223\n",
      "Epoch 863: train loss = 0.0000, val loss = 0.1219\n",
      "Epoch 864: train loss = 0.0000, val loss = 0.1222\n",
      "Epoch 865: train loss = 0.0000, val loss = 0.1218\n",
      "Epoch 866: train loss = 0.0000, val loss = 0.1218\n",
      "Epoch 867: train loss = 0.0000, val loss = 0.1225\n",
      "Epoch 868: train loss = 0.0000, val loss = 0.1225\n",
      "Epoch 869: train loss = 0.0000, val loss = 0.1217\n",
      "Epoch 870: train loss = 0.0000, val loss = 0.1224\n",
      "Epoch 871: train loss = 0.0000, val loss = 0.1220\n",
      "Epoch 872: train loss = 0.0000, val loss = 0.1227\n",
      "Epoch 873: train loss = 0.0000, val loss = 0.1214\n",
      "Epoch 874: train loss = 0.0001, val loss = 0.1222\n",
      "Epoch 875: train loss = 0.0000, val loss = 0.1215\n",
      "Epoch 876: train loss = 0.0000, val loss = 0.1223\n",
      "Epoch 877: train loss = 0.0000, val loss = 0.1222\n",
      "Epoch 878: train loss = 0.0000, val loss = 0.1217\n",
      "Epoch 879: train loss = 0.0000, val loss = 0.1214\n",
      "Epoch 880: train loss = 0.0000, val loss = 0.1218\n",
      "Epoch 881: train loss = 0.0000, val loss = 0.1223\n",
      "Epoch 882: train loss = 0.0001, val loss = 0.1218\n",
      "Epoch 883: train loss = 0.0000, val loss = 0.1223\n",
      "Epoch 884: train loss = 0.0001, val loss = 0.1214\n",
      "Epoch 885: train loss = 0.0001, val loss = 0.1218\n",
      "Epoch 886: train loss = 0.0001, val loss = 0.1223\n",
      "Epoch 887: train loss = 0.0000, val loss = 0.1228\n",
      "Epoch 888: train loss = 0.0000, val loss = 0.1219\n",
      "Epoch 889: train loss = 0.0000, val loss = 0.1216\n",
      "Epoch 890: train loss = 0.0000, val loss = 0.1210\n",
      "Epoch 891: train loss = 0.0001, val loss = 0.1219\n",
      "Epoch 892: train loss = 0.0001, val loss = 0.1222\n",
      "Epoch 893: train loss = 0.0001, val loss = 0.1222\n",
      "Epoch 894: train loss = 0.0001, val loss = 0.1228\n",
      "Epoch 895: train loss = 0.0001, val loss = 0.1218\n",
      "Epoch 896: train loss = 0.0001, val loss = 0.1220\n",
      "Epoch 897: train loss = 0.0001, val loss = 0.1223\n",
      "Epoch 898: train loss = 0.0001, val loss = 0.1227\n",
      "Epoch 899: train loss = 0.0001, val loss = 0.1225\n",
      "Epoch 900: train loss = 0.0001, val loss = 0.1222\n",
      "Epoch 901: train loss = 0.0001, val loss = 0.1216\n",
      "Epoch 902: train loss = 0.0001, val loss = 0.1236\n",
      "Epoch 903: train loss = 0.0001, val loss = 0.1217\n",
      "Epoch 904: train loss = 0.0001, val loss = 0.1231\n",
      "Epoch 905: train loss = 0.0001, val loss = 0.1206\n",
      "Epoch 906: train loss = 0.0001, val loss = 0.1229\n",
      "Epoch 907: train loss = 0.0001, val loss = 0.1217\n",
      "Epoch 908: train loss = 0.0001, val loss = 0.1220\n",
      "Epoch 909: train loss = 0.0001, val loss = 0.1217\n",
      "Epoch 910: train loss = 0.0001, val loss = 0.1223\n",
      "Epoch 911: train loss = 0.0001, val loss = 0.1206\n",
      "Epoch 912: train loss = 0.0002, val loss = 0.1214\n",
      "Epoch 913: train loss = 0.0002, val loss = 0.1222\n",
      "Epoch 914: train loss = 0.0002, val loss = 0.1233\n",
      "Epoch 915: train loss = 0.0002, val loss = 0.1214\n",
      "Epoch 916: train loss = 0.0002, val loss = 0.1207\n",
      "Epoch 917: train loss = 0.0002, val loss = 0.1198\n",
      "Epoch 918: train loss = 0.0002, val loss = 0.1229\n",
      "Epoch 919: train loss = 0.0002, val loss = 0.1190\n",
      "Epoch 920: train loss = 0.0002, val loss = 0.1222\n",
      "Epoch 921: train loss = 0.0003, val loss = 0.1217\n",
      "Epoch 922: train loss = 0.0003, val loss = 0.1228\n",
      "Epoch 923: train loss = 0.0003, val loss = 0.1215\n",
      "Epoch 924: train loss = 0.0003, val loss = 0.1239\n",
      "Epoch 925: train loss = 0.0004, val loss = 0.1202\n",
      "Epoch 926: train loss = 0.0003, val loss = 0.1203\n",
      "Epoch 927: train loss = 0.0003, val loss = 0.1185\n",
      "Epoch 928: train loss = 0.0003, val loss = 0.1201\n",
      "Epoch 929: train loss = 0.0003, val loss = 0.1197\n",
      "Epoch 930: train loss = 0.0003, val loss = 0.1196\n",
      "Epoch 931: train loss = 0.0003, val loss = 0.1200\n",
      "Epoch 932: train loss = 0.0003, val loss = 0.1179\n",
      "Epoch 933: train loss = 0.0003, val loss = 0.1233\n",
      "Epoch 934: train loss = 0.0004, val loss = 0.1199\n",
      "Epoch 935: train loss = 0.0004, val loss = 0.1219\n",
      "Epoch 936: train loss = 0.0004, val loss = 0.1207\n",
      "Epoch 937: train loss = 0.0004, val loss = 0.1207\n",
      "Epoch 938: train loss = 0.0004, val loss = 0.1200\n",
      "Epoch 939: train loss = 0.0004, val loss = 0.1188\n",
      "Epoch 940: train loss = 0.0004, val loss = 0.1199\n",
      "Epoch 941: train loss = 0.0004, val loss = 0.1184\n",
      "Epoch 942: train loss = 0.0004, val loss = 0.1208\n",
      "Epoch 943: train loss = 0.0004, val loss = 0.1173\n",
      "Epoch 944: train loss = 0.0004, val loss = 0.1190\n",
      "Epoch 945: train loss = 0.0004, val loss = 0.1186\n",
      "Epoch 946: train loss = 0.0004, val loss = 0.1178\n",
      "Epoch 947: train loss = 0.0004, val loss = 0.1186\n",
      "Epoch 948: train loss = 0.0003, val loss = 0.1188\n",
      "Epoch 949: train loss = 0.0003, val loss = 0.1208\n",
      "Epoch 950: train loss = 0.0003, val loss = 0.1178\n",
      "Epoch 951: train loss = 0.0003, val loss = 0.1192\n",
      "Epoch 952: train loss = 0.0002, val loss = 0.1184\n",
      "Epoch 953: train loss = 0.0002, val loss = 0.1193\n",
      "Epoch 954: train loss = 0.0003, val loss = 0.1197\n",
      "Epoch 955: train loss = 0.0003, val loss = 0.1190\n",
      "Epoch 956: train loss = 0.0003, val loss = 0.1172\n",
      "Epoch 957: train loss = 0.0003, val loss = 0.1191\n",
      "Epoch 958: train loss = 0.0002, val loss = 0.1188\n",
      "Epoch 959: train loss = 0.0002, val loss = 0.1174\n",
      "Epoch 960: train loss = 0.0002, val loss = 0.1186\n",
      "Epoch 961: train loss = 0.0002, val loss = 0.1194\n",
      "Epoch 962: train loss = 0.0002, val loss = 0.1179\n",
      "Epoch 963: train loss = 0.0001, val loss = 0.1187\n",
      "Epoch 964: train loss = 0.0001, val loss = 0.1185\n",
      "Epoch 965: train loss = 0.0001, val loss = 0.1178\n",
      "Epoch 966: train loss = 0.0001, val loss = 0.1190\n",
      "Epoch 967: train loss = 0.0001, val loss = 0.1180\n",
      "Epoch 968: train loss = 0.0001, val loss = 0.1193\n",
      "Epoch 969: train loss = 0.0001, val loss = 0.1181\n",
      "Epoch 970: train loss = 0.0001, val loss = 0.1187\n",
      "Epoch 971: train loss = 0.0001, val loss = 0.1176\n",
      "Epoch 972: train loss = 0.0001, val loss = 0.1191\n",
      "Epoch 973: train loss = 0.0001, val loss = 0.1173\n",
      "Epoch 974: train loss = 0.0001, val loss = 0.1178\n",
      "Epoch 975: train loss = 0.0001, val loss = 0.1185\n",
      "Epoch 976: train loss = 0.0001, val loss = 0.1178\n",
      "Epoch 977: train loss = 0.0001, val loss = 0.1182\n",
      "Epoch 978: train loss = 0.0001, val loss = 0.1177\n",
      "Epoch 979: train loss = 0.0001, val loss = 0.1187\n",
      "Epoch 980: train loss = 0.0002, val loss = 0.1178\n",
      "Epoch 981: train loss = 0.0001, val loss = 0.1170\n",
      "Epoch 982: train loss = 0.0001, val loss = 0.1181\n",
      "Epoch 983: train loss = 0.0001, val loss = 0.1176\n",
      "Epoch 984: train loss = 0.0001, val loss = 0.1179\n",
      "Epoch 985: train loss = 0.0001, val loss = 0.1185\n",
      "Epoch 986: train loss = 0.0001, val loss = 0.1187\n",
      "Epoch 987: train loss = 0.0001, val loss = 0.1183\n",
      "Epoch 988: train loss = 0.0001, val loss = 0.1178\n",
      "Epoch 989: train loss = 0.0001, val loss = 0.1179\n",
      "Epoch 990: train loss = 0.0001, val loss = 0.1182\n",
      "Epoch 991: train loss = 0.0001, val loss = 0.1180\n",
      "Epoch 992: train loss = 0.0001, val loss = 0.1165\n",
      "Epoch 993: train loss = 0.0001, val loss = 0.1177\n",
      "Epoch 994: train loss = 0.0001, val loss = 0.1171\n",
      "Epoch 995: train loss = 0.0001, val loss = 0.1176\n",
      "Epoch 996: train loss = 0.0001, val loss = 0.1196\n",
      "Epoch 997: train loss = 0.0001, val loss = 0.1174\n",
      "Epoch 998: train loss = 0.0001, val loss = 0.1182\n",
      "Epoch 999: train loss = 0.0001, val loss = 0.1173\n",
      "Epoch 1000: train loss = 0.0001, val loss = 0.1186\n",
      "Epoch 1001: train loss = 0.0001, val loss = 0.1184\n",
      "Epoch 1002: train loss = 0.0001, val loss = 0.1174\n",
      "Epoch 1003: train loss = 0.0001, val loss = 0.1173\n",
      "Epoch 1004: train loss = 0.0002, val loss = 0.1190\n",
      "Epoch 1005: train loss = 0.0002, val loss = 0.1164\n",
      "Epoch 1006: train loss = 0.0001, val loss = 0.1177\n",
      "Epoch 1007: train loss = 0.0002, val loss = 0.1178\n",
      "Epoch 1008: train loss = 0.0002, val loss = 0.1182\n",
      "Epoch 1009: train loss = 0.0001, val loss = 0.1187\n",
      "Epoch 1010: train loss = 0.0001, val loss = 0.1170\n",
      "Epoch 1011: train loss = 0.0002, val loss = 0.1177\n",
      "Epoch 1012: train loss = 0.0002, val loss = 0.1176\n",
      "Epoch 1013: train loss = 0.0002, val loss = 0.1169\n",
      "Epoch 1014: train loss = 0.0002, val loss = 0.1178\n",
      "Epoch 1015: train loss = 0.0002, val loss = 0.1164\n",
      "Epoch 1016: train loss = 0.0002, val loss = 0.1179\n",
      "Epoch 1017: train loss = 0.0001, val loss = 0.1159\n",
      "Epoch 1018: train loss = 0.0001, val loss = 0.1185\n",
      "Epoch 1019: train loss = 0.0001, val loss = 0.1165\n",
      "Epoch 1020: train loss = 0.0001, val loss = 0.1178\n",
      "Epoch 1021: train loss = 0.0001, val loss = 0.1168\n",
      "Epoch 1022: train loss = 0.0001, val loss = 0.1178\n",
      "Epoch 1023: train loss = 0.0001, val loss = 0.1176\n",
      "Epoch 1024: train loss = 0.0001, val loss = 0.1189\n",
      "Epoch 1025: train loss = 0.0001, val loss = 0.1179\n",
      "Epoch 1026: train loss = 0.0002, val loss = 0.1181\n",
      "Epoch 1027: train loss = 0.0001, val loss = 0.1184\n",
      "Epoch 1028: train loss = 0.0001, val loss = 0.1184\n",
      "Epoch 1029: train loss = 0.0002, val loss = 0.1168\n",
      "Epoch 1030: train loss = 0.0002, val loss = 0.1180\n",
      "Epoch 1031: train loss = 0.0001, val loss = 0.1168\n",
      "Epoch 1032: train loss = 0.0001, val loss = 0.1170\n",
      "Epoch 1033: train loss = 0.0001, val loss = 0.1166\n",
      "Epoch 1034: train loss = 0.0001, val loss = 0.1179\n",
      "Epoch 1035: train loss = 0.0002, val loss = 0.1160\n",
      "Epoch 1036: train loss = 0.0002, val loss = 0.1172\n",
      "Epoch 1037: train loss = 0.0002, val loss = 0.1157\n",
      "Epoch 1038: train loss = 0.0002, val loss = 0.1177\n",
      "Epoch 1039: train loss = 0.0002, val loss = 0.1162\n",
      "Epoch 1040: train loss = 0.0002, val loss = 0.1176\n",
      "Epoch 1041: train loss = 0.0002, val loss = 0.1165\n",
      "Epoch 1042: train loss = 0.0003, val loss = 0.1167\n",
      "Epoch 1043: train loss = 0.0004, val loss = 0.1154\n",
      "Epoch 1044: train loss = 0.0004, val loss = 0.1164\n",
      "Epoch 1045: train loss = 0.0004, val loss = 0.1173\n",
      "Epoch 1046: train loss = 0.0003, val loss = 0.1163\n",
      "Epoch 1047: train loss = 0.0003, val loss = 0.1179\n",
      "Epoch 1048: train loss = 0.0003, val loss = 0.1137\n",
      "Epoch 1049: train loss = 0.0002, val loss = 0.1167\n",
      "Epoch 1050: train loss = 0.0002, val loss = 0.1138\n",
      "Epoch 1051: train loss = 0.0001, val loss = 0.1171\n",
      "Epoch 1052: train loss = 0.0001, val loss = 0.1152\n",
      "Epoch 1053: train loss = 0.0001, val loss = 0.1162\n",
      "Epoch 1054: train loss = 0.0001, val loss = 0.1155\n",
      "Epoch 1055: train loss = 0.0001, val loss = 0.1157\n",
      "Epoch 1056: train loss = 0.0001, val loss = 0.1162\n",
      "Epoch 1057: train loss = 0.0001, val loss = 0.1156\n",
      "Epoch 1058: train loss = 0.0001, val loss = 0.1165\n",
      "Epoch 1059: train loss = 0.0001, val loss = 0.1165\n",
      "Epoch 1060: train loss = 0.0001, val loss = 0.1161\n",
      "Epoch 1061: train loss = 0.0001, val loss = 0.1164\n",
      "Epoch 1062: train loss = 0.0001, val loss = 0.1150\n",
      "Epoch 1063: train loss = 0.0001, val loss = 0.1163\n",
      "Epoch 1064: train loss = 0.0001, val loss = 0.1151\n",
      "Epoch 1065: train loss = 0.0001, val loss = 0.1161\n",
      "Epoch 1066: train loss = 0.0001, val loss = 0.1151\n",
      "Epoch 1067: train loss = 0.0001, val loss = 0.1160\n",
      "Epoch 1068: train loss = 0.0001, val loss = 0.1163\n",
      "Epoch 1069: train loss = 0.0001, val loss = 0.1158\n",
      "Epoch 1070: train loss = 0.0001, val loss = 0.1156\n",
      "Epoch 1071: train loss = 0.0001, val loss = 0.1175\n",
      "Epoch 1072: train loss = 0.0001, val loss = 0.1154\n",
      "Epoch 1073: train loss = 0.0002, val loss = 0.1174\n",
      "Epoch 1074: train loss = 0.0002, val loss = 0.1156\n",
      "Epoch 1075: train loss = 0.0002, val loss = 0.1155\n",
      "Epoch 1076: train loss = 0.0002, val loss = 0.1153\n",
      "Epoch 1077: train loss = 0.0002, val loss = 0.1143\n",
      "Epoch 1078: train loss = 0.0002, val loss = 0.1164\n",
      "Epoch 1079: train loss = 0.0002, val loss = 0.1155\n",
      "Epoch 1080: train loss = 0.0001, val loss = 0.1152\n",
      "Epoch 1081: train loss = 0.0001, val loss = 0.1172\n",
      "Epoch 1082: train loss = 0.0002, val loss = 0.1140\n",
      "Epoch 1083: train loss = 0.0002, val loss = 0.1165\n",
      "Epoch 1084: train loss = 0.0001, val loss = 0.1152\n",
      "Epoch 1085: train loss = 0.0002, val loss = 0.1165\n",
      "Epoch 1086: train loss = 0.0002, val loss = 0.1156\n",
      "Epoch 1087: train loss = 0.0002, val loss = 0.1161\n",
      "Epoch 1088: train loss = 0.0002, val loss = 0.1166\n",
      "Epoch 1089: train loss = 0.0002, val loss = 0.1148\n",
      "Epoch 1090: train loss = 0.0001, val loss = 0.1152\n",
      "Epoch 1091: train loss = 0.0001, val loss = 0.1158\n",
      "Epoch 1092: train loss = 0.0001, val loss = 0.1168\n",
      "Epoch 1093: train loss = 0.0001, val loss = 0.1153\n",
      "Epoch 1094: train loss = 0.0002, val loss = 0.1160\n",
      "Epoch 1095: train loss = 0.0001, val loss = 0.1140\n",
      "Epoch 1096: train loss = 0.0001, val loss = 0.1146\n",
      "Epoch 1097: train loss = 0.0001, val loss = 0.1150\n",
      "Epoch 1098: train loss = 0.0001, val loss = 0.1156\n",
      "Epoch 1099: train loss = 0.0001, val loss = 0.1149\n",
      "Epoch 1100: train loss = 0.0001, val loss = 0.1149\n",
      "Epoch 1101: train loss = 0.0001, val loss = 0.1160\n",
      "Epoch 1102: train loss = 0.0001, val loss = 0.1149\n",
      "Epoch 1103: train loss = 0.0001, val loss = 0.1153\n",
      "Epoch 1104: train loss = 0.0001, val loss = 0.1148\n",
      "Epoch 1105: train loss = 0.0001, val loss = 0.1164\n",
      "Epoch 1106: train loss = 0.0001, val loss = 0.1152\n",
      "Epoch 1107: train loss = 0.0001, val loss = 0.1151\n",
      "Epoch 1108: train loss = 0.0001, val loss = 0.1154\n",
      "Epoch 1109: train loss = 0.0001, val loss = 0.1137\n",
      "Epoch 1110: train loss = 0.0001, val loss = 0.1159\n",
      "Epoch 1111: train loss = 0.0001, val loss = 0.1150\n",
      "Epoch 1112: train loss = 0.0001, val loss = 0.1150\n",
      "Epoch 1113: train loss = 0.0001, val loss = 0.1153\n",
      "Epoch 1114: train loss = 0.0001, val loss = 0.1157\n",
      "Epoch 1115: train loss = 0.0001, val loss = 0.1151\n",
      "Epoch 1116: train loss = 0.0001, val loss = 0.1138\n",
      "Epoch 1117: train loss = 0.0001, val loss = 0.1155\n",
      "Epoch 1118: train loss = 0.0001, val loss = 0.1138\n",
      "Epoch 1119: train loss = 0.0001, val loss = 0.1160\n",
      "Epoch 1120: train loss = 0.0001, val loss = 0.1141\n",
      "Epoch 1121: train loss = 0.0001, val loss = 0.1165\n",
      "Epoch 1122: train loss = 0.0001, val loss = 0.1144\n",
      "Epoch 1123: train loss = 0.0001, val loss = 0.1150\n",
      "Epoch 1124: train loss = 0.0002, val loss = 0.1146\n",
      "Epoch 1125: train loss = 0.0002, val loss = 0.1153\n",
      "Epoch 1126: train loss = 0.0002, val loss = 0.1134\n",
      "Epoch 1127: train loss = 0.0001, val loss = 0.1143\n",
      "Epoch 1128: train loss = 0.0001, val loss = 0.1134\n",
      "Epoch 1129: train loss = 0.0001, val loss = 0.1146\n",
      "Epoch 1130: train loss = 0.0001, val loss = 0.1138\n",
      "Epoch 1131: train loss = 0.0001, val loss = 0.1147\n",
      "Epoch 1132: train loss = 0.0001, val loss = 0.1132\n",
      "Epoch 1133: train loss = 0.0002, val loss = 0.1146\n",
      "Epoch 1134: train loss = 0.0002, val loss = 0.1132\n",
      "Epoch 1135: train loss = 0.0001, val loss = 0.1148\n",
      "Epoch 1136: train loss = 0.0001, val loss = 0.1136\n",
      "Epoch 1137: train loss = 0.0002, val loss = 0.1156\n",
      "Epoch 1138: train loss = 0.0002, val loss = 0.1146\n",
      "Epoch 1139: train loss = 0.0002, val loss = 0.1143\n",
      "Epoch 1140: train loss = 0.0002, val loss = 0.1139\n",
      "Epoch 1141: train loss = 0.0002, val loss = 0.1141\n",
      "Epoch 1142: train loss = 0.0002, val loss = 0.1152\n",
      "Epoch 1143: train loss = 0.0002, val loss = 0.1147\n",
      "Epoch 1144: train loss = 0.0002, val loss = 0.1136\n",
      "Epoch 1145: train loss = 0.0002, val loss = 0.1151\n",
      "Epoch 1146: train loss = 0.0002, val loss = 0.1157\n",
      "Epoch 1147: train loss = 0.0003, val loss = 0.1148\n",
      "Epoch 1148: train loss = 0.0003, val loss = 0.1144\n",
      "Epoch 1149: train loss = 0.0003, val loss = 0.1139\n",
      "Epoch 1150: train loss = 0.0003, val loss = 0.1148\n",
      "Epoch 1151: train loss = 0.0003, val loss = 0.1145\n",
      "Epoch 1152: train loss = 0.0003, val loss = 0.1150\n",
      "Epoch 1153: train loss = 0.0002, val loss = 0.1141\n",
      "Epoch 1154: train loss = 0.0002, val loss = 0.1133\n",
      "Epoch 1155: train loss = 0.0002, val loss = 0.1145\n",
      "Epoch 1156: train loss = 0.0003, val loss = 0.1141\n",
      "Epoch 1157: train loss = 0.0003, val loss = 0.1131\n",
      "Epoch 1158: train loss = 0.0003, val loss = 0.1123\n",
      "Epoch 1159: train loss = 0.0002, val loss = 0.1133\n",
      "Epoch 1160: train loss = 0.0002, val loss = 0.1137\n",
      "Epoch 1161: train loss = 0.0002, val loss = 0.1122\n",
      "Epoch 1162: train loss = 0.0002, val loss = 0.1135\n",
      "Epoch 1163: train loss = 0.0002, val loss = 0.1125\n",
      "Epoch 1164: train loss = 0.0002, val loss = 0.1133\n",
      "Epoch 1165: train loss = 0.0002, val loss = 0.1133\n",
      "Epoch 1166: train loss = 0.0001, val loss = 0.1134\n",
      "Epoch 1167: train loss = 0.0002, val loss = 0.1126\n",
      "Epoch 1168: train loss = 0.0002, val loss = 0.1139\n",
      "Epoch 1169: train loss = 0.0002, val loss = 0.1119\n",
      "Epoch 1170: train loss = 0.0002, val loss = 0.1138\n",
      "Epoch 1171: train loss = 0.0002, val loss = 0.1137\n",
      "Epoch 1172: train loss = 0.0002, val loss = 0.1131\n",
      "Epoch 1173: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1174: train loss = 0.0001, val loss = 0.1130\n",
      "Epoch 1175: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1176: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1177: train loss = 0.0001, val loss = 0.1125\n",
      "Epoch 1178: train loss = 0.0001, val loss = 0.1128\n",
      "Epoch 1179: train loss = 0.0001, val loss = 0.1128\n",
      "Epoch 1180: train loss = 0.0002, val loss = 0.1132\n",
      "Epoch 1181: train loss = 0.0001, val loss = 0.1118\n",
      "Epoch 1182: train loss = 0.0001, val loss = 0.1131\n",
      "Epoch 1183: train loss = 0.0001, val loss = 0.1133\n",
      "Epoch 1184: train loss = 0.0001, val loss = 0.1128\n",
      "Epoch 1185: train loss = 0.0001, val loss = 0.1127\n",
      "Epoch 1186: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 1187: train loss = 0.0001, val loss = 0.1125\n",
      "Epoch 1188: train loss = 0.0001, val loss = 0.1126\n",
      "Epoch 1189: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 1190: train loss = 0.0001, val loss = 0.1127\n",
      "Epoch 1191: train loss = 0.0001, val loss = 0.1121\n",
      "Epoch 1192: train loss = 0.0001, val loss = 0.1122\n",
      "Epoch 1193: train loss = 0.0001, val loss = 0.1121\n",
      "Epoch 1194: train loss = 0.0001, val loss = 0.1129\n",
      "Epoch 1195: train loss = 0.0001, val loss = 0.1115\n",
      "Epoch 1196: train loss = 0.0001, val loss = 0.1127\n",
      "Epoch 1197: train loss = 0.0001, val loss = 0.1112\n",
      "Epoch 1198: train loss = 0.0001, val loss = 0.1110\n",
      "Epoch 1199: train loss = 0.0002, val loss = 0.1137\n",
      "Epoch 1200: train loss = 0.0001, val loss = 0.1117\n",
      "Epoch 1201: train loss = 0.0001, val loss = 0.1103\n",
      "Epoch 1202: train loss = 0.0001, val loss = 0.1129\n",
      "Epoch 1203: train loss = 0.0001, val loss = 0.1124\n",
      "Epoch 1204: train loss = 0.0001, val loss = 0.1128\n",
      "Epoch 1205: train loss = 0.0001, val loss = 0.1116\n",
      "Epoch 1206: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1207: train loss = 0.0001, val loss = 0.1112\n",
      "Epoch 1208: train loss = 0.0001, val loss = 0.1127\n",
      "Epoch 1209: train loss = 0.0001, val loss = 0.1130\n",
      "Epoch 1210: train loss = 0.0001, val loss = 0.1125\n",
      "Epoch 1211: train loss = 0.0001, val loss = 0.1122\n",
      "Epoch 1212: train loss = 0.0001, val loss = 0.1117\n",
      "Epoch 1213: train loss = 0.0001, val loss = 0.1121\n",
      "Epoch 1214: train loss = 0.0001, val loss = 0.1118\n",
      "Epoch 1215: train loss = 0.0001, val loss = 0.1115\n",
      "Epoch 1216: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1217: train loss = 0.0001, val loss = 0.1115\n",
      "Epoch 1218: train loss = 0.0001, val loss = 0.1120\n",
      "Epoch 1219: train loss = 0.0001, val loss = 0.1112\n",
      "Epoch 1220: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1221: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 1222: train loss = 0.0001, val loss = 0.1119\n",
      "Epoch 1223: train loss = 0.0001, val loss = 0.1129\n",
      "Epoch 1224: train loss = 0.0001, val loss = 0.1118\n",
      "Epoch 1225: train loss = 0.0001, val loss = 0.1132\n",
      "Epoch 1226: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1227: train loss = 0.0001, val loss = 0.1123\n",
      "Epoch 1228: train loss = 0.0001, val loss = 0.1115\n",
      "Epoch 1229: train loss = 0.0001, val loss = 0.1132\n",
      "Epoch 1230: train loss = 0.0002, val loss = 0.1113\n",
      "Epoch 1231: train loss = 0.0002, val loss = 0.1127\n",
      "Epoch 1232: train loss = 0.0002, val loss = 0.1099\n",
      "Epoch 1233: train loss = 0.0002, val loss = 0.1125\n",
      "Epoch 1234: train loss = 0.0002, val loss = 0.1119\n",
      "Epoch 1235: train loss = 0.0002, val loss = 0.1121\n",
      "Epoch 1236: train loss = 0.0002, val loss = 0.1118\n",
      "Epoch 1237: train loss = 0.0002, val loss = 0.1124\n",
      "Epoch 1238: train loss = 0.0003, val loss = 0.1110\n",
      "Epoch 1239: train loss = 0.0003, val loss = 0.1116\n",
      "Epoch 1240: train loss = 0.0003, val loss = 0.1094\n",
      "Epoch 1241: train loss = 0.0003, val loss = 0.1087\n",
      "Epoch 1242: train loss = 0.0003, val loss = 0.1106\n",
      "Epoch 1243: train loss = 0.0003, val loss = 0.1128\n",
      "Epoch 1244: train loss = 0.0003, val loss = 0.1097\n",
      "Epoch 1245: train loss = 0.0003, val loss = 0.1115\n",
      "Epoch 1246: train loss = 0.0003, val loss = 0.1104\n",
      "Epoch 1247: train loss = 0.0003, val loss = 0.1113\n",
      "Epoch 1248: train loss = 0.0004, val loss = 0.1118\n",
      "Epoch 1249: train loss = 0.0004, val loss = 0.1103\n",
      "Epoch 1250: train loss = 0.0003, val loss = 0.1104\n",
      "Epoch 1251: train loss = 0.0003, val loss = 0.1100\n",
      "Epoch 1252: train loss = 0.0003, val loss = 0.1079\n",
      "Epoch 1253: train loss = 0.0003, val loss = 0.1110\n",
      "Epoch 1254: train loss = 0.0003, val loss = 0.1079\n",
      "Epoch 1255: train loss = 0.0002, val loss = 0.1100\n",
      "Epoch 1256: train loss = 0.0002, val loss = 0.1089\n",
      "Epoch 1257: train loss = 0.0002, val loss = 0.1097\n",
      "Epoch 1258: train loss = 0.0002, val loss = 0.1087\n",
      "Epoch 1259: train loss = 0.0002, val loss = 0.1094\n",
      "Epoch 1260: train loss = 0.0002, val loss = 0.1091\n",
      "Epoch 1261: train loss = 0.0001, val loss = 0.1106\n",
      "Epoch 1262: train loss = 0.0001, val loss = 0.1106\n",
      "Epoch 1263: train loss = 0.0001, val loss = 0.1114\n",
      "Epoch 1264: train loss = 0.0001, val loss = 0.1105\n",
      "Epoch 1265: train loss = 0.0001, val loss = 0.1105\n",
      "Epoch 1266: train loss = 0.0001, val loss = 0.1104\n",
      "Epoch 1267: train loss = 0.0001, val loss = 0.1100\n",
      "Epoch 1268: train loss = 0.0001, val loss = 0.1096\n",
      "Epoch 1269: train loss = 0.0001, val loss = 0.1104\n",
      "Epoch 1270: train loss = 0.0001, val loss = 0.1100\n",
      "Epoch 1271: train loss = 0.0001, val loss = 0.1105\n",
      "Epoch 1272: train loss = 0.0001, val loss = 0.1098\n",
      "Epoch 1273: train loss = 0.0001, val loss = 0.1102\n",
      "Epoch 1274: train loss = 0.0001, val loss = 0.1089\n",
      "Epoch 1275: train loss = 0.0001, val loss = 0.1107\n",
      "Epoch 1276: train loss = 0.0001, val loss = 0.1107\n",
      "Epoch 1277: train loss = 0.0001, val loss = 0.1092\n",
      "Epoch 1278: train loss = 0.0001, val loss = 0.1105\n",
      "Epoch 1279: train loss = 0.0000, val loss = 0.1100\n",
      "Epoch 1280: train loss = 0.0001, val loss = 0.1107\n",
      "Epoch 1281: train loss = 0.0001, val loss = 0.1101\n",
      "Epoch 1282: train loss = 0.0001, val loss = 0.1100\n",
      "Epoch 1283: train loss = 0.0001, val loss = 0.1096\n",
      "Epoch 1284: train loss = 0.0001, val loss = 0.1099\n",
      "Epoch 1285: train loss = 0.0001, val loss = 0.1101\n",
      "Epoch 1286: train loss = 0.0001, val loss = 0.1099\n",
      "Epoch 1287: train loss = 0.0001, val loss = 0.1094\n",
      "Epoch 1288: train loss = 0.0001, val loss = 0.1105\n",
      "Epoch 1289: train loss = 0.0001, val loss = 0.1094\n",
      "Epoch 1290: train loss = 0.0001, val loss = 0.1113\n",
      "Epoch 1291: train loss = 0.0001, val loss = 0.1101\n",
      "Epoch 1292: train loss = 0.0001, val loss = 0.1102\n",
      "Epoch 1293: train loss = 0.0001, val loss = 0.1098\n",
      "Epoch 1294: train loss = 0.0001, val loss = 0.1104\n",
      "Epoch 1295: train loss = 0.0001, val loss = 0.1105\n",
      "Epoch 1296: train loss = 0.0001, val loss = 0.1097\n",
      "Epoch 1297: train loss = 0.0001, val loss = 0.1098\n",
      "Epoch 1298: train loss = 0.0001, val loss = 0.1100\n",
      "Epoch 1299: train loss = 0.0001, val loss = 0.1101\n",
      "Epoch 1300: train loss = 0.0001, val loss = 0.1102\n",
      "Epoch 1301: train loss = 0.0001, val loss = 0.1108\n",
      "Epoch 1302: train loss = 0.0001, val loss = 0.1092\n",
      "Epoch 1303: train loss = 0.0001, val loss = 0.1100\n",
      "Epoch 1304: train loss = 0.0001, val loss = 0.1093\n",
      "Epoch 1305: train loss = 0.0001, val loss = 0.1099\n",
      "Epoch 1306: train loss = 0.0001, val loss = 0.1087\n",
      "Epoch 1307: train loss = 0.0001, val loss = 0.1099\n",
      "Epoch 1308: train loss = 0.0001, val loss = 0.1090\n",
      "Epoch 1309: train loss = 0.0001, val loss = 0.1103\n",
      "Epoch 1310: train loss = 0.0001, val loss = 0.1092\n",
      "Epoch 1311: train loss = 0.0001, val loss = 0.1092\n",
      "Epoch 1312: train loss = 0.0002, val loss = 0.1090\n",
      "Epoch 1313: train loss = 0.0001, val loss = 0.1075\n",
      "Epoch 1314: train loss = 0.0002, val loss = 0.1090\n",
      "Epoch 1315: train loss = 0.0002, val loss = 0.1092\n",
      "Epoch 1316: train loss = 0.0002, val loss = 0.1091\n",
      "Epoch 1317: train loss = 0.0002, val loss = 0.1070\n",
      "Epoch 1318: train loss = 0.0002, val loss = 0.1093\n",
      "Epoch 1319: train loss = 0.0002, val loss = 0.1085\n",
      "Epoch 1320: train loss = 0.0002, val loss = 0.1087\n",
      "Epoch 1321: train loss = 0.0002, val loss = 0.1068\n",
      "Epoch 1322: train loss = 0.0002, val loss = 0.1092\n",
      "Epoch 1323: train loss = 0.0002, val loss = 0.1085\n",
      "Epoch 1324: train loss = 0.0002, val loss = 0.1090\n",
      "Epoch 1325: train loss = 0.0001, val loss = 0.1084\n",
      "Epoch 1326: train loss = 0.0002, val loss = 0.1098\n",
      "Epoch 1327: train loss = 0.0002, val loss = 0.1089\n",
      "Epoch 1328: train loss = 0.0002, val loss = 0.1081\n",
      "Epoch 1329: train loss = 0.0002, val loss = 0.1080\n",
      "Epoch 1330: train loss = 0.0001, val loss = 0.1080\n",
      "Epoch 1331: train loss = 0.0001, val loss = 0.1090\n",
      "Epoch 1332: train loss = 0.0001, val loss = 0.1068\n",
      "Epoch 1333: train loss = 0.0001, val loss = 0.1082\n",
      "Epoch 1334: train loss = 0.0001, val loss = 0.1082\n",
      "Epoch 1335: train loss = 0.0001, val loss = 0.1073\n",
      "Epoch 1336: train loss = 0.0002, val loss = 0.1091\n",
      "Epoch 1337: train loss = 0.0002, val loss = 0.1077\n",
      "Epoch 1338: train loss = 0.0002, val loss = 0.1092\n",
      "Epoch 1339: train loss = 0.0001, val loss = 0.1082\n",
      "Epoch 1340: train loss = 0.0001, val loss = 0.1080\n",
      "Epoch 1341: train loss = 0.0001, val loss = 0.1075\n",
      "Epoch 1342: train loss = 0.0001, val loss = 0.1081\n",
      "Epoch 1343: train loss = 0.0001, val loss = 0.1093\n",
      "Epoch 1344: train loss = 0.0001, val loss = 0.1087\n",
      "Epoch 1345: train loss = 0.0001, val loss = 0.1093\n",
      "Epoch 1346: train loss = 0.0001, val loss = 0.1073\n",
      "Epoch 1347: train loss = 0.0001, val loss = 0.1079\n",
      "Epoch 1348: train loss = 0.0001, val loss = 0.1089\n",
      "Epoch 1349: train loss = 0.0001, val loss = 0.1089\n",
      "Epoch 1350: train loss = 0.0001, val loss = 0.1074\n",
      "Epoch 1351: train loss = 0.0001, val loss = 0.1093\n",
      "Epoch 1352: train loss = 0.0002, val loss = 0.1091\n",
      "Epoch 1353: train loss = 0.0002, val loss = 0.1085\n",
      "Epoch 1354: train loss = 0.0002, val loss = 0.1076\n",
      "Epoch 1355: train loss = 0.0002, val loss = 0.1086\n",
      "Epoch 1356: train loss = 0.0002, val loss = 0.1069\n",
      "Epoch 1357: train loss = 0.0002, val loss = 0.1082\n",
      "Epoch 1358: train loss = 0.0001, val loss = 0.1075\n",
      "Epoch 1359: train loss = 0.0001, val loss = 0.1083\n",
      "Epoch 1360: train loss = 0.0001, val loss = 0.1070\n",
      "Epoch 1361: train loss = 0.0001, val loss = 0.1080\n",
      "Epoch 1362: train loss = 0.0002, val loss = 0.1078\n",
      "Epoch 1363: train loss = 0.0003, val loss = 0.1074\n",
      "Epoch 1364: train loss = 0.0003, val loss = 0.1081\n",
      "Epoch 1365: train loss = 0.0002, val loss = 0.1065\n",
      "Epoch 1366: train loss = 0.0002, val loss = 0.1081\n",
      "Epoch 1367: train loss = 0.0002, val loss = 0.1055\n",
      "Epoch 1368: train loss = 0.0002, val loss = 0.1093\n",
      "Epoch 1369: train loss = 0.0002, val loss = 0.1077\n",
      "Epoch 1370: train loss = 0.0002, val loss = 0.1081\n",
      "Epoch 1371: train loss = 0.0002, val loss = 0.1088\n",
      "Epoch 1372: train loss = 0.0002, val loss = 0.1068\n",
      "Epoch 1373: train loss = 0.0002, val loss = 0.1085\n",
      "Epoch 1374: train loss = 0.0003, val loss = 0.1082\n",
      "Epoch 1375: train loss = 0.0002, val loss = 0.1068\n",
      "Epoch 1376: train loss = 0.0003, val loss = 0.1091\n",
      "Epoch 1377: train loss = 0.0003, val loss = 0.1081\n",
      "Epoch 1378: train loss = 0.0002, val loss = 0.1071\n",
      "Epoch 1379: train loss = 0.0002, val loss = 0.1068\n",
      "Epoch 1380: train loss = 0.0003, val loss = 0.1064\n",
      "Epoch 1381: train loss = 0.0002, val loss = 0.1063\n",
      "Epoch 1382: train loss = 0.0002, val loss = 0.1069\n",
      "Epoch 1383: train loss = 0.0002, val loss = 0.1058\n",
      "Epoch 1384: train loss = 0.0002, val loss = 0.1076\n",
      "Epoch 1385: train loss = 0.0001, val loss = 0.1055\n",
      "Epoch 1386: train loss = 0.0001, val loss = 0.1079\n",
      "Epoch 1387: train loss = 0.0001, val loss = 0.1058\n",
      "Epoch 1388: train loss = 0.0001, val loss = 0.1072\n",
      "Epoch 1389: train loss = 0.0001, val loss = 0.1068\n",
      "Epoch 1390: train loss = 0.0001, val loss = 0.1075\n",
      "Epoch 1391: train loss = 0.0001, val loss = 0.1065\n",
      "Epoch 1392: train loss = 0.0001, val loss = 0.1073\n",
      "Epoch 1393: train loss = 0.0001, val loss = 0.1068\n",
      "Epoch 1394: train loss = 0.0001, val loss = 0.1074\n",
      "Epoch 1395: train loss = 0.0001, val loss = 0.1073\n",
      "Epoch 1396: train loss = 0.0001, val loss = 0.1065\n",
      "Epoch 1397: train loss = 0.0001, val loss = 0.1060\n",
      "Epoch 1398: train loss = 0.0001, val loss = 0.1071\n",
      "Epoch 1399: train loss = 0.0001, val loss = 0.1070\n",
      "Epoch 1400: train loss = 0.0000, val loss = 0.1070\n",
      "Epoch 1401: train loss = 0.0000, val loss = 0.1067\n",
      "Epoch 1402: train loss = 0.0000, val loss = 0.1069\n",
      "Epoch 1403: train loss = 0.0000, val loss = 0.1075\n",
      "Epoch 1404: train loss = 0.0000, val loss = 0.1066\n",
      "Epoch 1405: train loss = 0.0000, val loss = 0.1077\n",
      "Epoch 1406: train loss = 0.0000, val loss = 0.1068\n",
      "Epoch 1407: train loss = 0.0000, val loss = 0.1076\n",
      "Epoch 1408: train loss = 0.0000, val loss = 0.1075\n",
      "Epoch 1409: train loss = 0.0000, val loss = 0.1070\n",
      "Epoch 1410: train loss = 0.0000, val loss = 0.1070\n",
      "Epoch 1411: train loss = 0.0001, val loss = 0.1068\n",
      "Epoch 1412: train loss = 0.0001, val loss = 0.1068\n",
      "Epoch 1413: train loss = 0.0001, val loss = 0.1073\n",
      "Epoch 1414: train loss = 0.0001, val loss = 0.1069\n",
      "Epoch 1415: train loss = 0.0001, val loss = 0.1074\n",
      "Epoch 1416: train loss = 0.0000, val loss = 0.1067\n",
      "Epoch 1417: train loss = 0.0000, val loss = 0.1078\n",
      "Epoch 1418: train loss = 0.0000, val loss = 0.1061\n",
      "Epoch 1419: train loss = 0.0000, val loss = 0.1074\n",
      "Epoch 1420: train loss = 0.0000, val loss = 0.1067\n",
      "Epoch 1421: train loss = 0.0000, val loss = 0.1071\n",
      "Epoch 1422: train loss = 0.0000, val loss = 0.1072\n",
      "Epoch 1423: train loss = 0.0001, val loss = 0.1070\n",
      "Epoch 1424: train loss = 0.0001, val loss = 0.1067\n",
      "Epoch 1425: train loss = 0.0001, val loss = 0.1074\n",
      "Epoch 1426: train loss = 0.0001, val loss = 0.1058\n",
      "Epoch 1427: train loss = 0.0001, val loss = 0.1069\n",
      "Epoch 1428: train loss = 0.0001, val loss = 0.1068\n",
      "Epoch 1429: train loss = 0.0001, val loss = 0.1070\n",
      "Epoch 1430: train loss = 0.0001, val loss = 0.1059\n",
      "Epoch 1431: train loss = 0.0001, val loss = 0.1072\n",
      "Epoch 1432: train loss = 0.0001, val loss = 0.1066\n",
      "Epoch 1433: train loss = 0.0001, val loss = 0.1066\n",
      "Epoch 1434: train loss = 0.0001, val loss = 0.1064\n",
      "Epoch 1435: train loss = 0.0001, val loss = 0.1061\n",
      "Epoch 1436: train loss = 0.0001, val loss = 0.1065\n",
      "Epoch 1437: train loss = 0.0001, val loss = 0.1063\n",
      "Epoch 1438: train loss = 0.0001, val loss = 0.1055\n",
      "Epoch 1439: train loss = 0.0001, val loss = 0.1074\n",
      "Epoch 1440: train loss = 0.0001, val loss = 0.1059\n",
      "Epoch 1441: train loss = 0.0002, val loss = 0.1070\n",
      "Epoch 1442: train loss = 0.0002, val loss = 0.1054\n",
      "Epoch 1443: train loss = 0.0002, val loss = 0.1062\n",
      "Epoch 1444: train loss = 0.0002, val loss = 0.1043\n",
      "Epoch 1445: train loss = 0.0002, val loss = 0.1083\n",
      "Epoch 1446: train loss = 0.0003, val loss = 0.1070\n",
      "Epoch 1447: train loss = 0.0003, val loss = 0.1050\n",
      "Epoch 1448: train loss = 0.0003, val loss = 0.1071\n",
      "Epoch 1449: train loss = 0.0003, val loss = 0.1054\n",
      "Epoch 1450: train loss = 0.0003, val loss = 0.1039\n",
      "Epoch 1451: train loss = 0.0003, val loss = 0.1058\n",
      "Epoch 1452: train loss = 0.0003, val loss = 0.1028\n",
      "Epoch 1453: train loss = 0.0003, val loss = 0.1070\n",
      "Epoch 1454: train loss = 0.0003, val loss = 0.1033\n",
      "Epoch 1455: train loss = 0.0003, val loss = 0.1050\n",
      "Epoch 1456: train loss = 0.0002, val loss = 0.1047\n",
      "Epoch 1457: train loss = 0.0003, val loss = 0.1051\n",
      "Epoch 1458: train loss = 0.0003, val loss = 0.1049\n",
      "Epoch 1459: train loss = 0.0003, val loss = 0.1072\n",
      "Epoch 1460: train loss = 0.0003, val loss = 0.1038\n",
      "Epoch 1461: train loss = 0.0002, val loss = 0.1054\n",
      "Epoch 1462: train loss = 0.0002, val loss = 0.1054\n",
      "Epoch 1463: train loss = 0.0003, val loss = 0.1047\n",
      "Epoch 1464: train loss = 0.0003, val loss = 0.1048\n",
      "Epoch 1465: train loss = 0.0003, val loss = 0.1053\n",
      "Epoch 1466: train loss = 0.0003, val loss = 0.1038\n",
      "Epoch 1467: train loss = 0.0003, val loss = 0.1042\n",
      "Epoch 1468: train loss = 0.0002, val loss = 0.1063\n",
      "Epoch 1469: train loss = 0.0002, val loss = 0.1028\n",
      "Epoch 1470: train loss = 0.0002, val loss = 0.1040\n",
      "Epoch 1471: train loss = 0.0002, val loss = 0.1042\n",
      "Epoch 1472: train loss = 0.0001, val loss = 0.1043\n",
      "Epoch 1473: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 1474: train loss = 0.0001, val loss = 0.1043\n",
      "Epoch 1475: train loss = 0.0001, val loss = 0.1054\n",
      "Epoch 1476: train loss = 0.0001, val loss = 0.1056\n",
      "Epoch 1477: train loss = 0.0001, val loss = 0.1049\n",
      "Epoch 1478: train loss = 0.0001, val loss = 0.1055\n",
      "Epoch 1479: train loss = 0.0001, val loss = 0.1049\n",
      "Epoch 1480: train loss = 0.0001, val loss = 0.1051\n",
      "Epoch 1481: train loss = 0.0000, val loss = 0.1045\n",
      "Epoch 1482: train loss = 0.0000, val loss = 0.1057\n",
      "Epoch 1483: train loss = 0.0000, val loss = 0.1045\n",
      "Epoch 1484: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 1485: train loss = 0.0000, val loss = 0.1052\n",
      "Epoch 1486: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 1487: train loss = 0.0000, val loss = 0.1047\n",
      "Epoch 1488: train loss = 0.0000, val loss = 0.1055\n",
      "Epoch 1489: train loss = 0.0000, val loss = 0.1044\n",
      "Epoch 1490: train loss = 0.0001, val loss = 0.1050\n",
      "Epoch 1491: train loss = 0.0000, val loss = 0.1050\n",
      "Epoch 1492: train loss = 0.0000, val loss = 0.1047\n",
      "Epoch 1493: train loss = 0.0000, val loss = 0.1051\n",
      "Epoch 1494: train loss = 0.0000, val loss = 0.1052\n",
      "Epoch 1495: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 1496: train loss = 0.0000, val loss = 0.1052\n",
      "Epoch 1497: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 1498: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 1499: train loss = 0.0000, val loss = 0.1054\n",
      "Epoch 1500: train loss = 0.0000, val loss = 0.1051\n",
      "Epoch 1501: train loss = 0.0000, val loss = 0.1054\n",
      "Epoch 1502: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 1503: train loss = 0.0000, val loss = 0.1051\n",
      "Epoch 1504: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 1505: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 1506: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 1507: train loss = 0.0000, val loss = 0.1046\n",
      "Epoch 1508: train loss = 0.0000, val loss = 0.1050\n",
      "Epoch 1509: train loss = 0.0000, val loss = 0.1053\n",
      "Epoch 1510: train loss = 0.0000, val loss = 0.1046\n",
      "Epoch 1511: train loss = 0.0000, val loss = 0.1045\n",
      "Epoch 1512: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 1513: train loss = 0.0000, val loss = 0.1045\n",
      "Epoch 1514: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 1515: train loss = 0.0000, val loss = 0.1050\n",
      "Epoch 1516: train loss = 0.0000, val loss = 0.1046\n",
      "Epoch 1517: train loss = 0.0000, val loss = 0.1051\n",
      "Epoch 1518: train loss = 0.0000, val loss = 0.1046\n",
      "Epoch 1519: train loss = 0.0000, val loss = 0.1053\n",
      "Epoch 1520: train loss = 0.0000, val loss = 0.1044\n",
      "Epoch 1521: train loss = 0.0000, val loss = 0.1056\n",
      "Epoch 1522: train loss = 0.0000, val loss = 0.1045\n",
      "Epoch 1523: train loss = 0.0000, val loss = 0.1051\n",
      "Epoch 1524: train loss = 0.0000, val loss = 0.1051\n",
      "Epoch 1525: train loss = 0.0000, val loss = 0.1047\n",
      "Epoch 1526: train loss = 0.0000, val loss = 0.1049\n",
      "Epoch 1527: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 1528: train loss = 0.0000, val loss = 0.1053\n",
      "Epoch 1529: train loss = 0.0000, val loss = 0.1048\n",
      "Epoch 1530: train loss = 0.0000, val loss = 0.1046\n",
      "Epoch 1531: train loss = 0.0000, val loss = 0.1057\n",
      "Epoch 1532: train loss = 0.0000, val loss = 0.1047\n",
      "Epoch 1533: train loss = 0.0000, val loss = 0.1057\n",
      "Epoch 1534: train loss = 0.0000, val loss = 0.1043\n",
      "Epoch 1535: train loss = 0.0001, val loss = 0.1047\n",
      "Epoch 1536: train loss = 0.0001, val loss = 0.1047\n",
      "Epoch 1537: train loss = 0.0001, val loss = 0.1049\n",
      "Epoch 1538: train loss = 0.0001, val loss = 0.1052\n",
      "Epoch 1539: train loss = 0.0001, val loss = 0.1058\n",
      "Epoch 1540: train loss = 0.0001, val loss = 0.1055\n",
      "Epoch 1541: train loss = 0.0001, val loss = 0.1049\n",
      "Epoch 1542: train loss = 0.0001, val loss = 0.1042\n",
      "Epoch 1543: train loss = 0.0001, val loss = 0.1040\n",
      "Epoch 1544: train loss = 0.0001, val loss = 0.1045\n",
      "Epoch 1545: train loss = 0.0001, val loss = 0.1048\n",
      "Epoch 1546: train loss = 0.0001, val loss = 0.1041\n",
      "Epoch 1547: train loss = 0.0001, val loss = 0.1041\n",
      "Epoch 1548: train loss = 0.0002, val loss = 0.1044\n",
      "Epoch 1549: train loss = 0.0002, val loss = 0.1038\n",
      "Epoch 1550: train loss = 0.0002, val loss = 0.1045\n",
      "Epoch 1551: train loss = 0.0002, val loss = 0.1059\n",
      "Epoch 1552: train loss = 0.0002, val loss = 0.1037\n",
      "Epoch 1553: train loss = 0.0002, val loss = 0.1045\n",
      "Epoch 1554: train loss = 0.0003, val loss = 0.1050\n",
      "Epoch 1555: train loss = 0.0003, val loss = 0.1059\n",
      "Epoch 1556: train loss = 0.0003, val loss = 0.1022\n",
      "Epoch 1557: train loss = 0.0003, val loss = 0.1030\n",
      "Epoch 1558: train loss = 0.0003, val loss = 0.1051\n",
      "Epoch 1559: train loss = 0.0003, val loss = 0.1027\n",
      "Epoch 1560: train loss = 0.0002, val loss = 0.1029\n",
      "Epoch 1561: train loss = 0.0003, val loss = 0.1047\n",
      "Epoch 1562: train loss = 0.0003, val loss = 0.1020\n",
      "Epoch 1563: train loss = 0.0003, val loss = 0.1043\n",
      "Epoch 1564: train loss = 0.0003, val loss = 0.1032\n",
      "Epoch 1565: train loss = 0.0003, val loss = 0.1044\n",
      "Epoch 1566: train loss = 0.0002, val loss = 0.1036\n",
      "Epoch 1567: train loss = 0.0002, val loss = 0.1039\n",
      "Epoch 1568: train loss = 0.0002, val loss = 0.1041\n",
      "Epoch 1569: train loss = 0.0002, val loss = 0.1024\n",
      "Epoch 1570: train loss = 0.0002, val loss = 0.1035\n",
      "Epoch 1571: train loss = 0.0002, val loss = 0.1038\n",
      "Epoch 1572: train loss = 0.0002, val loss = 0.1048\n",
      "Epoch 1573: train loss = 0.0002, val loss = 0.1042\n",
      "Epoch 1574: train loss = 0.0003, val loss = 0.1029\n",
      "Epoch 1575: train loss = 0.0002, val loss = 0.1024\n",
      "Epoch 1576: train loss = 0.0003, val loss = 0.1036\n",
      "Epoch 1577: train loss = 0.0002, val loss = 0.1034\n",
      "Epoch 1578: train loss = 0.0002, val loss = 0.1024\n",
      "Epoch 1579: train loss = 0.0002, val loss = 0.1035\n",
      "Epoch 1580: train loss = 0.0002, val loss = 0.1017\n",
      "Epoch 1581: train loss = 0.0002, val loss = 0.1043\n",
      "Epoch 1582: train loss = 0.0002, val loss = 0.1028\n",
      "Epoch 1583: train loss = 0.0001, val loss = 0.1034\n",
      "Epoch 1584: train loss = 0.0001, val loss = 0.1027\n",
      "Epoch 1585: train loss = 0.0001, val loss = 0.1038\n",
      "Epoch 1586: train loss = 0.0001, val loss = 0.1040\n",
      "Epoch 1587: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1588: train loss = 0.0001, val loss = 0.1051\n",
      "Epoch 1589: train loss = 0.0001, val loss = 0.1007\n",
      "Epoch 1590: train loss = 0.0001, val loss = 0.1032\n",
      "Epoch 1591: train loss = 0.0001, val loss = 0.1025\n",
      "Epoch 1592: train loss = 0.0001, val loss = 0.1041\n",
      "Epoch 1593: train loss = 0.0001, val loss = 0.1038\n",
      "Epoch 1594: train loss = 0.0001, val loss = 0.1027\n",
      "Epoch 1595: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 1596: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 1597: train loss = 0.0001, val loss = 0.1025\n",
      "Epoch 1598: train loss = 0.0001, val loss = 0.1043\n",
      "Epoch 1599: train loss = 0.0001, val loss = 0.1032\n",
      "Epoch 1600: train loss = 0.0001, val loss = 0.1035\n",
      "Epoch 1601: train loss = 0.0001, val loss = 0.1029\n",
      "Epoch 1602: train loss = 0.0001, val loss = 0.1035\n",
      "Epoch 1603: train loss = 0.0001, val loss = 0.1028\n",
      "Epoch 1604: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 1605: train loss = 0.0001, val loss = 0.1031\n",
      "Epoch 1606: train loss = 0.0001, val loss = 0.1036\n",
      "Epoch 1607: train loss = 0.0001, val loss = 0.1032\n",
      "Epoch 1608: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 1609: train loss = 0.0000, val loss = 0.1026\n",
      "Epoch 1610: train loss = 0.0000, val loss = 0.1029\n",
      "Epoch 1611: train loss = 0.0000, val loss = 0.1031\n",
      "Epoch 1612: train loss = 0.0000, val loss = 0.1032\n",
      "Epoch 1613: train loss = 0.0000, val loss = 0.1026\n",
      "Epoch 1614: train loss = 0.0000, val loss = 0.1028\n",
      "Epoch 1615: train loss = 0.0000, val loss = 0.1034\n",
      "Epoch 1616: train loss = 0.0001, val loss = 0.1028\n",
      "Epoch 1617: train loss = 0.0000, val loss = 0.1028\n",
      "Epoch 1618: train loss = 0.0000, val loss = 0.1024\n",
      "Epoch 1619: train loss = 0.0000, val loss = 0.1028\n",
      "Epoch 1620: train loss = 0.0000, val loss = 0.1022\n",
      "Epoch 1621: train loss = 0.0000, val loss = 0.1047\n",
      "Epoch 1622: train loss = 0.0001, val loss = 0.1015\n",
      "Epoch 1623: train loss = 0.0001, val loss = 0.1041\n",
      "Epoch 1624: train loss = 0.0001, val loss = 0.1019\n",
      "Epoch 1625: train loss = 0.0001, val loss = 0.1035\n",
      "Epoch 1626: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 1627: train loss = 0.0001, val loss = 0.1027\n",
      "Epoch 1628: train loss = 0.0001, val loss = 0.1022\n",
      "Epoch 1629: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 1630: train loss = 0.0001, val loss = 0.1020\n",
      "Epoch 1631: train loss = 0.0001, val loss = 0.1030\n",
      "Epoch 1632: train loss = 0.0001, val loss = 0.1026\n",
      "Epoch 1633: train loss = 0.0001, val loss = 0.1024\n",
      "Epoch 1634: train loss = 0.0001, val loss = 0.1020\n",
      "Epoch 1635: train loss = 0.0001, val loss = 0.1031\n",
      "Epoch 1636: train loss = 0.0001, val loss = 0.1021\n",
      "Epoch 1637: train loss = 0.0001, val loss = 0.1027\n",
      "Epoch 1638: train loss = 0.0001, val loss = 0.1019\n",
      "Epoch 1639: train loss = 0.0001, val loss = 0.1032\n",
      "Epoch 1640: train loss = 0.0001, val loss = 0.1028\n",
      "Epoch 1641: train loss = 0.0001, val loss = 0.1024\n",
      "Epoch 1642: train loss = 0.0001, val loss = 0.1031\n",
      "Epoch 1643: train loss = 0.0001, val loss = 0.1018\n",
      "Epoch 1644: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 1645: train loss = 0.0001, val loss = 0.1034\n",
      "Epoch 1646: train loss = 0.0002, val loss = 0.1012\n",
      "Epoch 1647: train loss = 0.0001, val loss = 0.1038\n",
      "Epoch 1648: train loss = 0.0001, val loss = 0.1021\n",
      "Epoch 1649: train loss = 0.0001, val loss = 0.1027\n",
      "Epoch 1650: train loss = 0.0001, val loss = 0.1022\n",
      "Epoch 1651: train loss = 0.0001, val loss = 0.1036\n",
      "Epoch 1652: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1653: train loss = 0.0001, val loss = 0.1022\n",
      "Epoch 1654: train loss = 0.0001, val loss = 0.1015\n",
      "Epoch 1655: train loss = 0.0001, val loss = 0.1027\n",
      "Epoch 1656: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 1657: train loss = 0.0001, val loss = 0.1036\n",
      "Epoch 1658: train loss = 0.0001, val loss = 0.1022\n",
      "Epoch 1659: train loss = 0.0001, val loss = 0.1020\n",
      "Epoch 1660: train loss = 0.0001, val loss = 0.1017\n",
      "Epoch 1661: train loss = 0.0001, val loss = 0.1022\n",
      "Epoch 1662: train loss = 0.0001, val loss = 0.1020\n",
      "Epoch 1663: train loss = 0.0001, val loss = 0.1017\n",
      "Epoch 1664: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 1665: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1666: train loss = 0.0001, val loss = 0.1021\n",
      "Epoch 1667: train loss = 0.0002, val loss = 0.1021\n",
      "Epoch 1668: train loss = 0.0002, val loss = 0.1017\n",
      "Epoch 1669: train loss = 0.0002, val loss = 0.1031\n",
      "Epoch 1670: train loss = 0.0002, val loss = 0.0996\n",
      "Epoch 1671: train loss = 0.0002, val loss = 0.1023\n",
      "Epoch 1672: train loss = 0.0002, val loss = 0.1005\n",
      "Epoch 1673: train loss = 0.0002, val loss = 0.1001\n",
      "Epoch 1674: train loss = 0.0002, val loss = 0.1020\n",
      "Epoch 1675: train loss = 0.0002, val loss = 0.1005\n",
      "Epoch 1676: train loss = 0.0002, val loss = 0.1012\n",
      "Epoch 1677: train loss = 0.0003, val loss = 0.1017\n",
      "Epoch 1678: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 1679: train loss = 0.0003, val loss = 0.0995\n",
      "Epoch 1680: train loss = 0.0002, val loss = 0.0997\n",
      "Epoch 1681: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 1682: train loss = 0.0002, val loss = 0.0993\n",
      "Epoch 1683: train loss = 0.0002, val loss = 0.0998\n",
      "Epoch 1684: train loss = 0.0002, val loss = 0.1010\n",
      "Epoch 1685: train loss = 0.0003, val loss = 0.1021\n",
      "Epoch 1686: train loss = 0.0002, val loss = 0.1007\n",
      "Epoch 1687: train loss = 0.0002, val loss = 0.1013\n",
      "Epoch 1688: train loss = 0.0002, val loss = 0.1004\n",
      "Epoch 1689: train loss = 0.0001, val loss = 0.1007\n",
      "Epoch 1690: train loss = 0.0001, val loss = 0.0997\n",
      "Epoch 1691: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1692: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1693: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1694: train loss = 0.0001, val loss = 0.1009\n",
      "Epoch 1695: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1696: train loss = 0.0001, val loss = 0.1012\n",
      "Epoch 1697: train loss = 0.0001, val loss = 0.1002\n",
      "Epoch 1698: train loss = 0.0001, val loss = 0.1009\n",
      "Epoch 1699: train loss = 0.0001, val loss = 0.1001\n",
      "Epoch 1700: train loss = 0.0000, val loss = 0.1016\n",
      "Epoch 1701: train loss = 0.0000, val loss = 0.0998\n",
      "Epoch 1702: train loss = 0.0001, val loss = 0.1005\n",
      "Epoch 1703: train loss = 0.0001, val loss = 0.1012\n",
      "Epoch 1704: train loss = 0.0001, val loss = 0.1010\n",
      "Epoch 1705: train loss = 0.0001, val loss = 0.1010\n",
      "Epoch 1706: train loss = 0.0000, val loss = 0.1003\n",
      "Epoch 1707: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 1708: train loss = 0.0001, val loss = 0.1005\n",
      "Epoch 1709: train loss = 0.0001, val loss = 0.1006\n",
      "Epoch 1710: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 1711: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 1712: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 1713: train loss = 0.0000, val loss = 0.1008\n",
      "Epoch 1714: train loss = 0.0000, val loss = 0.1001\n",
      "Epoch 1715: train loss = 0.0000, val loss = 0.1002\n",
      "Epoch 1716: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 1717: train loss = 0.0000, val loss = 0.1007\n",
      "Epoch 1718: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 1719: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 1720: train loss = 0.0000, val loss = 0.1011\n",
      "Epoch 1721: train loss = 0.0000, val loss = 0.1003\n",
      "Epoch 1722: train loss = 0.0000, val loss = 0.1008\n",
      "Epoch 1723: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 1724: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 1725: train loss = 0.0000, val loss = 0.1007\n",
      "Epoch 1726: train loss = 0.0000, val loss = 0.1008\n",
      "Epoch 1727: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 1728: train loss = 0.0000, val loss = 0.1011\n",
      "Epoch 1729: train loss = 0.0000, val loss = 0.1007\n",
      "Epoch 1730: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 1731: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 1732: train loss = 0.0000, val loss = 0.1006\n",
      "Epoch 1733: train loss = 0.0000, val loss = 0.1009\n",
      "Epoch 1734: train loss = 0.0000, val loss = 0.1010\n",
      "Epoch 1735: train loss = 0.0000, val loss = 0.1008\n",
      "Epoch 1736: train loss = 0.0000, val loss = 0.1001\n",
      "Epoch 1737: train loss = 0.0000, val loss = 0.1010\n",
      "Epoch 1738: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 1739: train loss = 0.0000, val loss = 0.1010\n",
      "Epoch 1740: train loss = 0.0000, val loss = 0.1004\n",
      "Epoch 1741: train loss = 0.0000, val loss = 0.1006\n",
      "Epoch 1742: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 1743: train loss = 0.0000, val loss = 0.1003\n",
      "Epoch 1744: train loss = 0.0000, val loss = 0.1010\n",
      "Epoch 1745: train loss = 0.0000, val loss = 0.1001\n",
      "Epoch 1746: train loss = 0.0000, val loss = 0.1001\n",
      "Epoch 1747: train loss = 0.0000, val loss = 0.1005\n",
      "Epoch 1748: train loss = 0.0000, val loss = 0.1013\n",
      "Epoch 1749: train loss = 0.0001, val loss = 0.1001\n",
      "Epoch 1750: train loss = 0.0001, val loss = 0.1005\n",
      "Epoch 1751: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 1752: train loss = 0.0001, val loss = 0.0995\n",
      "Epoch 1753: train loss = 0.0001, val loss = 0.1011\n",
      "Epoch 1754: train loss = 0.0001, val loss = 0.0994\n",
      "Epoch 1755: train loss = 0.0001, val loss = 0.1003\n",
      "Epoch 1756: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 1757: train loss = 0.0001, val loss = 0.1010\n",
      "Epoch 1758: train loss = 0.0001, val loss = 0.1001\n",
      "Epoch 1759: train loss = 0.0001, val loss = 0.1009\n",
      "Epoch 1760: train loss = 0.0001, val loss = 0.1005\n",
      "Epoch 1761: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 1762: train loss = 0.0001, val loss = 0.1006\n",
      "Epoch 1763: train loss = 0.0001, val loss = 0.1010\n",
      "Epoch 1764: train loss = 0.0001, val loss = 0.1002\n",
      "Epoch 1765: train loss = 0.0001, val loss = 0.0999\n",
      "Epoch 1766: train loss = 0.0001, val loss = 0.1001\n",
      "Epoch 1767: train loss = 0.0001, val loss = 0.1019\n",
      "Epoch 1768: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 1769: train loss = 0.0001, val loss = 0.1010\n",
      "Epoch 1770: train loss = 0.0001, val loss = 0.1012\n",
      "Epoch 1771: train loss = 0.0001, val loss = 0.0997\n",
      "Epoch 1772: train loss = 0.0001, val loss = 0.1017\n",
      "Epoch 1773: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 1774: train loss = 0.0001, val loss = 0.1010\n",
      "Epoch 1775: train loss = 0.0001, val loss = 0.1009\n",
      "Epoch 1776: train loss = 0.0001, val loss = 0.0999\n",
      "Epoch 1777: train loss = 0.0001, val loss = 0.1000\n",
      "Epoch 1778: train loss = 0.0000, val loss = 0.1007\n",
      "Epoch 1779: train loss = 0.0001, val loss = 0.0996\n",
      "Epoch 1780: train loss = 0.0001, val loss = 0.1009\n",
      "Epoch 1781: train loss = 0.0001, val loss = 0.1004\n",
      "Epoch 1782: train loss = 0.0001, val loss = 0.1002\n",
      "Epoch 1783: train loss = 0.0001, val loss = 0.0992\n",
      "Epoch 1784: train loss = 0.0001, val loss = 0.1016\n",
      "Epoch 1785: train loss = 0.0001, val loss = 0.1001\n",
      "Epoch 1786: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 1787: train loss = 0.0002, val loss = 0.0996\n",
      "Epoch 1788: train loss = 0.0002, val loss = 0.1008\n",
      "Epoch 1789: train loss = 0.0002, val loss = 0.0986\n",
      "Epoch 1790: train loss = 0.0002, val loss = 0.0994\n",
      "Epoch 1791: train loss = 0.0002, val loss = 0.0991\n",
      "Epoch 1792: train loss = 0.0003, val loss = 0.1011\n",
      "Epoch 1793: train loss = 0.0003, val loss = 0.0989\n",
      "Epoch 1794: train loss = 0.0004, val loss = 0.1010\n",
      "Epoch 1795: train loss = 0.0005, val loss = 0.0983\n",
      "Epoch 1796: train loss = 0.0005, val loss = 0.0992\n",
      "Epoch 1797: train loss = 0.0006, val loss = 0.0995\n",
      "Epoch 1798: train loss = 0.0006, val loss = 0.1004\n",
      "Epoch 1799: train loss = 0.0006, val loss = 0.0980\n",
      "Epoch 1800: train loss = 0.0006, val loss = 0.0964\n",
      "Epoch 1801: train loss = 0.0005, val loss = 0.0994\n",
      "Epoch 1802: train loss = 0.0005, val loss = 0.0967\n",
      "Epoch 1803: train loss = 0.0004, val loss = 0.0975\n",
      "Epoch 1804: train loss = 0.0003, val loss = 0.0990\n",
      "Epoch 1805: train loss = 0.0003, val loss = 0.0985\n",
      "Epoch 1806: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 1807: train loss = 0.0002, val loss = 0.0980\n",
      "Epoch 1808: train loss = 0.0002, val loss = 0.0970\n",
      "Epoch 1809: train loss = 0.0002, val loss = 0.0984\n",
      "Epoch 1810: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 1811: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 1812: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 1813: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 1814: train loss = 0.0001, val loss = 0.0989\n",
      "Epoch 1815: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1816: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 1817: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 1818: train loss = 0.0001, val loss = 0.0987\n",
      "Epoch 1819: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 1820: train loss = 0.0000, val loss = 0.0983\n",
      "Epoch 1821: train loss = 0.0000, val loss = 0.0982\n",
      "Epoch 1822: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1823: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1824: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1825: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1826: train loss = 0.0000, val loss = 0.0988\n",
      "Epoch 1827: train loss = 0.0000, val loss = 0.0989\n",
      "Epoch 1828: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1829: train loss = 0.0000, val loss = 0.0979\n",
      "Epoch 1830: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1831: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1832: train loss = 0.0000, val loss = 0.0988\n",
      "Epoch 1833: train loss = 0.0000, val loss = 0.0988\n",
      "Epoch 1834: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1835: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1836: train loss = 0.0000, val loss = 0.0983\n",
      "Epoch 1837: train loss = 0.0000, val loss = 0.0989\n",
      "Epoch 1838: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1839: train loss = 0.0000, val loss = 0.0984\n",
      "Epoch 1840: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1841: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1842: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1843: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1844: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1845: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1846: train loss = 0.0000, val loss = 0.0991\n",
      "Epoch 1847: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1848: train loss = 0.0000, val loss = 0.0988\n",
      "Epoch 1849: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1850: train loss = 0.0000, val loss = 0.0988\n",
      "Epoch 1851: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1852: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1853: train loss = 0.0000, val loss = 0.0984\n",
      "Epoch 1854: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1855: train loss = 0.0000, val loss = 0.0988\n",
      "Epoch 1856: train loss = 0.0000, val loss = 0.0988\n",
      "Epoch 1857: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1858: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1859: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1860: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1861: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1862: train loss = 0.0000, val loss = 0.0983\n",
      "Epoch 1863: train loss = 0.0000, val loss = 0.0984\n",
      "Epoch 1864: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1865: train loss = 0.0000, val loss = 0.0987\n",
      "Epoch 1866: train loss = 0.0000, val loss = 0.0989\n",
      "Epoch 1867: train loss = 0.0000, val loss = 0.0986\n",
      "Epoch 1868: train loss = 0.0000, val loss = 0.0992\n",
      "Epoch 1869: train loss = 0.0000, val loss = 0.0985\n",
      "Epoch 1870: train loss = 0.0000, val loss = 0.0990\n",
      "Epoch 1871: train loss = 0.0000, val loss = 0.0992\n",
      "Epoch 1872: train loss = 0.0001, val loss = 0.0991\n",
      "Epoch 1873: train loss = 0.0001, val loss = 0.0983\n",
      "Epoch 1874: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 1875: train loss = 0.0001, val loss = 0.0984\n",
      "Epoch 1876: train loss = 0.0001, val loss = 0.0991\n",
      "Epoch 1877: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 1878: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 1879: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 1880: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1881: train loss = 0.0001, val loss = 0.0987\n",
      "Epoch 1882: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1883: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 1884: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1885: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 1886: train loss = 0.0001, val loss = 0.0983\n",
      "Epoch 1887: train loss = 0.0001, val loss = 0.0985\n",
      "Epoch 1888: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 1889: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1890: train loss = 0.0001, val loss = 0.0984\n",
      "Epoch 1891: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 1892: train loss = 0.0001, val loss = 0.0988\n",
      "Epoch 1893: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 1894: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 1895: train loss = 0.0001, val loss = 0.0983\n",
      "Epoch 1896: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 1897: train loss = 0.0001, val loss = 0.0987\n",
      "Epoch 1898: train loss = 0.0001, val loss = 0.1000\n",
      "Epoch 1899: train loss = 0.0002, val loss = 0.0975\n",
      "Epoch 1900: train loss = 0.0001, val loss = 0.0987\n",
      "Epoch 1901: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 1902: train loss = 0.0002, val loss = 0.0976\n",
      "Epoch 1903: train loss = 0.0002, val loss = 0.0987\n",
      "Epoch 1904: train loss = 0.0002, val loss = 0.0972\n",
      "Epoch 1905: train loss = 0.0002, val loss = 0.0974\n",
      "Epoch 1906: train loss = 0.0002, val loss = 0.0974\n",
      "Epoch 1907: train loss = 0.0002, val loss = 0.0976\n",
      "Epoch 1908: train loss = 0.0002, val loss = 0.0983\n",
      "Epoch 1909: train loss = 0.0002, val loss = 0.0969\n",
      "Epoch 1910: train loss = 0.0002, val loss = 0.0985\n",
      "Epoch 1911: train loss = 0.0002, val loss = 0.0980\n",
      "Epoch 1912: train loss = 0.0002, val loss = 0.0978\n",
      "Epoch 1913: train loss = 0.0001, val loss = 0.0965\n",
      "Epoch 1914: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 1915: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 1916: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 1917: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 1918: train loss = 0.0001, val loss = 0.0981\n",
      "Epoch 1919: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 1920: train loss = 0.0001, val loss = 0.0975\n",
      "Epoch 1921: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 1922: train loss = 0.0001, val loss = 0.0969\n",
      "Epoch 1923: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 1924: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 1925: train loss = 0.0001, val loss = 0.0990\n",
      "Epoch 1926: train loss = 0.0001, val loss = 0.0986\n",
      "Epoch 1927: train loss = 0.0001, val loss = 0.0984\n",
      "Epoch 1928: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 1929: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 1930: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1931: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1932: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 1933: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 1934: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 1935: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 1936: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1937: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 1938: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 1939: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 1940: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 1941: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 1942: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 1943: train loss = 0.0001, val loss = 0.0970\n",
      "Epoch 1944: train loss = 0.0000, val loss = 0.0976\n",
      "Epoch 1945: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1946: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 1947: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 1948: train loss = 0.0001, val loss = 0.0972\n",
      "Epoch 1949: train loss = 0.0001, val loss = 0.0979\n",
      "Epoch 1950: train loss = 0.0000, val loss = 0.0970\n",
      "Epoch 1951: train loss = 0.0000, val loss = 0.0975\n",
      "Epoch 1952: train loss = 0.0000, val loss = 0.0973\n",
      "Epoch 1953: train loss = 0.0000, val loss = 0.0972\n",
      "Epoch 1954: train loss = 0.0000, val loss = 0.0979\n",
      "Epoch 1955: train loss = 0.0000, val loss = 0.0972\n",
      "Epoch 1956: train loss = 0.0000, val loss = 0.0972\n",
      "Epoch 1957: train loss = 0.0000, val loss = 0.0975\n",
      "Epoch 1958: train loss = 0.0000, val loss = 0.0978\n",
      "Epoch 1959: train loss = 0.0001, val loss = 0.0977\n",
      "Epoch 1960: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 1961: train loss = 0.0001, val loss = 0.0980\n",
      "Epoch 1962: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 1963: train loss = 0.0001, val loss = 0.0976\n",
      "Epoch 1964: train loss = 0.0001, val loss = 0.0971\n",
      "Epoch 1965: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 1966: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 1967: train loss = 0.0001, val loss = 0.0963\n",
      "Epoch 1968: train loss = 0.0001, val loss = 0.0968\n",
      "Epoch 1969: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 1970: train loss = 0.0001, val loss = 0.0978\n",
      "Epoch 1971: train loss = 0.0002, val loss = 0.0960\n",
      "Epoch 1972: train loss = 0.0002, val loss = 0.0973\n",
      "Epoch 1973: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 1974: train loss = 0.0003, val loss = 0.0961\n",
      "Epoch 1975: train loss = 0.0003, val loss = 0.0974\n",
      "Epoch 1976: train loss = 0.0003, val loss = 0.0976\n",
      "Epoch 1977: train loss = 0.0003, val loss = 0.0966\n",
      "Epoch 1978: train loss = 0.0003, val loss = 0.0971\n",
      "Epoch 1979: train loss = 0.0003, val loss = 0.0960\n",
      "Epoch 1980: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 1981: train loss = 0.0002, val loss = 0.0968\n",
      "Epoch 1982: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 1983: train loss = 0.0002, val loss = 0.0977\n",
      "Epoch 1984: train loss = 0.0002, val loss = 0.0966\n",
      "Epoch 1985: train loss = 0.0002, val loss = 0.0977\n",
      "Epoch 1986: train loss = 0.0002, val loss = 0.0961\n",
      "Epoch 1987: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 1988: train loss = 0.0002, val loss = 0.0965\n",
      "Epoch 1989: train loss = 0.0001, val loss = 0.0953\n",
      "Epoch 1990: train loss = 0.0002, val loss = 0.0971\n",
      "Epoch 1991: train loss = 0.0002, val loss = 0.0961\n",
      "Epoch 1992: train loss = 0.0002, val loss = 0.0957\n",
      "Epoch 1993: train loss = 0.0001, val loss = 0.0974\n",
      "Epoch 1994: train loss = 0.0001, val loss = 0.0954\n",
      "Epoch 1995: train loss = 0.0001, val loss = 0.0982\n",
      "Epoch 1996: train loss = 0.0001, val loss = 0.0959\n",
      "Epoch 1997: train loss = 0.0001, val loss = 0.0973\n",
      "Epoch 1998: train loss = 0.0001, val loss = 0.0966\n",
      "Epoch 1999: train loss = 0.0001, val loss = 0.0967\n",
      "Epoch 2000: train loss = 0.0000, val loss = 0.0964\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0c8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mood_predictions(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a1aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  predicted_mood_class\n",
      "0   AS14.01                    30\n",
      "1   AS14.02                    25\n",
      "2   AS14.03                    24\n",
      "3   AS14.05                    29\n",
      "4   AS14.06                    33\n",
      "5   AS14.07                    26\n",
      "6   AS14.08                    26\n",
      "7   AS14.09                    29\n",
      "8   AS14.12                    26\n",
      "9   AS14.13                    31\n",
      "10  AS14.14                    26\n",
      "11  AS14.15                    29\n",
      "12  AS14.16                    29\n",
      "13  AS14.17                    28\n",
      "14  AS14.19                    31\n",
      "15  AS14.20                    26\n",
      "16  AS14.23                    32\n",
      "17  AS14.24                    27\n",
      "18  AS14.25                    23\n",
      "19  AS14.26                    27\n",
      "20  AS14.27                    31\n",
      "21  AS14.28                    27\n",
      "22  AS14.29                    24\n",
      "23  AS14.30                    29\n",
      "24  AS14.31                    32\n",
      "25  AS14.32                    27\n",
      "26  AS14.33                    26\n"
     ]
    }
   ],
   "source": [
    "# Run predictions on test_df\n",
    "test_predictions = predict(model, test_df, id_map, device)\n",
    "\n",
    "# Attach predictions to test_df\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted_mood_class'] = test_predictions\n",
    "\n",
    "# Optional: save to CSV or examine\n",
    "print(test_df_with_preds[['id', 'predicted_mood_class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f48181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>circumplex.arousal</th>\n",
       "      <th>circumplex.valence</th>\n",
       "      <th>activity</th>\n",
       "      <th>lagged_mood</th>\n",
       "      <th>appCat.builtin</th>\n",
       "      <th>appCat.communication</th>\n",
       "      <th>appCat.entertainment</th>\n",
       "      <th>appCat.finance</th>\n",
       "      <th>...</th>\n",
       "      <th>appCat.unknown</th>\n",
       "      <th>appCat.utilities</th>\n",
       "      <th>appCat.weather</th>\n",
       "      <th>call</th>\n",
       "      <th>mood</th>\n",
       "      <th>screen</th>\n",
       "      <th>sms</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>step</th>\n",
       "      <th>days_since_last_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AS14.01</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>-0.608451</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.122363</td>\n",
       "      <td>1.277717</td>\n",
       "      <td>-0.232045</td>\n",
       "      <td>-0.684146</td>\n",
       "      <td>-0.290839</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>1.557465</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-0.819495</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AS14.02</td>\n",
       "      <td>2014-04-25</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-0.846597</td>\n",
       "      <td>2.634312</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>0.472601</td>\n",
       "      <td>0.724486</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AS14.03</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>-0.862697</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.203546</td>\n",
       "      <td>0.938569</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>1.461196</td>\n",
       "      <td>1.340934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AS14.05</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>-0.354206</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.177764</td>\n",
       "      <td>-0.757174</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.001296</td>\n",
       "      <td>-0.475535</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.180507</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.234516</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AS14.06</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.203546</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-0.893175</td>\n",
       "      <td>-0.469772</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>1.461196</td>\n",
       "      <td>1.340934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AS14.07</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>-2.133924</td>\n",
       "      <td>-0.430561</td>\n",
       "      <td>-0.351702</td>\n",
       "      <td>-2.113768</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>-0.295573</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AS14.08</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.176589</td>\n",
       "      <td>-0.418025</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-0.999773</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.135937</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AS14.09</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>-0.430561</td>\n",
       "      <td>-0.062091</td>\n",
       "      <td>-1.435471</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AS14.12</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>0.154285</td>\n",
       "      <td>-0.821178</td>\n",
       "      <td>-0.871864</td>\n",
       "      <td>-1.435471</td>\n",
       "      <td>0.009194</td>\n",
       "      <td>-0.607143</td>\n",
       "      <td>-0.398595</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.092725</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.717215</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.177846</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AS14.13</td>\n",
       "      <td>2014-05-04</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>1.113571</td>\n",
       "      <td>1.277717</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>1.470812</td>\n",
       "      <td>1.234516</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AS14.14</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.136329</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>0.901677</td>\n",
       "      <td>-0.965774</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.149214</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AS14.15</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>0.917022</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.203546</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>1.461196</td>\n",
       "      <td>1.340934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AS14.16</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>2.442495</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.069690</td>\n",
       "      <td>0.599420</td>\n",
       "      <td>-0.336422</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>0.054612</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.993354</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>1.340934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AS14.17</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>1.679758</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>0.047178</td>\n",
       "      <td>0.599420</td>\n",
       "      <td>-0.325955</td>\n",
       "      <td>-0.646200</td>\n",
       "      <td>-0.466739</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>0.081588</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.756200</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AS14.19</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>0.535653</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-1.203546</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.011950</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-1.189414</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AS14.20</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>0.154285</td>\n",
       "      <td>-1.602412</td>\n",
       "      <td>-1.167603</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AS14.23</td>\n",
       "      <td>2014-05-04</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>1.052554</td>\n",
       "      <td>2.634312</td>\n",
       "      <td>-0.296778</td>\n",
       "      <td>-0.067734</td>\n",
       "      <td>-0.396059</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.624963</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>1.470812</td>\n",
       "      <td>1.234516</td>\n",
       "      <td>2.714409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AS14.24</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>-2.133924</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>1.384333</td>\n",
       "      <td>-0.757174</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>0.384467</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AS14.25</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>0.154285</td>\n",
       "      <td>-0.430561</td>\n",
       "      <td>-0.664405</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.129908</td>\n",
       "      <td>-0.576976</td>\n",
       "      <td>-0.375697</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>0.430165</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.583790</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>1.461196</td>\n",
       "      <td>1.340934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AS14.26</td>\n",
       "      <td>2014-05-30</td>\n",
       "      <td>-0.455904</td>\n",
       "      <td>-1.602412</td>\n",
       "      <td>-0.907789</td>\n",
       "      <td>-0.757174</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>0.472601</td>\n",
       "      <td>2.707934</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AS14.27</td>\n",
       "      <td>2014-05-15</td>\n",
       "      <td>2.442495</td>\n",
       "      <td>-0.430561</td>\n",
       "      <td>-0.289162</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>1.857885</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AS14.28</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>0.154285</td>\n",
       "      <td>-0.430561</td>\n",
       "      <td>-0.833284</td>\n",
       "      <td>-0.757174</td>\n",
       "      <td>0.342241</td>\n",
       "      <td>-0.545076</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.636707</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>1.461196</td>\n",
       "      <td>1.340934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AS14.29</td>\n",
       "      <td>2014-05-15</td>\n",
       "      <td>-0.608451</td>\n",
       "      <td>-1.602412</td>\n",
       "      <td>0.090394</td>\n",
       "      <td>-1.435471</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>1.857885</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AS14.30</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>-1.371188</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>0.967397</td>\n",
       "      <td>1.277717</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-1.523822</td>\n",
       "      <td>1.291186</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AS14.31</td>\n",
       "      <td>2014-05-17</td>\n",
       "      <td>0.917022</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>-0.926884</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.347052</td>\n",
       "      <td>-0.974839</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>0.571174</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.156300</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>1.971225</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AS14.32</td>\n",
       "      <td>2014-05-14</td>\n",
       "      <td>-0.760998</td>\n",
       "      <td>-1.133671</td>\n",
       "      <td>0.497766</td>\n",
       "      <td>0.260272</td>\n",
       "      <td>-0.291296</td>\n",
       "      <td>-0.864358</td>\n",
       "      <td>-0.413027</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.044734</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>-0.525610</td>\n",
       "      <td>1.801215</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AS14.33</td>\n",
       "      <td>2014-05-31</td>\n",
       "      <td>-2.896660</td>\n",
       "      <td>0.741290</td>\n",
       "      <td>0.793274</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>-1.014385</td>\n",
       "      <td>-0.478573</td>\n",
       "      <td>-0.222202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.245516</td>\n",
       "      <td>-0.199723</td>\n",
       "      <td>-0.680786</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.190682</td>\n",
       "      <td>-0.451866</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>2.764604</td>\n",
       "      <td>-0.032541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       date  circumplex.arousal  circumplex.valence  activity  \\\n",
       "0   AS14.01 2014-05-05           -0.608451            0.741290 -1.122363   \n",
       "1   AS14.02 2014-04-25           -1.371188            0.741290 -0.846597   \n",
       "2   AS14.03 2014-05-08           -0.862697            0.741290 -1.203546   \n",
       "3   AS14.05 2014-05-05           -0.354206            0.741290 -1.177764   \n",
       "4   AS14.06 2014-05-08           -1.371188            0.741290 -1.203546   \n",
       "5   AS14.07 2014-05-05           -2.133924           -0.430561 -0.351702   \n",
       "6   AS14.08 2014-05-05           -1.371188            0.741290 -1.176589   \n",
       "7   AS14.09 2014-05-05           -1.371188           -0.430561 -0.062091   \n",
       "8   AS14.12 2014-05-05            0.154285           -0.821178 -0.871864   \n",
       "9   AS14.13 2014-05-04           -1.371188            0.741290  1.113571   \n",
       "10  AS14.14 2014-05-05           -1.371188            0.741290 -1.136329   \n",
       "11  AS14.15 2014-05-08            0.917022            0.741290 -1.203546   \n",
       "12  AS14.16 2014-05-05            2.442495            0.741290 -1.069690   \n",
       "13  AS14.17 2014-05-05            1.679758            0.741290  0.047178   \n",
       "14  AS14.19 2014-05-05            0.535653            0.741290 -1.203546   \n",
       "15  AS14.20 2014-05-05            0.154285           -1.602412 -1.167603   \n",
       "16  AS14.23 2014-05-04           -1.371188            0.741290  1.052554   \n",
       "17  AS14.24 2014-06-09           -2.133924            0.741290  1.384333   \n",
       "18  AS14.25 2014-05-08            0.154285           -0.430561 -0.664405   \n",
       "19  AS14.26 2014-05-30           -0.455904           -1.602412 -0.907789   \n",
       "20  AS14.27 2014-05-15            2.442495           -0.430561 -0.289162   \n",
       "21  AS14.28 2014-05-08            0.154285           -0.430561 -0.833284   \n",
       "22  AS14.29 2014-05-15           -0.608451           -1.602412  0.090394   \n",
       "23  AS14.30 2014-05-05           -1.371188            0.741290  0.967397   \n",
       "24  AS14.31 2014-05-17            0.917022            0.741290 -0.926884   \n",
       "25  AS14.32 2014-05-14           -0.760998           -1.133671  0.497766   \n",
       "26  AS14.33 2014-05-31           -2.896660            0.741290  0.793274   \n",
       "\n",
       "    lagged_mood  appCat.builtin  appCat.communication  appCat.entertainment  \\\n",
       "0      1.277717       -0.232045             -0.684146             -0.290839   \n",
       "1      2.634312       -0.348711             -1.014385             -0.478573   \n",
       "2      0.938569       -0.348711             -1.014385             -0.478573   \n",
       "3     -0.757174       -0.348711             -1.001296             -0.475535   \n",
       "4     -0.078877       -0.348711             -0.893175             -0.469772   \n",
       "5     -2.113768       -0.348711             -1.014385             -0.478573   \n",
       "6     -0.418025       -0.348711             -0.999773             -0.478573   \n",
       "7     -1.435471       -0.348711             -1.014385             -0.478573   \n",
       "8     -1.435471        0.009194             -0.607143             -0.398595   \n",
       "9      1.277717       -0.348711             -1.014385             -0.478573   \n",
       "10    -0.078877        0.901677             -0.965774             -0.478573   \n",
       "11    -0.078877       -0.348711             -1.014385             -0.478573   \n",
       "12     0.599420       -0.336422             -1.014385              0.054612   \n",
       "13     0.599420       -0.325955             -0.646200             -0.466739   \n",
       "14    -0.078877       -0.348711             -1.011950             -0.478573   \n",
       "15    -0.078877       -0.348711             -1.014385             -0.478573   \n",
       "16     2.634312       -0.296778             -0.067734             -0.396059   \n",
       "17    -0.757174       -0.348711             -1.014385             -0.478573   \n",
       "18    -0.078877       -0.129908             -0.576976             -0.375697   \n",
       "19    -0.757174       -0.348711             -1.014385             -0.478573   \n",
       "20    -0.078877       -0.348711             -1.014385             -0.478573   \n",
       "21    -0.757174        0.342241             -0.545076             -0.478573   \n",
       "22    -1.435471       -0.348711             -1.014385             -0.478573   \n",
       "23     1.277717       -0.348711             -1.014385             -0.478573   \n",
       "24    -0.078877       -0.347052             -0.974839             -0.478573   \n",
       "25     0.260272       -0.291296             -0.864358             -0.413027   \n",
       "26    -0.078877       -0.348711             -1.014385             -0.478573   \n",
       "\n",
       "    appCat.finance  ...  appCat.unknown  appCat.utilities  appCat.weather  \\\n",
       "0        -0.222202  ...       -0.227556          1.557465       -0.199723   \n",
       "1        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "2        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "3        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "4        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "5        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "6        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "7        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "8        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "9        -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "10       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "11       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "12       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "13       -0.222202  ...       -0.227556          0.081588       -0.199723   \n",
       "14       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "15       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "16       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "17       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "18       -0.222202  ...       -0.227556         -0.245516        0.430165   \n",
       "19       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "20       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "21       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "22       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "23       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "24       -0.222202  ...       -0.227556          0.571174       -0.199723   \n",
       "25       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "26       -0.222202  ...       -0.227556         -0.245516       -0.199723   \n",
       "\n",
       "        call  mood    screen       sms  day_of_week      step  \\\n",
       "0  -0.680786  32.0 -0.819495 -0.451866    -1.523822  1.291186   \n",
       "1  -0.680786  28.0 -1.190682 -0.451866     0.472601  0.724486   \n",
       "2  -0.680786  31.0 -1.190682 -0.451866    -0.026505  1.461196   \n",
       "3  -0.680786  28.0 -1.180507 -0.451866    -1.523822  1.234516   \n",
       "4  -0.680786  27.0 -1.190682 -0.451866    -0.026505  1.461196   \n",
       "5  -0.680786  32.0 -1.190682 -0.451866    -1.523822 -0.295573   \n",
       "6  -0.680786  28.0 -1.135937 -0.451866    -1.523822  1.291186   \n",
       "7  -0.680786  27.0 -1.190682 -0.451866    -1.523822  1.291186   \n",
       "8  -0.092725  20.0 -0.717215 -0.451866    -1.523822  1.177846   \n",
       "9  -0.680786  29.0 -1.190682 -0.451866     1.470812  1.234516   \n",
       "10 -0.680786  28.0 -1.149214 -0.451866    -1.523822  1.291186   \n",
       "11 -0.680786  28.0 -1.190682 -0.451866    -0.026505  1.461196   \n",
       "12 -0.680786  28.0 -0.993354 -0.451866    -1.523822  1.291186   \n",
       "13 -0.680786   NaN -0.756200 -0.451866    -1.523822  1.291186   \n",
       "14 -0.680786  27.0 -1.189414 -0.451866    -1.523822  1.291186   \n",
       "15 -0.680786  30.0 -1.190682 -0.451866    -1.523822  1.291186   \n",
       "16 -0.680786   NaN -0.624963 -0.451866     1.470812  1.234516   \n",
       "17 -0.680786  26.0 -1.190682 -0.451866    -1.523822  0.384467   \n",
       "18 -0.680786  28.0 -0.583790 -0.451866    -0.026505  1.461196   \n",
       "19 -0.680786  30.0 -1.190682 -0.451866     0.472601  2.707934   \n",
       "20 -0.680786  30.0 -1.190682 -0.451866    -0.026505  1.857885   \n",
       "21 -0.680786  26.0 -0.636707 -0.451866    -0.026505  1.461196   \n",
       "22 -0.680786  31.0 -1.190682 -0.451866    -0.026505  1.857885   \n",
       "23 -0.680786  32.0 -1.190682 -0.451866    -1.523822  1.291186   \n",
       "24 -0.680786   NaN -1.156300 -0.451866     0.971706  1.971225   \n",
       "25 -0.680786   NaN -1.044734 -0.451866    -0.525610  1.801215   \n",
       "26 -0.680786  28.0 -1.190682 -0.451866     0.971706  2.764604   \n",
       "\n",
       "    days_since_last_obs  \n",
       "0             -0.032541  \n",
       "1             -0.032541  \n",
       "2              1.340934  \n",
       "3             -0.032541  \n",
       "4              1.340934  \n",
       "5             -0.032541  \n",
       "6             -0.032541  \n",
       "7             -0.032541  \n",
       "8             -0.032541  \n",
       "9             -0.032541  \n",
       "10            -0.032541  \n",
       "11             1.340934  \n",
       "12             1.340934  \n",
       "13            -0.032541  \n",
       "14            -0.032541  \n",
       "15            -0.032541  \n",
       "16             2.714409  \n",
       "17            -0.032541  \n",
       "18             1.340934  \n",
       "19            -0.032541  \n",
       "20            -0.032541  \n",
       "21             1.340934  \n",
       "22            -0.032541  \n",
       "23            -0.032541  \n",
       "24            -0.032541  \n",
       "25            -0.032541  \n",
       "26            -0.032541  \n",
       "\n",
       "[27 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3933caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "        31, 32, 33, 34, 35, 36, 38], dtype=int32),\n",
       " mood\n",
       " 28    301\n",
       " 29    138\n",
       " 32    128\n",
       " 30    124\n",
       " 26    103\n",
       " 27     98\n",
       " 31     98\n",
       " 24     66\n",
       " 25     65\n",
       " 33     21\n",
       " 22     17\n",
       " 23     17\n",
       " 34     13\n",
       " 20      7\n",
       " 35      6\n",
       " 21      5\n",
       " 36      4\n",
       " 14      3\n",
       " 18      3\n",
       " 19      2\n",
       " 15      2\n",
       " 12      1\n",
       " 16      1\n",
       " 38      1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sort_values('mood')\n",
    "train_df['mood'].unique(), train_df['mood'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
