{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60ca131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added the path (/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1) to sys.path\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ./initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf5c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rik/Documents/VU/DMT/DataMiningTechniquesA1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_loading import DataPreprocessor\n",
    "from mood_RNN_classifier import RNNClassifier, MoodDataset, OrdinalLabelSmoothingLoss, objective, train_epoch, train_final_model, evaluate, predict, plot_mood_predictions\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c42a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 24 outliers from 1002 observations. Percentage: 2.40%\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataPreprocessor()\n",
    "train_df_split, val_df_split, test_df = data_loader.load_and_preprocess_data(\"1d\", 0.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6928e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume train_df and test_df are loaded and preprocessed\n",
    "id_map = {id_: idx for idx, id_ in enumerate(train_df_split['id'].unique())}\n",
    "input_dim = train_df_split.drop(columns=['id', 'mood', 'date']).shape[1]\n",
    "id_count = len(id_map)\n",
    "output_dim = train_df_split['mood'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c89292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-19 18:56:48,122] A new study created in memory with name: no-name-2427b8a8-c6af-4429-9c77-8d56432f230e\n",
      "[I 2025-04-19 18:56:53,042] Trial 0 finished with value: 0.5048529823919884 and parameters: {'hidden_dim': 93, 'id_embed_dim': 5, 'lr': 0.00028867285835867815, 'batch_size': 64, 'alpha': 0.13017570799659403}. Best is trial 0 with value: 0.5048529823919884.\n",
      "[I 2025-04-19 18:56:53,296] Trial 1 finished with value: 0.4159012647499715 and parameters: {'hidden_dim': 65, 'id_embed_dim': 10, 'lr': 0.006649848901327181, 'batch_size': 32, 'alpha': 0.2673661350003736}. Best is trial 1 with value: 0.4159012647499715.\n",
      "[I 2025-04-19 18:56:53,426] Trial 2 finished with value: 0.19989992095563644 and parameters: {'hidden_dim': 76, 'id_embed_dim': 5, 'lr': 0.0005573336571753028, 'batch_size': 128, 'alpha': 0.03022474133374591}. Best is trial 2 with value: 0.19989992095563644.\n",
      "[I 2025-04-19 18:56:53,724] Trial 3 finished with value: 0.263832452153801 and parameters: {'hidden_dim': 125, 'id_embed_dim': 8, 'lr': 0.00017868240649221776, 'batch_size': 32, 'alpha': 0.07020080725186584}. Best is trial 2 with value: 0.19989992095563644.\n",
      "[I 2025-04-19 18:56:53,906] Trial 4 finished with value: 0.025194344719997923 and parameters: {'hidden_dim': 49, 'id_embed_dim': 6, 'lr': 0.0012598884585626662, 'batch_size': 64, 'alpha': 0.01642617483089343}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:54,089] Trial 5 finished with value: 0.15323681300295924 and parameters: {'hidden_dim': 36, 'id_embed_dim': 12, 'lr': 0.0014247135524921108, 'batch_size': 64, 'alpha': 0.08316296671577991}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:54,261] Trial 6 finished with value: 0.155755505638015 and parameters: {'hidden_dim': 106, 'id_embed_dim': 9, 'lr': 0.0020827699599611283, 'batch_size': 128, 'alpha': 0.09368599160208874}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:54,427] Trial 7 finished with value: 0.3732596734412631 and parameters: {'hidden_dim': 93, 'id_embed_dim': 7, 'lr': 0.0037081147210427036, 'batch_size': 128, 'alpha': 0.2598023532480123}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:54,634] Trial 8 finished with value: 0.09930502774572014 and parameters: {'hidden_dim': 117, 'id_embed_dim': 14, 'lr': 0.006856105712959703, 'batch_size': 64, 'alpha': 0.04721340393733774}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:54,828] Trial 9 finished with value: 0.05344768406305098 and parameters: {'hidden_dim': 111, 'id_embed_dim': 15, 'lr': 0.0020324367047623447, 'batch_size': 64, 'alpha': 0.035957013617759784}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:54,995] Trial 10 finished with value: 0.532070179182784 and parameters: {'hidden_dim': 39, 'id_embed_dim': 6, 'lr': 0.0006180730263002975, 'batch_size': 64, 'alpha': 0.18816742770997158}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:55,189] Trial 11 finished with value: 0.25266427451506596 and parameters: {'hidden_dim': 54, 'id_embed_dim': 16, 'lr': 0.002254051729896639, 'batch_size': 64, 'alpha': 0.1752850267000451}. Best is trial 4 with value: 0.025194344719997923.\n",
      "[I 2025-04-19 18:56:55,369] Trial 12 finished with value: 0.019691712314024903 and parameters: {'hidden_dim': 55, 'id_embed_dim': 12, 'lr': 0.0008702310731029855, 'batch_size': 64, 'alpha': 0.013232385718627088}. Best is trial 12 with value: 0.019691712314024903.\n",
      "[I 2025-04-19 18:56:55,542] Trial 13 finished with value: 0.03249871557144294 and parameters: {'hidden_dim': 52, 'id_embed_dim': 12, 'lr': 0.0008518504681790495, 'batch_size': 64, 'alpha': 0.01863803808946915}. Best is trial 12 with value: 0.019691712314024903.\n",
      "[I 2025-04-19 18:56:55,739] Trial 14 finished with value: 0.5821978015110905 and parameters: {'hidden_dim': 55, 'id_embed_dim': 12, 'lr': 0.0003208097165868625, 'batch_size': 64, 'alpha': 0.12280117910215275}. Best is trial 12 with value: 0.019691712314024903.\n",
      "[I 2025-04-19 18:56:56,043] Trial 15 finished with value: 0.956723463714571 and parameters: {'hidden_dim': 72, 'id_embed_dim': 4, 'lr': 0.00011238659883999077, 'batch_size': 32, 'alpha': 0.21135943724292522}. Best is trial 12 with value: 0.019691712314024903.\n",
      "[I 2025-04-19 18:56:56,224] Trial 16 finished with value: 0.017720218439747516 and parameters: {'hidden_dim': 43, 'id_embed_dim': 11, 'lr': 0.0010981207383012007, 'batch_size': 64, 'alpha': 0.011928750302972739}. Best is trial 16 with value: 0.017720218439747516.\n",
      "[I 2025-04-19 18:56:56,416] Trial 17 finished with value: 0.20307542103573792 and parameters: {'hidden_dim': 65, 'id_embed_dim': 13, 'lr': 0.0004872118075648782, 'batch_size': 64, 'alpha': 0.0611316526466026}. Best is trial 16 with value: 0.017720218439747516.\n",
      "[I 2025-04-19 18:56:56,549] Trial 18 finished with value: 0.19375749615798318 and parameters: {'hidden_dim': 43, 'id_embed_dim': 10, 'lr': 0.0036864794826125887, 'batch_size': 128, 'alpha': 0.11505752647961195}. Best is trial 16 with value: 0.017720218439747516.\n",
      "[I 2025-04-19 18:56:56,787] Trial 19 finished with value: 0.015849441878105466 and parameters: {'hidden_dim': 32, 'id_embed_dim': 11, 'lr': 0.0007668611342898348, 'batch_size': 32, 'alpha': 0.011526854342424944}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:57,021] Trial 20 finished with value: 0.4448808521256411 and parameters: {'hidden_dim': 32, 'id_embed_dim': 10, 'lr': 0.0003578701758553896, 'batch_size': 32, 'alpha': 0.1524266565648748}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:57,268] Trial 21 finished with value: 0.017490635885107787 and parameters: {'hidden_dim': 41, 'id_embed_dim': 11, 'lr': 0.000776742054159151, 'batch_size': 32, 'alpha': 0.013273722026483147}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:57,513] Trial 22 finished with value: 0.08984602573222684 and parameters: {'hidden_dim': 44, 'id_embed_dim': 11, 'lr': 0.0010024097622224162, 'batch_size': 32, 'alpha': 0.05116361246962889}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:57,749] Trial 23 finished with value: 0.2005734298014103 and parameters: {'hidden_dim': 32, 'id_embed_dim': 9, 'lr': 0.000646218050802833, 'batch_size': 32, 'alpha': 0.091960061224893}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:57,989] Trial 24 finished with value: 0.38636130453052375 and parameters: {'hidden_dim': 45, 'id_embed_dim': 11, 'lr': 0.001451957528453578, 'batch_size': 32, 'alpha': 0.29692198757876287}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:58,250] Trial 25 finished with value: 0.10776368892730627 and parameters: {'hidden_dim': 63, 'id_embed_dim': 14, 'lr': 0.00042671644711242526, 'batch_size': 32, 'alpha': 0.048050888174297965}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:58,491] Trial 26 finished with value: 0.08598165443741289 and parameters: {'hidden_dim': 40, 'id_embed_dim': 9, 'lr': 0.00017889677779217849, 'batch_size': 32, 'alpha': 0.01200589998759284}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:58,784] Trial 27 finished with value: 0.12204243287556153 and parameters: {'hidden_dim': 87, 'id_embed_dim': 11, 'lr': 0.002854066315942261, 'batch_size': 32, 'alpha': 0.07468970133866375}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:59,040] Trial 28 finished with value: 0.05974781994980977 and parameters: {'hidden_dim': 60, 'id_embed_dim': 13, 'lr': 0.0010601348488869147, 'batch_size': 32, 'alpha': 0.03816638501139431}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:59,282] Trial 29 finished with value: 0.23501574970725783 and parameters: {'hidden_dim': 47, 'id_embed_dim': 8, 'lr': 0.0007612925766639728, 'batch_size': 32, 'alpha': 0.14239671407773558}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:59,427] Trial 30 finished with value: 0.7200522203194467 and parameters: {'hidden_dim': 84, 'id_embed_dim': 13, 'lr': 0.00024185806833961716, 'batch_size': 128, 'alpha': 0.10819394219359471}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:59,590] Trial 31 finished with value: 0.023528451001957842 and parameters: {'hidden_dim': 37, 'id_embed_dim': 11, 'lr': 0.0008297069025071785, 'batch_size': 64, 'alpha': 0.012337937491175751}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:59,784] Trial 32 finished with value: 0.049129267440254525 and parameters: {'hidden_dim': 57, 'id_embed_dim': 12, 'lr': 0.0015578054742644, 'batch_size': 64, 'alpha': 0.029897528224700497}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:56:59,968] Trial 33 finished with value: 0.10388370508089997 and parameters: {'hidden_dim': 69, 'id_embed_dim': 10, 'lr': 0.0004744589328896066, 'batch_size': 64, 'alpha': 0.028560027926091594}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:57:00,210] Trial 34 finished with value: 0.1150677991764886 and parameters: {'hidden_dim': 50, 'id_embed_dim': 14, 'lr': 0.0006763715479047374, 'batch_size': 32, 'alpha': 0.061280884381485365}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:57:00,371] Trial 35 finished with value: 0.06350995443369213 and parameters: {'hidden_dim': 32, 'id_embed_dim': 11, 'lr': 0.001109810497800288, 'batch_size': 64, 'alpha': 0.02872408797003046}. Best is trial 19 with value: 0.015849441878105466.\n",
      "[I 2025-04-19 18:57:00,610] Trial 36 finished with value: 0.01263453021533507 and parameters: {'hidden_dim': 42, 'id_embed_dim': 8, 'lr': 0.0017045302706380115, 'batch_size': 32, 'alpha': 0.010994952380849627}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:00,844] Trial 37 finished with value: 0.09141074165813905 and parameters: {'hidden_dim': 40, 'id_embed_dim': 8, 'lr': 0.0017880471731409474, 'batch_size': 32, 'alpha': 0.055798391549699754}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:01,113] Trial 38 finished with value: 0.14640764421538302 and parameters: {'hidden_dim': 77, 'id_embed_dim': 7, 'lr': 0.009527250593448321, 'batch_size': 32, 'alpha': 0.06822399250195191}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:01,355] Trial 39 finished with value: 0.14366206280270913 and parameters: {'hidden_dim': 47, 'id_embed_dim': 9, 'lr': 0.0027393391756101193, 'batch_size': 32, 'alpha': 0.08823106050802672}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:01,487] Trial 40 finished with value: 0.17063433279220322 and parameters: {'hidden_dim': 36, 'id_embed_dim': 7, 'lr': 0.0012584078582857678, 'batch_size': 128, 'alpha': 0.041817469993414055}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:01,725] Trial 41 finished with value: 0.01942064248977747 and parameters: {'hidden_dim': 42, 'id_embed_dim': 10, 'lr': 0.0010383862018492008, 'batch_size': 32, 'alpha': 0.014228286578979297}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:02,015] Trial 42 finished with value: 0.03785899824889979 and parameters: {'hidden_dim': 42, 'id_embed_dim': 10, 'lr': 0.001618293118848079, 'batch_size': 32, 'alpha': 0.02613940007192831}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:02,247] Trial 43 finished with value: 0.06856991081757653 and parameters: {'hidden_dim': 36, 'id_embed_dim': 8, 'lr': 0.0012310261557780956, 'batch_size': 32, 'alpha': 0.04115813906432937}. Best is trial 36 with value: 0.01263453021533507.\n",
      "[I 2025-04-19 18:57:02,487] Trial 44 finished with value: 0.011996831086681302 and parameters: {'hidden_dim': 50, 'id_embed_dim': 10, 'lr': 0.0005529516326445658, 'batch_size': 32, 'alpha': 0.010086289645709923}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:02,727] Trial 45 finished with value: 0.049341175116990744 and parameters: {'hidden_dim': 50, 'id_embed_dim': 9, 'lr': 0.0005544557495665363, 'batch_size': 32, 'alpha': 0.02528041815231326}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:03,020] Trial 46 finished with value: 0.4244172384864406 and parameters: {'hidden_dim': 100, 'id_embed_dim': 6, 'lr': 0.000397007025281535, 'batch_size': 32, 'alpha': 0.2442903227940515}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:03,265] Trial 47 finished with value: 0.13424783631374962 and parameters: {'hidden_dim': 59, 'id_embed_dim': 11, 'lr': 0.0005561104543485283, 'batch_size': 32, 'alpha': 0.0746688737625717}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:03,392] Trial 48 finished with value: 0.03689721201483468 and parameters: {'hidden_dim': 52, 'id_embed_dim': 12, 'lr': 0.0007733625001876296, 'batch_size': 128, 'alpha': 0.010712277793616632}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:03,625] Trial 49 finished with value: 0.16734610117019566 and parameters: {'hidden_dim': 36, 'id_embed_dim': 9, 'lr': 0.00029140436190332316, 'batch_size': 32, 'alpha': 0.03881410512984642}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:03,866] Trial 50 finished with value: 0.16303251538061558 and parameters: {'hidden_dim': 48, 'id_embed_dim': 10, 'lr': 0.002578714621852092, 'batch_size': 32, 'alpha': 0.10352174393258604}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:04,104] Trial 51 finished with value: 0.03778193963873655 and parameters: {'hidden_dim': 42, 'id_embed_dim': 10, 'lr': 0.0009540682771765578, 'batch_size': 32, 'alpha': 0.023404651011683597}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:04,339] Trial 52 finished with value: 0.012990530748340421 and parameters: {'hidden_dim': 40, 'id_embed_dim': 12, 'lr': 0.0019191686766523133, 'batch_size': 32, 'alpha': 0.010727873118118018}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:04,570] Trial 53 finished with value: 0.09275183753859728 and parameters: {'hidden_dim': 35, 'id_embed_dim': 13, 'lr': 0.004339451381719791, 'batch_size': 32, 'alpha': 0.053275333215483396}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:04,856] Trial 54 finished with value: 0.031566268295273744 and parameters: {'hidden_dim': 127, 'id_embed_dim': 12, 'lr': 0.0021631599079889614, 'batch_size': 32, 'alpha': 0.02087444882709534}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:05,093] Trial 55 finished with value: 0.049725290135781565 and parameters: {'hidden_dim': 39, 'id_embed_dim': 11, 'lr': 0.0018352060990726963, 'batch_size': 32, 'alpha': 0.0330055088509239}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:05,265] Trial 56 finished with value: 0.07235143608168552 and parameters: {'hidden_dim': 46, 'id_embed_dim': 12, 'lr': 0.003403700971309473, 'batch_size': 64, 'alpha': 0.04651414645041964}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:05,389] Trial 57 finished with value: 0.6129912469620095 and parameters: {'hidden_dim': 32, 'id_embed_dim': 5, 'lr': 0.00128511396194884, 'batch_size': 128, 'alpha': 0.19832315232114694}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:05,629] Trial 58 finished with value: 0.012570840775742567 and parameters: {'hidden_dim': 44, 'id_embed_dim': 13, 'lr': 0.0006707839186178263, 'batch_size': 32, 'alpha': 0.010414142350866604}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:05,928] Trial 59 finished with value: 0.03146737382600182 and parameters: {'hidden_dim': 119, 'id_embed_dim': 14, 'lr': 0.0006743913795151333, 'batch_size': 32, 'alpha': 0.021236536220860743}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:06,171] Trial 60 finished with value: 0.10709276509688313 and parameters: {'hidden_dim': 52, 'id_embed_dim': 16, 'lr': 0.004805488229676769, 'batch_size': 32, 'alpha': 0.061230269579113095}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:06,409] Trial 61 finished with value: 0.024197335277957126 and parameters: {'hidden_dim': 44, 'id_embed_dim': 13, 'lr': 0.0008858908250816531, 'batch_size': 32, 'alpha': 0.01805582648388667}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:06,646] Trial 62 finished with value: 0.016999170506247004 and parameters: {'hidden_dim': 39, 'id_embed_dim': 15, 'lr': 0.0005045520961839468, 'batch_size': 32, 'alpha': 0.010605018273975659}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:06,883] Trial 63 finished with value: 0.08770920241013505 and parameters: {'hidden_dim': 39, 'id_embed_dim': 13, 'lr': 0.0005197824739334207, 'batch_size': 32, 'alpha': 0.035196596734245265}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:07,114] Trial 64 finished with value: 0.026585035754325696 and parameters: {'hidden_dim': 34, 'id_embed_dim': 15, 'lr': 0.00035240299681567946, 'batch_size': 32, 'alpha': 0.011129746788888718}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:07,349] Trial 65 finished with value: 0.11105689182317346 and parameters: {'hidden_dim': 38, 'id_embed_dim': 4, 'lr': 0.00041539457846625133, 'batch_size': 32, 'alpha': 0.03398896274328442}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:07,588] Trial 66 finished with value: 0.03504055839284022 and parameters: {'hidden_dim': 45, 'id_embed_dim': 15, 'lr': 0.0006631520522825033, 'batch_size': 32, 'alpha': 0.020499840628906918}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:07,840] Trial 67 finished with value: 0.08594285455861486 and parameters: {'hidden_dim': 56, 'id_embed_dim': 15, 'lr': 0.0007318806287407889, 'batch_size': 32, 'alpha': 0.048828250249651706}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:08,079] Trial 68 finished with value: 0.3291447722822204 and parameters: {'hidden_dim': 50, 'id_embed_dim': 16, 'lr': 0.000478672352091378, 'batch_size': 32, 'alpha': 0.17313544434834988}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:08,314] Trial 69 finished with value: 0.016038026940777786 and parameters: {'hidden_dim': 40, 'id_embed_dim': 11, 'lr': 0.000612572428037595, 'batch_size': 32, 'alpha': 0.01091620564899}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:08,577] Trial 70 finished with value: 0.05383260408066269 and parameters: {'hidden_dim': 62, 'id_embed_dim': 14, 'lr': 0.0006321344795118169, 'batch_size': 32, 'alpha': 0.03159691270150776}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:08,811] Trial 71 finished with value: 0.03764204510060468 and parameters: {'hidden_dim': 41, 'id_embed_dim': 11, 'lr': 0.0005878556333133854, 'batch_size': 32, 'alpha': 0.019172605504828688}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:09,044] Trial 72 finished with value: 0.03373584403355319 and parameters: {'hidden_dim': 38, 'id_embed_dim': 12, 'lr': 0.00026118809630599246, 'batch_size': 32, 'alpha': 0.010427341111642948}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:09,274] Trial 73 finished with value: 0.07618282640114762 and parameters: {'hidden_dim': 34, 'id_embed_dim': 11, 'lr': 0.0009294143794421632, 'batch_size': 32, 'alpha': 0.04196356448388253}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:09,511] Trial 74 finished with value: 0.08197510074403949 and parameters: {'hidden_dim': 48, 'id_embed_dim': 12, 'lr': 0.00036177821183869767, 'batch_size': 32, 'alpha': 0.02811523545420451}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:09,750] Trial 75 finished with value: 0.02636700764970672 and parameters: {'hidden_dim': 44, 'id_embed_dim': 12, 'lr': 0.0007758472952032008, 'batch_size': 32, 'alpha': 0.0182651022533368}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:09,982] Trial 76 finished with value: 0.056255746660824106 and parameters: {'hidden_dim': 40, 'id_embed_dim': 13, 'lr': 0.0004595876476666704, 'batch_size': 32, 'alpha': 0.025527043011776072}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:10,229] Trial 77 finished with value: 0.03067659558658313 and parameters: {'hidden_dim': 54, 'id_embed_dim': 9, 'lr': 0.00021274904935602895, 'batch_size': 32, 'alpha': 0.01071689625310338}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:10,358] Trial 78 finished with value: 0.1292447118382705 and parameters: {'hidden_dim': 46, 'id_embed_dim': 10, 'lr': 0.001168994988967776, 'batch_size': 128, 'alpha': 0.0430185757592246}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:10,589] Trial 79 finished with value: 0.21775545829668977 and parameters: {'hidden_dim': 34, 'id_embed_dim': 8, 'lr': 0.00242357663953111, 'batch_size': 32, 'alpha': 0.129412092495859}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:10,825] Trial 80 finished with value: 0.39880470449763133 and parameters: {'hidden_dim': 42, 'id_embed_dim': 11, 'lr': 0.0005237391807113552, 'batch_size': 32, 'alpha': 0.2269126417594307}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:10,988] Trial 81 finished with value: 0.025529945451290087 and parameters: {'hidden_dim': 37, 'id_embed_dim': 11, 'lr': 0.001487891383896211, 'batch_size': 64, 'alpha': 0.017827404248950927}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:11,256] Trial 82 finished with value: 0.052989679507743144 and parameters: {'hidden_dim': 68, 'id_embed_dim': 11, 'lr': 0.001740517633832262, 'batch_size': 32, 'alpha': 0.03495298861543724}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:11,431] Trial 83 finished with value: 0.046106903792772075 and parameters: {'hidden_dim': 52, 'id_embed_dim': 10, 'lr': 0.0013844117058465738, 'batch_size': 64, 'alpha': 0.026079215161301482}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:11,605] Trial 84 finished with value: 0.02994301332566971 and parameters: {'hidden_dim': 43, 'id_embed_dim': 10, 'lr': 0.0010308074113054844, 'batch_size': 64, 'alpha': 0.017074132797185514}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:11,776] Trial 85 finished with value: 0.08790634314816698 and parameters: {'hidden_dim': 49, 'id_embed_dim': 12, 'lr': 0.0019457556139114648, 'batch_size': 64, 'alpha': 0.055102901790117155}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:12,005] Trial 86 finished with value: 0.016966269778689945 and parameters: {'hidden_dim': 32, 'id_embed_dim': 14, 'lr': 0.0008544130093374493, 'batch_size': 32, 'alpha': 0.011792160185608254}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:12,235] Trial 87 finished with value: 0.01703315572743129 and parameters: {'hidden_dim': 32, 'id_embed_dim': 14, 'lr': 0.0007182105511238394, 'batch_size': 32, 'alpha': 0.010395961968339787}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:12,464] Trial 88 finished with value: 0.04509180773021583 and parameters: {'hidden_dim': 32, 'id_embed_dim': 14, 'lr': 0.0008335798413772817, 'batch_size': 32, 'alpha': 0.026018508571240932}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:12,696] Trial 89 finished with value: 0.4548992877616022 and parameters: {'hidden_dim': 35, 'id_embed_dim': 15, 'lr': 0.0006156926248773275, 'batch_size': 32, 'alpha': 0.2952446777172888}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:12,932] Trial 90 finished with value: 0.014808101705590585 and parameters: {'hidden_dim': 38, 'id_embed_dim': 13, 'lr': 0.00042911840599328574, 'batch_size': 32, 'alpha': 0.010099576349524655}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:13,168] Trial 91 finished with value: 0.09801798509924035 and parameters: {'hidden_dim': 38, 'id_embed_dim': 14, 'lr': 0.00011113763353696532, 'batch_size': 32, 'alpha': 0.010826034288523077}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:13,398] Trial 92 finished with value: 0.04988719102807511 and parameters: {'hidden_dim': 32, 'id_embed_dim': 13, 'lr': 0.00042991527307536983, 'batch_size': 32, 'alpha': 0.018670768619250926}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:13,640] Trial 93 finished with value: 0.12434074708393641 and parameters: {'hidden_dim': 40, 'id_embed_dim': 13, 'lr': 0.0003216884563622755, 'batch_size': 32, 'alpha': 0.03275962286307889}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:13,871] Trial 94 finished with value: 0.04264324785847413 and parameters: {'hidden_dim': 36, 'id_embed_dim': 14, 'lr': 0.0007150780016651557, 'batch_size': 32, 'alpha': 0.023372014006295832}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:14,102] Trial 95 finished with value: 0.09228841411439996 and parameters: {'hidden_dim': 33, 'id_embed_dim': 15, 'lr': 0.0005717006985231958, 'batch_size': 32, 'alpha': 0.03823013393201859}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:14,336] Trial 96 finished with value: 0.015045542213925742 and parameters: {'hidden_dim': 37, 'id_embed_dim': 13, 'lr': 0.0005343046427366244, 'batch_size': 32, 'alpha': 0.010372734203504822}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:14,571] Trial 97 finished with value: 0.09977044064299505 and parameters: {'hidden_dim': 37, 'id_embed_dim': 13, 'lr': 0.00038413210950675656, 'batch_size': 32, 'alpha': 0.029177088259336625}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:14,719] Trial 98 finished with value: 0.11467150139405315 and parameters: {'hidden_dim': 94, 'id_embed_dim': 7, 'lr': 0.0005005693528509745, 'batch_size': 128, 'alpha': 0.01733964274727456}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:14,959] Trial 99 finished with value: 0.0370738474386079 and parameters: {'hidden_dim': 45, 'id_embed_dim': 13, 'lr': 0.0008595442001538596, 'batch_size': 32, 'alpha': 0.024226889316114865}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:15,197] Trial 100 finished with value: 0.11388884809680451 and parameters: {'hidden_dim': 41, 'id_embed_dim': 12, 'lr': 0.00046533889991412237, 'batch_size': 32, 'alpha': 0.0442509618448263}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:15,429] Trial 101 finished with value: 0.014681540037456312 and parameters: {'hidden_dim': 35, 'id_embed_dim': 14, 'lr': 0.0006843159932908842, 'batch_size': 32, 'alpha': 0.010846455686805407}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:15,661] Trial 102 finished with value: 0.029880505335286148 and parameters: {'hidden_dim': 35, 'id_embed_dim': 14, 'lr': 0.0005939138212119047, 'batch_size': 32, 'alpha': 0.016216996538770644}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:15,896] Trial 103 finished with value: 0.03185319553192397 and parameters: {'hidden_dim': 39, 'id_embed_dim': 14, 'lr': 0.0005323197199400331, 'batch_size': 32, 'alpha': 0.015457093817303305}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:16,132] Trial 104 finished with value: 0.06149633766565108 and parameters: {'hidden_dim': 43, 'id_embed_dim': 13, 'lr': 0.0006451572127697904, 'batch_size': 32, 'alpha': 0.03188243232683201}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:16,367] Trial 105 finished with value: 0.035636695732075466 and parameters: {'hidden_dim': 38, 'id_embed_dim': 15, 'lr': 0.0009628131012376499, 'batch_size': 32, 'alpha': 0.022906796623823417}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:16,605] Trial 106 finished with value: 0.012080691544745201 and parameters: {'hidden_dim': 47, 'id_embed_dim': 13, 'lr': 0.000792958376957564, 'batch_size': 32, 'alpha': 0.010123121354440132}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:16,845] Trial 107 finished with value: 0.06537740610371855 and parameters: {'hidden_dim': 47, 'id_embed_dim': 13, 'lr': 0.0008060336582913465, 'batch_size': 32, 'alpha': 0.038475281931877874}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:17,129] Trial 108 finished with value: 0.02962686919740268 and parameters: {'hidden_dim': 81, 'id_embed_dim': 12, 'lr': 0.001111386828037896, 'batch_size': 32, 'alpha': 0.022190051935351004}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:17,366] Trial 109 finished with value: 0.05475487677674545 and parameters: {'hidden_dim': 34, 'id_embed_dim': 13, 'lr': 0.0007227781120012979, 'batch_size': 32, 'alpha': 0.029512715377822307}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:17,672] Trial 110 finished with value: 0.10620019960224181 and parameters: {'hidden_dim': 41, 'id_embed_dim': 12, 'lr': 0.002941296896663173, 'batch_size': 32, 'alpha': 0.06755535412049612}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:17,989] Trial 111 finished with value: 0.035419005312417685 and parameters: {'hidden_dim': 36, 'id_embed_dim': 14, 'lr': 0.00044237389295010966, 'batch_size': 32, 'alpha': 0.014708487097271725}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:18,249] Trial 112 finished with value: 0.013635453508984773 and parameters: {'hidden_dim': 44, 'id_embed_dim': 13, 'lr': 0.000665034509101885, 'batch_size': 32, 'alpha': 0.010486385014804032}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:18,487] Trial 113 finished with value: 0.02129910566977092 and parameters: {'hidden_dim': 44, 'id_embed_dim': 13, 'lr': 0.0009032953871029802, 'batch_size': 32, 'alpha': 0.01605657798523148}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:18,725] Trial 114 finished with value: 0.012100737653952792 and parameters: {'hidden_dim': 46, 'id_embed_dim': 12, 'lr': 0.0005873880298101833, 'batch_size': 32, 'alpha': 0.010358285986782362}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:18,966] Trial 115 finished with value: 0.034121966787746975 and parameters: {'hidden_dim': 50, 'id_embed_dim': 12, 'lr': 0.000581688969118505, 'batch_size': 32, 'alpha': 0.021212575040305513}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:19,204] Trial 116 finished with value: 0.2781527974551782 and parameters: {'hidden_dim': 46, 'id_embed_dim': 12, 'lr': 0.00012854692825198607, 'batch_size': 32, 'alpha': 0.029639688639903305}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:19,444] Trial 117 finished with value: 0.013421381102468734 and parameters: {'hidden_dim': 47, 'id_embed_dim': 13, 'lr': 0.0006921683355520111, 'batch_size': 32, 'alpha': 0.010249688366306926}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:19,581] Trial 118 finished with value: 0.21810308782229745 and parameters: {'hidden_dim': 53, 'id_embed_dim': 13, 'lr': 0.0006667657828670855, 'batch_size': 128, 'alpha': 0.03566879649736801}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:19,819] Trial 119 finished with value: 0.2425447195992434 and parameters: {'hidden_dim': 48, 'id_embed_dim': 13, 'lr': 0.0007793115572420772, 'batch_size': 32, 'alpha': 0.1474471824275362}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:20,079] Trial 120 finished with value: 0.30595201522784127 and parameters: {'hidden_dim': 59, 'id_embed_dim': 13, 'lr': 0.0005410583458151895, 'batch_size': 32, 'alpha': 0.1664839300422148}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:20,314] Trial 121 finished with value: 0.013164198124095014 and parameters: {'hidden_dim': 43, 'id_embed_dim': 12, 'lr': 0.0006195836713560999, 'batch_size': 32, 'alpha': 0.010401479315198109}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:20,555] Trial 122 finished with value: 0.03165360358088536 and parameters: {'hidden_dim': 50, 'id_embed_dim': 12, 'lr': 0.0007083088240604034, 'batch_size': 32, 'alpha': 0.020953897157211757}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:20,798] Trial 123 finished with value: 0.022123931241886958 and parameters: {'hidden_dim': 45, 'id_embed_dim': 12, 'lr': 0.0006279803617260838, 'batch_size': 32, 'alpha': 0.016078470891848745}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:21,035] Trial 124 finished with value: 0.016616879549241605 and parameters: {'hidden_dim': 43, 'id_embed_dim': 13, 'lr': 0.00040466887609472477, 'batch_size': 32, 'alpha': 0.010429212466380544}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:21,274] Trial 125 finished with value: 0.049120314251211356 and parameters: {'hidden_dim': 47, 'id_embed_dim': 12, 'lr': 0.00047311155278088364, 'batch_size': 32, 'alpha': 0.025326057816265304}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:21,510] Trial 126 finished with value: 0.03022677344935281 and parameters: {'hidden_dim': 42, 'id_embed_dim': 13, 'lr': 0.0005599428162243593, 'batch_size': 32, 'alpha': 0.01796778873194184}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:21,750] Trial 127 finished with value: 0.041624384883203004 and parameters: {'hidden_dim': 51, 'id_embed_dim': 12, 'lr': 0.0007626942755268834, 'batch_size': 32, 'alpha': 0.02611466925122241}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:21,989] Trial 128 finished with value: 0.013794950370613793 and parameters: {'hidden_dim': 44, 'id_embed_dim': 13, 'lr': 0.0006839187970056608, 'batch_size': 32, 'alpha': 0.010015425757911277}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:22,237] Trial 129 finished with value: 0.01264217102381968 and parameters: {'hidden_dim': 54, 'id_embed_dim': 8, 'lr': 0.0006723626569757486, 'batch_size': 32, 'alpha': 0.010358563705293264}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:22,488] Trial 130 finished with value: 0.034023291634437736 and parameters: {'hidden_dim': 57, 'id_embed_dim': 8, 'lr': 0.000671757208669223, 'batch_size': 32, 'alpha': 0.021364538700961985}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:22,747] Trial 131 finished with value: 0.01333412978994219 and parameters: {'hidden_dim': 54, 'id_embed_dim': 7, 'lr': 0.0005032534585758616, 'batch_size': 32, 'alpha': 0.010300552506298797}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:22,996] Trial 132 finished with value: 0.026373778574896933 and parameters: {'hidden_dim': 55, 'id_embed_dim': 7, 'lr': 0.0006067040074989262, 'batch_size': 32, 'alpha': 0.016770267263812668}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:23,247] Trial 133 finished with value: 0.022064028153742168 and parameters: {'hidden_dim': 54, 'id_embed_dim': 6, 'lr': 0.0003324616483364762, 'batch_size': 32, 'alpha': 0.010488638785982012}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:23,486] Trial 134 finished with value: 0.023454558776509492 and parameters: {'hidden_dim': 49, 'id_embed_dim': 8, 'lr': 0.000979180216930574, 'batch_size': 32, 'alpha': 0.016104014188602696}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:23,725] Trial 135 finished with value: 0.08354135888411586 and parameters: {'hidden_dim': 46, 'id_embed_dim': 7, 'lr': 0.0004991607135736125, 'batch_size': 32, 'alpha': 0.03244364952313055}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:23,969] Trial 136 finished with value: 0.04029394434134763 and parameters: {'hidden_dim': 51, 'id_embed_dim': 5, 'lr': 0.0006834936012623306, 'batch_size': 32, 'alpha': 0.023451427436392322}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:24,206] Trial 137 finished with value: 0.04272329426349554 and parameters: {'hidden_dim': 44, 'id_embed_dim': 8, 'lr': 0.0022470074597887918, 'batch_size': 32, 'alpha': 0.028499838527858215}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:24,445] Trial 138 finished with value: 0.021785927950439595 and parameters: {'hidden_dim': 48, 'id_embed_dim': 9, 'lr': 0.0008219668574681884, 'batch_size': 32, 'alpha': 0.015416835825884219}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:24,693] Trial 139 finished with value: 0.01903850828906647 and parameters: {'hidden_dim': 56, 'id_embed_dim': 6, 'lr': 0.00036699310508558935, 'batch_size': 32, 'alpha': 0.01047616537264112}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:24,823] Trial 140 finished with value: 0.2252171567284075 and parameters: {'hidden_dim': 41, 'id_embed_dim': 8, 'lr': 0.00043916033878130153, 'batch_size': 128, 'alpha': 0.021040021607862655}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:25,066] Trial 141 finished with value: 0.013744583689516648 and parameters: {'hidden_dim': 45, 'id_embed_dim': 6, 'lr': 0.0005083869580112506, 'batch_size': 32, 'alpha': 0.010295352550081878}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:25,305] Trial 142 finished with value: 0.0265933306873741 and parameters: {'hidden_dim': 47, 'id_embed_dim': 6, 'lr': 0.0005756164357046589, 'batch_size': 32, 'alpha': 0.01552636343831818}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:25,544] Trial 143 finished with value: 0.0419432050303409 and parameters: {'hidden_dim': 45, 'id_embed_dim': 7, 'lr': 0.0004991030469877142, 'batch_size': 32, 'alpha': 0.0207951146386872}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:25,785] Trial 144 finished with value: 0.013917404556072745 and parameters: {'hidden_dim': 52, 'id_embed_dim': 6, 'lr': 0.0006281659392680913, 'batch_size': 32, 'alpha': 0.01036268567165229}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:26,042] Trial 145 finished with value: 0.05012785542504232 and parameters: {'hidden_dim': 53, 'id_embed_dim': 6, 'lr': 0.0006404136897928596, 'batch_size': 32, 'alpha': 0.0267064157030424}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:26,287] Trial 146 finished with value: 0.025284140881963242 and parameters: {'hidden_dim': 49, 'id_embed_dim': 5, 'lr': 0.0007327587459350834, 'batch_size': 32, 'alpha': 0.016640243746299854}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:26,548] Trial 147 finished with value: 0.05545777894724581 and parameters: {'hidden_dim': 52, 'id_embed_dim': 6, 'lr': 0.0016475508580742065, 'batch_size': 32, 'alpha': 0.035643237792150194}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:26,826] Trial 148 finished with value: 0.034844099859097845 and parameters: {'hidden_dim': 60, 'id_embed_dim': 7, 'lr': 0.0006147095273079736, 'batch_size': 32, 'alpha': 0.02145060684569167}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:27,085] Trial 149 finished with value: 0.014328641382823313 and parameters: {'hidden_dim': 58, 'id_embed_dim': 6, 'lr': 0.0009065429350544167, 'batch_size': 32, 'alpha': 0.010425114816138841}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:27,334] Trial 150 finished with value: 0.048815693902341944 and parameters: {'hidden_dim': 58, 'id_embed_dim': 5, 'lr': 0.0008822376906792691, 'batch_size': 32, 'alpha': 0.02820004913259077}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:27,601] Trial 151 finished with value: 0.018985169820655557 and parameters: {'hidden_dim': 62, 'id_embed_dim': 6, 'lr': 0.0007630911806803794, 'batch_size': 32, 'alpha': 0.013750256892176207}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:27,860] Trial 152 finished with value: 0.01407460998603724 and parameters: {'hidden_dim': 54, 'id_embed_dim': 6, 'lr': 0.0006746512425424378, 'batch_size': 32, 'alpha': 0.010435057529965356}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:28,100] Trial 153 finished with value: 0.02978331427601047 and parameters: {'hidden_dim': 51, 'id_embed_dim': 6, 'lr': 0.0005377209761968973, 'batch_size': 32, 'alpha': 0.016819062669875158}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:28,350] Trial 154 finished with value: 0.012962466441935166 and parameters: {'hidden_dim': 55, 'id_embed_dim': 7, 'lr': 0.0013395771045885498, 'batch_size': 32, 'alpha': 0.01021481396242644}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:28,605] Trial 155 finished with value: 0.032244701778754256 and parameters: {'hidden_dim': 54, 'id_embed_dim': 7, 'lr': 0.0014026934955166134, 'batch_size': 32, 'alpha': 0.023077647966439574}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:28,868] Trial 156 finished with value: 0.02465827651041791 and parameters: {'hidden_dim': 66, 'id_embed_dim': 7, 'lr': 0.001968969893352975, 'batch_size': 32, 'alpha': 0.017769342073077475}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:29,109] Trial 157 finished with value: 0.012618664035895713 and parameters: {'hidden_dim': 48, 'id_embed_dim': 6, 'lr': 0.0013069423839425945, 'batch_size': 32, 'alpha': 0.010576650929250765}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:29,398] Trial 158 finished with value: 0.048973083440074346 and parameters: {'hidden_dim': 109, 'id_embed_dim': 7, 'lr': 0.0019121208517414031, 'batch_size': 32, 'alpha': 0.030409824314393007}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:29,638] Trial 159 finished with value: 0.03475550349269595 and parameters: {'hidden_dim': 48, 'id_embed_dim': 7, 'lr': 0.0012566777534719943, 'batch_size': 32, 'alpha': 0.02335538088839132}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:29,877] Trial 160 finished with value: 0.02247664192341324 and parameters: {'hidden_dim': 44, 'id_embed_dim': 9, 'lr': 0.0016707830813244126, 'batch_size': 32, 'alpha': 0.015870251160804153}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:30,129] Trial 161 finished with value: 0.01925594586514889 and parameters: {'hidden_dim': 56, 'id_embed_dim': 6, 'lr': 0.0014081831134342578, 'batch_size': 32, 'alpha': 0.014787288768235882}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:30,370] Trial 162 finished with value: 0.01203117413180215 and parameters: {'hidden_dim': 50, 'id_embed_dim': 6, 'lr': 0.0010778967285613565, 'batch_size': 32, 'alpha': 0.010187221708273513}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:30,615] Trial 163 finished with value: 0.013249351762067107 and parameters: {'hidden_dim': 49, 'id_embed_dim': 5, 'lr': 0.0015475289201492474, 'batch_size': 32, 'alpha': 0.010316973147257644}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:30,904] Trial 164 finished with value: 0.0299727403859895 and parameters: {'hidden_dim': 46, 'id_embed_dim': 4, 'lr': 0.00113004635280471, 'batch_size': 32, 'alpha': 0.02019939050313752}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:31,167] Trial 165 finished with value: 0.027051906388505062 and parameters: {'hidden_dim': 73, 'id_embed_dim': 9, 'lr': 0.001535834623218152, 'batch_size': 32, 'alpha': 0.020277434321006076}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:31,407] Trial 166 finished with value: 0.043315843868076354 and parameters: {'hidden_dim': 50, 'id_embed_dim': 5, 'lr': 0.0013118781344028361, 'batch_size': 32, 'alpha': 0.027023593449132792}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:31,645] Trial 167 finished with value: 0.012467784737061737 and parameters: {'hidden_dim': 43, 'id_embed_dim': 8, 'lr': 0.001789015644179466, 'batch_size': 32, 'alpha': 0.01031209555785646}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:31,881] Trial 168 finished with value: 0.021256633957189724 and parameters: {'hidden_dim': 42, 'id_embed_dim': 8, 'lr': 0.0016678426903466979, 'batch_size': 32, 'alpha': 0.017246605809168966}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:32,013] Trial 169 finished with value: 0.1942691379471829 and parameters: {'hidden_dim': 48, 'id_embed_dim': 8, 'lr': 0.0020767881830775366, 'batch_size': 128, 'alpha': 0.09792563539033775}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:32,253] Trial 170 finished with value: 0.03708264125244958 and parameters: {'hidden_dim': 46, 'id_embed_dim': 5, 'lr': 0.001788639047323981, 'batch_size': 32, 'alpha': 0.023662545973643395}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:32,489] Trial 171 finished with value: 0.20736081349222282 and parameters: {'hidden_dim': 43, 'id_embed_dim': 8, 'lr': 0.0012295423754858832, 'batch_size': 32, 'alpha': 0.11591856774717177}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:32,729] Trial 172 finished with value: 0.012252082221938255 and parameters: {'hidden_dim': 44, 'id_embed_dim': 7, 'lr': 0.0015358904568953515, 'batch_size': 32, 'alpha': 0.010131536980811562}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:32,969] Trial 173 finished with value: 0.02027198117702527 and parameters: {'hidden_dim': 48, 'id_embed_dim': 4, 'lr': 0.0014907296465326515, 'batch_size': 32, 'alpha': 0.015441406023486964}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:33,213] Trial 174 finished with value: 0.013258240416758042 and parameters: {'hidden_dim': 46, 'id_embed_dim': 7, 'lr': 0.0013683380138792298, 'batch_size': 32, 'alpha': 0.01016208881983874}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:33,449] Trial 175 finished with value: 0.02095170833058375 and parameters: {'hidden_dim': 40, 'id_embed_dim': 7, 'lr': 0.0013379286226985203, 'batch_size': 32, 'alpha': 0.015916648173491685}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:33,692] Trial 176 finished with value: 0.37832274114278924 and parameters: {'hidden_dim': 50, 'id_embed_dim': 7, 'lr': 0.001504942627374155, 'batch_size': 32, 'alpha': 0.25879041477537}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:33,933] Trial 177 finished with value: 0.012581264586152887 and parameters: {'hidden_dim': 47, 'id_embed_dim': 8, 'lr': 0.0011842979115726721, 'batch_size': 32, 'alpha': 0.010158679782269019}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:34,173] Trial 178 finished with value: 0.029550691996526 and parameters: {'hidden_dim': 47, 'id_embed_dim': 8, 'lr': 0.0010994612563928893, 'batch_size': 32, 'alpha': 0.020477178538052553}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:34,345] Trial 179 finished with value: 0.04930795230587622 and parameters: {'hidden_dim': 49, 'id_embed_dim': 8, 'lr': 0.0011891235064212427, 'batch_size': 64, 'alpha': 0.02664194380691124}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:34,582] Trial 180 finished with value: 0.02434676779167993 and parameters: {'hidden_dim': 42, 'id_embed_dim': 7, 'lr': 0.0010311544014869505, 'batch_size': 32, 'alpha': 0.01664337509032346}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:34,825] Trial 181 finished with value: 0.012030916246480512 and parameters: {'hidden_dim': 46, 'id_embed_dim': 7, 'lr': 0.0016101278029338245, 'batch_size': 32, 'alpha': 0.010409547294953449}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:35,068] Trial 182 finished with value: 0.019025161946402455 and parameters: {'hidden_dim': 46, 'id_embed_dim': 7, 'lr': 0.002451834571552849, 'batch_size': 32, 'alpha': 0.015009480396275494}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:35,320] Trial 183 finished with value: 0.012288593694119524 and parameters: {'hidden_dim': 51, 'id_embed_dim': 7, 'lr': 0.0017369392114878882, 'batch_size': 32, 'alpha': 0.010713504802376008}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:35,565] Trial 184 finished with value: 0.02899104093474553 and parameters: {'hidden_dim': 51, 'id_embed_dim': 7, 'lr': 0.001794282471762601, 'batch_size': 32, 'alpha': 0.020304634214102543}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:35,876] Trial 185 finished with value: 0.01958832189552766 and parameters: {'hidden_dim': 53, 'id_embed_dim': 7, 'lr': 0.0015904416956377641, 'batch_size': 32, 'alpha': 0.014833955131857634}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:36,144] Trial 186 finished with value: 0.012156571800771513 and parameters: {'hidden_dim': 49, 'id_embed_dim': 8, 'lr': 0.0020907340642504, 'batch_size': 32, 'alpha': 0.010036325118658714}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:36,383] Trial 187 finished with value: 0.03269566270641815 and parameters: {'hidden_dim': 45, 'id_embed_dim': 8, 'lr': 0.002075357157836097, 'batch_size': 32, 'alpha': 0.0235969394419488}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:36,623] Trial 188 finished with value: 0.04626348513857763 and parameters: {'hidden_dim': 49, 'id_embed_dim': 8, 'lr': 0.0023320976847664527, 'batch_size': 32, 'alpha': 0.031461852339155834}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:36,859] Trial 189 finished with value: 0.01238115375539414 and parameters: {'hidden_dim': 43, 'id_embed_dim': 8, 'lr': 0.0018358788328367423, 'batch_size': 32, 'alpha': 0.010392831451940386}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:37,099] Trial 190 finished with value: 0.02747432968782303 and parameters: {'hidden_dim': 43, 'id_embed_dim': 8, 'lr': 0.001791677800748669, 'batch_size': 32, 'alpha': 0.02009325126478209}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:37,332] Trial 191 finished with value: 0.14749414826694288 and parameters: {'hidden_dim': 41, 'id_embed_dim': 8, 'lr': 0.0013476265390400671, 'batch_size': 32, 'alpha': 0.0812342834510143}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:37,572] Trial 192 finished with value: 0.021081348727072093 and parameters: {'hidden_dim': 47, 'id_embed_dim': 8, 'lr': 0.0019194203233741842, 'batch_size': 32, 'alpha': 0.01519956746618829}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:37,812] Trial 193 finished with value: 0.284881989086481 and parameters: {'hidden_dim': 50, 'id_embed_dim': 9, 'lr': 0.0015781379562862135, 'batch_size': 32, 'alpha': 0.19785741946783356}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:38,061] Trial 194 finished with value: 0.01935311073535367 and parameters: {'hidden_dim': 45, 'id_embed_dim': 9, 'lr': 0.002213381008116036, 'batch_size': 32, 'alpha': 0.014648312789098564}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:38,348] Trial 195 finished with value: 0.015105189111335833 and parameters: {'hidden_dim': 43, 'id_embed_dim': 7, 'lr': 0.0014706918479780358, 'batch_size': 32, 'alpha': 0.011309328919754518}. Best is trial 44 with value: 0.011996831086681302.\n",
      "[I 2025-04-19 18:57:38,608] Trial 196 finished with value: 0.011931249885854865 and parameters: {'hidden_dim': 40, 'id_embed_dim': 8, 'lr': 0.001716668218424835, 'batch_size': 32, 'alpha': 0.010593412705517707}. Best is trial 196 with value: 0.011931249885854865.\n",
      "[I 2025-04-19 18:57:38,852] Trial 197 finished with value: 0.03026989693368288 and parameters: {'hidden_dim': 39, 'id_embed_dim': 8, 'lr': 0.0017333167422726333, 'batch_size': 32, 'alpha': 0.020297931993785816}. Best is trial 196 with value: 0.011931249885854865.\n",
      "[I 2025-04-19 18:57:39,100] Trial 198 finished with value: 0.0386811860633972 and parameters: {'hidden_dim': 41, 'id_embed_dim': 8, 'lr': 0.0020079618214160115, 'batch_size': 32, 'alpha': 0.02598634150732023}. Best is trial 196 with value: 0.011931249885854865.\n",
      "[I 2025-04-19 18:57:39,430] Trial 199 finished with value: 0.013081448138656472 and parameters: {'hidden_dim': 88, 'id_embed_dim': 8, 'lr': 0.00162775228445336, 'batch_size': 32, 'alpha': 0.010373401427104542}. Best is trial 196 with value: 0.011931249885854865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'hidden_dim': 40, 'id_embed_dim': 8, 'lr': 0.001716668218424835, 'batch_size': 32, 'alpha': 0.010593412705517707}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, train_df_split, val_df_split, id_map, input_dim, id_count, output_dim, device), n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparams:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c89fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "model = RNNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    id_count=id_count,\n",
    "    id_embed_dim=best_params['id_embed_dim'],\n",
    "    output_dim=output_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = OrdinalLabelSmoothingLoss(num_classes=output_dim, alpha=best_params['alpha'])\n",
    "\n",
    "train_loader = DataLoader(MoodDataset(train_df_split, id_map), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(MoodDataset(val_df_split, id_map), batch_size=best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0bfbe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.0979, val loss = 0.0664\n",
      "Epoch 2: train loss = 0.0421, val loss = 0.0256\n",
      "Epoch 3: train loss = 0.0177, val loss = 0.0166\n",
      "Epoch 4: train loss = 0.0127, val loss = 0.0148\n",
      "Epoch 5: train loss = 0.0112, val loss = 0.0138\n",
      "Epoch 6: train loss = 0.0104, val loss = 0.0134\n",
      "Epoch 7: train loss = 0.0099, val loss = 0.0131\n",
      "Epoch 8: train loss = 0.0094, val loss = 0.0130\n",
      "Epoch 9: train loss = 0.0089, val loss = 0.0131\n",
      "Epoch 10: train loss = 0.0086, val loss = 0.0122\n",
      "Epoch 11: train loss = 0.0083, val loss = 0.0120\n",
      "Epoch 12: train loss = 0.0078, val loss = 0.0118\n",
      "Epoch 13: train loss = 0.0075, val loss = 0.0121\n",
      "Epoch 14: train loss = 0.0073, val loss = 0.0123\n",
      "Epoch 15: train loss = 0.0070, val loss = 0.0124\n",
      "Epoch 16: train loss = 0.0067, val loss = 0.0122\n",
      "Epoch 17: train loss = 0.0066, val loss = 0.0119\n",
      "Epoch 18: train loss = 0.0065, val loss = 0.0120\n",
      "Epoch 19: train loss = 0.0063, val loss = 0.0128\n",
      "Epoch 20: train loss = 0.0062, val loss = 0.0124\n",
      "Epoch 21: train loss = 0.0060, val loss = 0.0130\n",
      "Epoch 22: train loss = 0.0060, val loss = 0.0122\n",
      "Epoch 23: train loss = 0.0059, val loss = 0.0124\n",
      "Epoch 24: train loss = 0.0056, val loss = 0.0123\n",
      "Epoch 25: train loss = 0.0055, val loss = 0.0125\n",
      "Epoch 26: train loss = 0.0053, val loss = 0.0127\n",
      "Epoch 27: train loss = 0.0053, val loss = 0.0128\n",
      "Epoch 28: train loss = 0.0052, val loss = 0.0133\n",
      "Epoch 29: train loss = 0.0050, val loss = 0.0129\n",
      "Epoch 30: train loss = 0.0049, val loss = 0.0132\n",
      "Epoch 31: train loss = 0.0049, val loss = 0.0139\n",
      "Epoch 32: train loss = 0.0048, val loss = 0.0138\n",
      "Epoch 33: train loss = 0.0046, val loss = 0.0141\n",
      "Epoch 34: train loss = 0.0046, val loss = 0.0143\n",
      "Epoch 35: train loss = 0.0045, val loss = 0.0143\n",
      "Epoch 36: train loss = 0.0045, val loss = 0.0146\n",
      "Epoch 37: train loss = 0.0043, val loss = 0.0147\n",
      "Epoch 38: train loss = 0.0042, val loss = 0.0143\n",
      "Epoch 39: train loss = 0.0041, val loss = 0.0144\n",
      "Epoch 40: train loss = 0.0040, val loss = 0.0149\n",
      "Epoch 41: train loss = 0.0039, val loss = 0.0147\n",
      "Epoch 42: train loss = 0.0038, val loss = 0.0157\n",
      "Epoch 43: train loss = 0.0038, val loss = 0.0150\n",
      "Epoch 44: train loss = 0.0037, val loss = 0.0151\n",
      "Epoch 45: train loss = 0.0036, val loss = 0.0147\n",
      "Epoch 46: train loss = 0.0035, val loss = 0.0158\n",
      "Epoch 47: train loss = 0.0035, val loss = 0.0148\n",
      "Epoch 48: train loss = 0.0034, val loss = 0.0152\n",
      "Epoch 49: train loss = 0.0032, val loss = 0.0154\n",
      "Epoch 50: train loss = 0.0032, val loss = 0.0154\n",
      "Epoch 51: train loss = 0.0031, val loss = 0.0155\n",
      "Epoch 52: train loss = 0.0030, val loss = 0.0160\n",
      "Epoch 53: train loss = 0.0031, val loss = 0.0157\n",
      "Epoch 54: train loss = 0.0030, val loss = 0.0170\n",
      "Epoch 55: train loss = 0.0029, val loss = 0.0157\n",
      "Epoch 56: train loss = 0.0028, val loss = 0.0157\n",
      "Epoch 57: train loss = 0.0028, val loss = 0.0156\n",
      "Epoch 58: train loss = 0.0027, val loss = 0.0174\n",
      "Epoch 59: train loss = 0.0026, val loss = 0.0160\n",
      "Epoch 60: train loss = 0.0025, val loss = 0.0162\n",
      "Epoch 61: train loss = 0.0025, val loss = 0.0162\n",
      "Epoch 62: train loss = 0.0024, val loss = 0.0161\n",
      "Epoch 63: train loss = 0.0024, val loss = 0.0170\n",
      "Epoch 64: train loss = 0.0023, val loss = 0.0168\n",
      "Epoch 65: train loss = 0.0023, val loss = 0.0178\n",
      "Epoch 66: train loss = 0.0022, val loss = 0.0169\n",
      "Epoch 67: train loss = 0.0021, val loss = 0.0169\n",
      "Epoch 68: train loss = 0.0021, val loss = 0.0170\n",
      "Epoch 69: train loss = 0.0020, val loss = 0.0176\n",
      "Epoch 70: train loss = 0.0019, val loss = 0.0176\n",
      "Epoch 71: train loss = 0.0019, val loss = 0.0178\n",
      "Epoch 72: train loss = 0.0019, val loss = 0.0201\n",
      "Epoch 73: train loss = 0.0019, val loss = 0.0191\n",
      "Epoch 74: train loss = 0.0019, val loss = 0.0190\n",
      "Epoch 75: train loss = 0.0017, val loss = 0.0188\n",
      "Epoch 76: train loss = 0.0017, val loss = 0.0198\n",
      "Epoch 77: train loss = 0.0017, val loss = 0.0196\n",
      "Epoch 78: train loss = 0.0016, val loss = 0.0193\n",
      "Epoch 79: train loss = 0.0016, val loss = 0.0200\n",
      "Epoch 80: train loss = 0.0016, val loss = 0.0211\n",
      "Epoch 81: train loss = 0.0016, val loss = 0.0211\n",
      "Epoch 82: train loss = 0.0015, val loss = 0.0216\n",
      "Epoch 83: train loss = 0.0015, val loss = 0.0212\n",
      "Epoch 84: train loss = 0.0015, val loss = 0.0219\n",
      "Epoch 85: train loss = 0.0014, val loss = 0.0208\n",
      "Epoch 86: train loss = 0.0013, val loss = 0.0217\n",
      "Epoch 87: train loss = 0.0013, val loss = 0.0212\n",
      "Epoch 88: train loss = 0.0013, val loss = 0.0218\n",
      "Epoch 89: train loss = 0.0013, val loss = 0.0212\n",
      "Epoch 90: train loss = 0.0012, val loss = 0.0215\n",
      "Epoch 91: train loss = 0.0012, val loss = 0.0221\n",
      "Epoch 92: train loss = 0.0012, val loss = 0.0231\n",
      "Epoch 93: train loss = 0.0012, val loss = 0.0219\n",
      "Epoch 94: train loss = 0.0012, val loss = 0.0225\n",
      "Epoch 95: train loss = 0.0011, val loss = 0.0225\n",
      "Epoch 96: train loss = 0.0010, val loss = 0.0218\n",
      "Epoch 97: train loss = 0.0011, val loss = 0.0231\n",
      "Epoch 98: train loss = 0.0010, val loss = 0.0231\n",
      "Epoch 99: train loss = 0.0010, val loss = 0.0232\n",
      "Epoch 100: train loss = 0.0010, val loss = 0.0239\n",
      "Epoch 101: train loss = 0.0009, val loss = 0.0233\n",
      "Epoch 102: train loss = 0.0009, val loss = 0.0231\n",
      "Epoch 103: train loss = 0.0009, val loss = 0.0241\n",
      "Epoch 104: train loss = 0.0009, val loss = 0.0234\n",
      "Epoch 105: train loss = 0.0008, val loss = 0.0241\n",
      "Epoch 106: train loss = 0.0008, val loss = 0.0245\n",
      "Epoch 107: train loss = 0.0008, val loss = 0.0237\n",
      "Epoch 108: train loss = 0.0008, val loss = 0.0251\n",
      "Epoch 109: train loss = 0.0008, val loss = 0.0237\n",
      "Epoch 110: train loss = 0.0008, val loss = 0.0235\n",
      "Epoch 111: train loss = 0.0007, val loss = 0.0236\n",
      "Epoch 112: train loss = 0.0007, val loss = 0.0239\n",
      "Epoch 113: train loss = 0.0007, val loss = 0.0259\n",
      "Epoch 114: train loss = 0.0007, val loss = 0.0235\n",
      "Epoch 115: train loss = 0.0007, val loss = 0.0245\n",
      "Epoch 116: train loss = 0.0007, val loss = 0.0246\n",
      "Epoch 117: train loss = 0.0006, val loss = 0.0251\n",
      "Epoch 118: train loss = 0.0006, val loss = 0.0249\n",
      "Epoch 119: train loss = 0.0006, val loss = 0.0242\n",
      "Epoch 120: train loss = 0.0006, val loss = 0.0262\n",
      "Epoch 121: train loss = 0.0006, val loss = 0.0251\n",
      "Epoch 122: train loss = 0.0005, val loss = 0.0258\n",
      "Epoch 123: train loss = 0.0005, val loss = 0.0261\n",
      "Epoch 124: train loss = 0.0005, val loss = 0.0259\n",
      "Epoch 125: train loss = 0.0005, val loss = 0.0256\n",
      "Epoch 126: train loss = 0.0005, val loss = 0.0259\n",
      "Epoch 127: train loss = 0.0006, val loss = 0.0256\n",
      "Epoch 128: train loss = 0.0005, val loss = 0.0254\n",
      "Epoch 129: train loss = 0.0005, val loss = 0.0264\n",
      "Epoch 130: train loss = 0.0005, val loss = 0.0258\n",
      "Epoch 131: train loss = 0.0004, val loss = 0.0265\n",
      "Epoch 132: train loss = 0.0005, val loss = 0.0267\n",
      "Epoch 133: train loss = 0.0004, val loss = 0.0267\n",
      "Epoch 134: train loss = 0.0004, val loss = 0.0266\n",
      "Epoch 135: train loss = 0.0004, val loss = 0.0264\n",
      "Epoch 136: train loss = 0.0004, val loss = 0.0266\n",
      "Epoch 137: train loss = 0.0004, val loss = 0.0266\n",
      "Epoch 138: train loss = 0.0004, val loss = 0.0264\n",
      "Epoch 139: train loss = 0.0004, val loss = 0.0260\n",
      "Epoch 140: train loss = 0.0004, val loss = 0.0264\n",
      "Epoch 141: train loss = 0.0003, val loss = 0.0271\n",
      "Epoch 142: train loss = 0.0004, val loss = 0.0270\n",
      "Epoch 143: train loss = 0.0004, val loss = 0.0266\n",
      "Epoch 144: train loss = 0.0003, val loss = 0.0278\n",
      "Epoch 145: train loss = 0.0003, val loss = 0.0270\n",
      "Epoch 146: train loss = 0.0003, val loss = 0.0277\n",
      "Epoch 147: train loss = 0.0003, val loss = 0.0285\n",
      "Epoch 148: train loss = 0.0003, val loss = 0.0273\n",
      "Epoch 149: train loss = 0.0003, val loss = 0.0282\n",
      "Epoch 150: train loss = 0.0003, val loss = 0.0279\n",
      "Epoch 151: train loss = 0.0003, val loss = 0.0280\n",
      "Epoch 152: train loss = 0.0003, val loss = 0.0278\n",
      "Epoch 153: train loss = 0.0003, val loss = 0.0285\n",
      "Epoch 154: train loss = 0.0003, val loss = 0.0291\n",
      "Epoch 155: train loss = 0.0003, val loss = 0.0289\n",
      "Epoch 156: train loss = 0.0003, val loss = 0.0290\n",
      "Epoch 157: train loss = 0.0002, val loss = 0.0296\n",
      "Epoch 158: train loss = 0.0002, val loss = 0.0303\n",
      "Epoch 159: train loss = 0.0002, val loss = 0.0304\n",
      "Epoch 160: train loss = 0.0002, val loss = 0.0302\n",
      "Epoch 161: train loss = 0.0002, val loss = 0.0306\n",
      "Epoch 162: train loss = 0.0002, val loss = 0.0308\n",
      "Epoch 163: train loss = 0.0002, val loss = 0.0307\n",
      "Epoch 164: train loss = 0.0002, val loss = 0.0320\n",
      "Epoch 165: train loss = 0.0002, val loss = 0.0306\n",
      "Epoch 166: train loss = 0.0002, val loss = 0.0306\n",
      "Epoch 167: train loss = 0.0002, val loss = 0.0317\n",
      "Epoch 168: train loss = 0.0002, val loss = 0.0314\n",
      "Epoch 169: train loss = 0.0002, val loss = 0.0317\n",
      "Epoch 170: train loss = 0.0002, val loss = 0.0314\n",
      "Epoch 171: train loss = 0.0002, val loss = 0.0319\n",
      "Epoch 172: train loss = 0.0002, val loss = 0.0326\n",
      "Epoch 173: train loss = 0.0002, val loss = 0.0329\n",
      "Epoch 174: train loss = 0.0002, val loss = 0.0321\n",
      "Epoch 175: train loss = 0.0002, val loss = 0.0329\n",
      "Epoch 176: train loss = 0.0002, val loss = 0.0334\n",
      "Epoch 177: train loss = 0.0002, val loss = 0.0331\n",
      "Epoch 178: train loss = 0.0002, val loss = 0.0338\n",
      "Epoch 179: train loss = 0.0002, val loss = 0.0333\n",
      "Epoch 180: train loss = 0.0002, val loss = 0.0330\n",
      "Epoch 181: train loss = 0.0002, val loss = 0.0332\n",
      "Epoch 182: train loss = 0.0002, val loss = 0.0325\n",
      "Epoch 183: train loss = 0.0002, val loss = 0.0337\n",
      "Epoch 184: train loss = 0.0002, val loss = 0.0335\n",
      "Epoch 185: train loss = 0.0002, val loss = 0.0349\n",
      "Epoch 186: train loss = 0.0002, val loss = 0.0338\n",
      "Epoch 187: train loss = 0.0002, val loss = 0.0340\n",
      "Epoch 188: train loss = 0.0002, val loss = 0.0350\n",
      "Epoch 189: train loss = 0.0002, val loss = 0.0340\n",
      "Epoch 190: train loss = 0.0002, val loss = 0.0347\n",
      "Epoch 191: train loss = 0.0002, val loss = 0.0335\n",
      "Epoch 192: train loss = 0.0001, val loss = 0.0340\n",
      "Epoch 193: train loss = 0.0001, val loss = 0.0341\n",
      "Epoch 194: train loss = 0.0001, val loss = 0.0341\n",
      "Epoch 195: train loss = 0.0001, val loss = 0.0342\n",
      "Epoch 196: train loss = 0.0002, val loss = 0.0338\n",
      "Epoch 197: train loss = 0.0002, val loss = 0.0348\n",
      "Epoch 198: train loss = 0.0002, val loss = 0.0332\n",
      "Epoch 199: train loss = 0.0001, val loss = 0.0349\n",
      "Epoch 200: train loss = 0.0002, val loss = 0.0345\n",
      "Epoch 201: train loss = 0.0002, val loss = 0.0336\n",
      "Epoch 202: train loss = 0.0002, val loss = 0.0342\n",
      "Epoch 203: train loss = 0.0002, val loss = 0.0345\n",
      "Epoch 204: train loss = 0.0001, val loss = 0.0342\n",
      "Epoch 205: train loss = 0.0001, val loss = 0.0343\n",
      "Epoch 206: train loss = 0.0001, val loss = 0.0335\n",
      "Epoch 207: train loss = 0.0001, val loss = 0.0330\n",
      "Epoch 208: train loss = 0.0002, val loss = 0.0334\n",
      "Epoch 209: train loss = 0.0002, val loss = 0.0338\n",
      "Epoch 210: train loss = 0.0001, val loss = 0.0334\n",
      "Epoch 211: train loss = 0.0001, val loss = 0.0340\n",
      "Epoch 212: train loss = 0.0001, val loss = 0.0337\n",
      "Epoch 213: train loss = 0.0001, val loss = 0.0345\n",
      "Epoch 214: train loss = 0.0001, val loss = 0.0341\n",
      "Epoch 215: train loss = 0.0001, val loss = 0.0341\n",
      "Epoch 216: train loss = 0.0001, val loss = 0.0329\n",
      "Epoch 217: train loss = 0.0001, val loss = 0.0338\n",
      "Epoch 218: train loss = 0.0001, val loss = 0.0348\n",
      "Epoch 219: train loss = 0.0001, val loss = 0.0337\n",
      "Epoch 220: train loss = 0.0001, val loss = 0.0331\n",
      "Epoch 221: train loss = 0.0001, val loss = 0.0324\n",
      "Epoch 222: train loss = 0.0001, val loss = 0.0324\n",
      "Epoch 223: train loss = 0.0001, val loss = 0.0333\n",
      "Epoch 224: train loss = 0.0001, val loss = 0.0338\n",
      "Epoch 225: train loss = 0.0001, val loss = 0.0334\n",
      "Epoch 226: train loss = 0.0001, val loss = 0.0328\n",
      "Epoch 227: train loss = 0.0001, val loss = 0.0337\n",
      "Epoch 228: train loss = 0.0001, val loss = 0.0332\n",
      "Epoch 229: train loss = 0.0001, val loss = 0.0332\n",
      "Epoch 230: train loss = 0.0001, val loss = 0.0330\n",
      "Epoch 231: train loss = 0.0001, val loss = 0.0330\n",
      "Epoch 232: train loss = 0.0001, val loss = 0.0339\n",
      "Epoch 233: train loss = 0.0001, val loss = 0.0325\n",
      "Epoch 234: train loss = 0.0001, val loss = 0.0336\n",
      "Epoch 235: train loss = 0.0001, val loss = 0.0326\n",
      "Epoch 236: train loss = 0.0001, val loss = 0.0321\n",
      "Epoch 237: train loss = 0.0001, val loss = 0.0316\n",
      "Epoch 238: train loss = 0.0001, val loss = 0.0330\n",
      "Epoch 239: train loss = 0.0001, val loss = 0.0310\n",
      "Epoch 240: train loss = 0.0001, val loss = 0.0326\n",
      "Epoch 241: train loss = 0.0001, val loss = 0.0320\n",
      "Epoch 242: train loss = 0.0001, val loss = 0.0333\n",
      "Epoch 243: train loss = 0.0002, val loss = 0.0310\n",
      "Epoch 244: train loss = 0.0002, val loss = 0.0337\n",
      "Epoch 245: train loss = 0.0002, val loss = 0.0325\n",
      "Epoch 246: train loss = 0.0002, val loss = 0.0333\n",
      "Epoch 247: train loss = 0.0002, val loss = 0.0318\n",
      "Epoch 248: train loss = 0.0001, val loss = 0.0313\n",
      "Epoch 249: train loss = 0.0001, val loss = 0.0309\n",
      "Epoch 250: train loss = 0.0002, val loss = 0.0313\n",
      "Epoch 251: train loss = 0.0001, val loss = 0.0310\n",
      "Epoch 252: train loss = 0.0001, val loss = 0.0302\n",
      "Epoch 253: train loss = 0.0001, val loss = 0.0304\n",
      "Epoch 254: train loss = 0.0001, val loss = 0.0313\n",
      "Epoch 255: train loss = 0.0001, val loss = 0.0309\n",
      "Epoch 256: train loss = 0.0001, val loss = 0.0301\n",
      "Epoch 257: train loss = 0.0001, val loss = 0.0315\n",
      "Epoch 258: train loss = 0.0001, val loss = 0.0308\n",
      "Epoch 259: train loss = 0.0001, val loss = 0.0304\n",
      "Epoch 260: train loss = 0.0001, val loss = 0.0308\n",
      "Epoch 261: train loss = 0.0001, val loss = 0.0300\n",
      "Epoch 262: train loss = 0.0001, val loss = 0.0314\n",
      "Epoch 263: train loss = 0.0001, val loss = 0.0313\n",
      "Epoch 264: train loss = 0.0001, val loss = 0.0311\n",
      "Epoch 265: train loss = 0.0001, val loss = 0.0300\n",
      "Epoch 266: train loss = 0.0001, val loss = 0.0308\n",
      "Epoch 267: train loss = 0.0001, val loss = 0.0307\n",
      "Epoch 268: train loss = 0.0001, val loss = 0.0305\n",
      "Epoch 269: train loss = 0.0001, val loss = 0.0300\n",
      "Epoch 270: train loss = 0.0001, val loss = 0.0306\n",
      "Epoch 271: train loss = 0.0001, val loss = 0.0305\n",
      "Epoch 272: train loss = 0.0001, val loss = 0.0306\n",
      "Epoch 273: train loss = 0.0001, val loss = 0.0311\n",
      "Epoch 274: train loss = 0.0001, val loss = 0.0304\n",
      "Epoch 275: train loss = 0.0001, val loss = 0.0302\n",
      "Epoch 276: train loss = 0.0001, val loss = 0.0294\n",
      "Epoch 277: train loss = 0.0001, val loss = 0.0306\n",
      "Epoch 278: train loss = 0.0001, val loss = 0.0303\n",
      "Epoch 279: train loss = 0.0001, val loss = 0.0305\n",
      "Epoch 280: train loss = 0.0001, val loss = 0.0289\n",
      "Epoch 281: train loss = 0.0001, val loss = 0.0297\n",
      "Epoch 282: train loss = 0.0001, val loss = 0.0293\n",
      "Epoch 283: train loss = 0.0001, val loss = 0.0296\n",
      "Epoch 284: train loss = 0.0001, val loss = 0.0296\n",
      "Epoch 285: train loss = 0.0001, val loss = 0.0297\n",
      "Epoch 286: train loss = 0.0001, val loss = 0.0298\n",
      "Epoch 287: train loss = 0.0001, val loss = 0.0294\n",
      "Epoch 288: train loss = 0.0001, val loss = 0.0290\n",
      "Epoch 289: train loss = 0.0001, val loss = 0.0294\n",
      "Epoch 290: train loss = 0.0001, val loss = 0.0291\n",
      "Epoch 291: train loss = 0.0001, val loss = 0.0281\n",
      "Epoch 292: train loss = 0.0001, val loss = 0.0300\n",
      "Epoch 293: train loss = 0.0001, val loss = 0.0286\n",
      "Epoch 294: train loss = 0.0001, val loss = 0.0295\n",
      "Epoch 295: train loss = 0.0001, val loss = 0.0299\n",
      "Epoch 296: train loss = 0.0001, val loss = 0.0282\n",
      "Epoch 297: train loss = 0.0001, val loss = 0.0297\n",
      "Epoch 298: train loss = 0.0001, val loss = 0.0280\n",
      "Epoch 299: train loss = 0.0001, val loss = 0.0281\n",
      "Epoch 300: train loss = 0.0001, val loss = 0.0287\n",
      "Epoch 301: train loss = 0.0001, val loss = 0.0286\n",
      "Epoch 302: train loss = 0.0001, val loss = 0.0283\n",
      "Epoch 303: train loss = 0.0001, val loss = 0.0282\n",
      "Epoch 304: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 305: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 306: train loss = 0.0001, val loss = 0.0283\n",
      "Epoch 307: train loss = 0.0001, val loss = 0.0286\n",
      "Epoch 308: train loss = 0.0001, val loss = 0.0282\n",
      "Epoch 309: train loss = 0.0001, val loss = 0.0283\n",
      "Epoch 310: train loss = 0.0001, val loss = 0.0284\n",
      "Epoch 311: train loss = 0.0001, val loss = 0.0292\n",
      "Epoch 312: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 313: train loss = 0.0001, val loss = 0.0284\n",
      "Epoch 314: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 315: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 316: train loss = 0.0001, val loss = 0.0281\n",
      "Epoch 317: train loss = 0.0001, val loss = 0.0284\n",
      "Epoch 318: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 319: train loss = 0.0001, val loss = 0.0290\n",
      "Epoch 320: train loss = 0.0001, val loss = 0.0281\n",
      "Epoch 321: train loss = 0.0001, val loss = 0.0288\n",
      "Epoch 322: train loss = 0.0001, val loss = 0.0286\n",
      "Epoch 323: train loss = 0.0001, val loss = 0.0283\n",
      "Epoch 324: train loss = 0.0001, val loss = 0.0282\n",
      "Epoch 325: train loss = 0.0001, val loss = 0.0294\n",
      "Epoch 326: train loss = 0.0001, val loss = 0.0281\n",
      "Epoch 327: train loss = 0.0001, val loss = 0.0278\n",
      "Epoch 328: train loss = 0.0001, val loss = 0.0287\n",
      "Epoch 329: train loss = 0.0001, val loss = 0.0281\n",
      "Epoch 330: train loss = 0.0001, val loss = 0.0284\n",
      "Epoch 331: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 332: train loss = 0.0001, val loss = 0.0285\n",
      "Epoch 333: train loss = 0.0001, val loss = 0.0280\n",
      "Epoch 334: train loss = 0.0001, val loss = 0.0282\n",
      "Epoch 335: train loss = 0.0001, val loss = 0.0282\n",
      "Epoch 336: train loss = 0.0001, val loss = 0.0282\n",
      "Epoch 337: train loss = 0.0000, val loss = 0.0278\n",
      "Epoch 338: train loss = 0.0001, val loss = 0.0280\n",
      "Epoch 339: train loss = 0.0001, val loss = 0.0283\n",
      "Epoch 340: train loss = 0.0001, val loss = 0.0277\n",
      "Epoch 341: train loss = 0.0001, val loss = 0.0278\n",
      "Epoch 342: train loss = 0.0001, val loss = 0.0275\n",
      "Epoch 343: train loss = 0.0001, val loss = 0.0275\n",
      "Epoch 344: train loss = 0.0001, val loss = 0.0276\n",
      "Epoch 345: train loss = 0.0001, val loss = 0.0277\n",
      "Epoch 346: train loss = 0.0001, val loss = 0.0280\n",
      "Epoch 347: train loss = 0.0001, val loss = 0.0277\n",
      "Epoch 348: train loss = 0.0001, val loss = 0.0284\n",
      "Epoch 349: train loss = 0.0001, val loss = 0.0284\n",
      "Epoch 350: train loss = 0.0001, val loss = 0.0278\n",
      "Epoch 351: train loss = 0.0001, val loss = 0.0271\n",
      "Epoch 352: train loss = 0.0001, val loss = 0.0280\n",
      "Epoch 353: train loss = 0.0001, val loss = 0.0281\n",
      "Epoch 354: train loss = 0.0002, val loss = 0.0271\n",
      "Epoch 355: train loss = 0.0002, val loss = 0.0277\n",
      "Epoch 356: train loss = 0.0001, val loss = 0.0274\n",
      "Epoch 357: train loss = 0.0001, val loss = 0.0274\n",
      "Epoch 358: train loss = 0.0001, val loss = 0.0273\n",
      "Epoch 359: train loss = 0.0001, val loss = 0.0275\n",
      "Epoch 360: train loss = 0.0000, val loss = 0.0269\n",
      "Epoch 361: train loss = 0.0001, val loss = 0.0270\n",
      "Epoch 362: train loss = 0.0001, val loss = 0.0272\n",
      "Epoch 363: train loss = 0.0000, val loss = 0.0273\n",
      "Epoch 364: train loss = 0.0000, val loss = 0.0276\n",
      "Epoch 365: train loss = 0.0000, val loss = 0.0273\n",
      "Epoch 366: train loss = 0.0000, val loss = 0.0274\n",
      "Epoch 367: train loss = 0.0000, val loss = 0.0273\n",
      "Epoch 368: train loss = 0.0000, val loss = 0.0281\n",
      "Epoch 369: train loss = 0.0000, val loss = 0.0274\n",
      "Epoch 370: train loss = 0.0000, val loss = 0.0276\n",
      "Epoch 371: train loss = 0.0000, val loss = 0.0275\n",
      "Epoch 372: train loss = 0.0000, val loss = 0.0277\n",
      "Epoch 373: train loss = 0.0000, val loss = 0.0275\n",
      "Epoch 374: train loss = 0.0000, val loss = 0.0280\n",
      "Epoch 375: train loss = 0.0000, val loss = 0.0277\n",
      "Epoch 376: train loss = 0.0001, val loss = 0.0279\n",
      "Epoch 377: train loss = 0.0001, val loss = 0.0272\n",
      "Epoch 378: train loss = 0.0001, val loss = 0.0273\n",
      "Epoch 379: train loss = 0.0001, val loss = 0.0270\n",
      "Epoch 380: train loss = 0.0001, val loss = 0.0275\n",
      "Epoch 381: train loss = 0.0001, val loss = 0.0276\n",
      "Epoch 382: train loss = 0.0001, val loss = 0.0272\n",
      "Epoch 383: train loss = 0.0001, val loss = 0.0265\n",
      "Epoch 384: train loss = 0.0001, val loss = 0.0279\n",
      "Epoch 385: train loss = 0.0001, val loss = 0.0275\n",
      "Epoch 386: train loss = 0.0001, val loss = 0.0280\n",
      "Epoch 387: train loss = 0.0001, val loss = 0.0266\n",
      "Epoch 388: train loss = 0.0001, val loss = 0.0271\n",
      "Epoch 389: train loss = 0.0001, val loss = 0.0267\n",
      "Epoch 390: train loss = 0.0001, val loss = 0.0269\n",
      "Epoch 391: train loss = 0.0001, val loss = 0.0268\n",
      "Epoch 392: train loss = 0.0001, val loss = 0.0267\n",
      "Epoch 393: train loss = 0.0001, val loss = 0.0273\n",
      "Epoch 394: train loss = 0.0001, val loss = 0.0272\n",
      "Epoch 395: train loss = 0.0001, val loss = 0.0274\n",
      "Epoch 396: train loss = 0.0001, val loss = 0.0274\n",
      "Epoch 397: train loss = 0.0001, val loss = 0.0269\n",
      "Epoch 398: train loss = 0.0001, val loss = 0.0269\n",
      "Epoch 399: train loss = 0.0000, val loss = 0.0270\n",
      "Epoch 400: train loss = 0.0001, val loss = 0.0270\n",
      "Epoch 401: train loss = 0.0000, val loss = 0.0272\n",
      "Epoch 402: train loss = 0.0000, val loss = 0.0274\n",
      "Epoch 403: train loss = 0.0000, val loss = 0.0269\n",
      "Epoch 404: train loss = 0.0000, val loss = 0.0265\n",
      "Epoch 405: train loss = 0.0000, val loss = 0.0268\n",
      "Epoch 406: train loss = 0.0000, val loss = 0.0267\n",
      "Epoch 407: train loss = 0.0000, val loss = 0.0267\n",
      "Epoch 408: train loss = 0.0000, val loss = 0.0267\n",
      "Epoch 409: train loss = 0.0000, val loss = 0.0272\n",
      "Epoch 410: train loss = 0.0000, val loss = 0.0270\n",
      "Epoch 411: train loss = 0.0000, val loss = 0.0265\n",
      "Epoch 412: train loss = 0.0001, val loss = 0.0267\n",
      "Epoch 413: train loss = 0.0000, val loss = 0.0266\n",
      "Epoch 414: train loss = 0.0000, val loss = 0.0267\n",
      "Epoch 415: train loss = 0.0000, val loss = 0.0266\n",
      "Epoch 416: train loss = 0.0001, val loss = 0.0264\n",
      "Epoch 417: train loss = 0.0001, val loss = 0.0265\n",
      "Epoch 418: train loss = 0.0001, val loss = 0.0270\n",
      "Epoch 419: train loss = 0.0001, val loss = 0.0272\n",
      "Epoch 420: train loss = 0.0001, val loss = 0.0268\n",
      "Epoch 421: train loss = 0.0001, val loss = 0.0264\n",
      "Epoch 422: train loss = 0.0001, val loss = 0.0262\n",
      "Epoch 423: train loss = 0.0001, val loss = 0.0269\n",
      "Epoch 424: train loss = 0.0001, val loss = 0.0273\n",
      "Epoch 425: train loss = 0.0001, val loss = 0.0261\n",
      "Epoch 426: train loss = 0.0001, val loss = 0.0273\n",
      "Epoch 427: train loss = 0.0001, val loss = 0.0266\n",
      "Epoch 428: train loss = 0.0001, val loss = 0.0264\n",
      "Epoch 429: train loss = 0.0001, val loss = 0.0267\n",
      "Epoch 430: train loss = 0.0001, val loss = 0.0262\n",
      "Epoch 431: train loss = 0.0001, val loss = 0.0261\n",
      "Epoch 432: train loss = 0.0001, val loss = 0.0262\n",
      "Epoch 433: train loss = 0.0001, val loss = 0.0258\n",
      "Epoch 434: train loss = 0.0001, val loss = 0.0260\n",
      "Epoch 435: train loss = 0.0001, val loss = 0.0261\n",
      "Epoch 436: train loss = 0.0001, val loss = 0.0262\n",
      "Epoch 437: train loss = 0.0001, val loss = 0.0267\n",
      "Epoch 438: train loss = 0.0001, val loss = 0.0255\n",
      "Epoch 439: train loss = 0.0001, val loss = 0.0257\n",
      "Epoch 440: train loss = 0.0001, val loss = 0.0259\n",
      "Epoch 441: train loss = 0.0001, val loss = 0.0255\n",
      "Epoch 442: train loss = 0.0001, val loss = 0.0261\n",
      "Epoch 443: train loss = 0.0001, val loss = 0.0252\n",
      "Epoch 444: train loss = 0.0001, val loss = 0.0259\n",
      "Epoch 445: train loss = 0.0001, val loss = 0.0261\n",
      "Epoch 446: train loss = 0.0001, val loss = 0.0263\n",
      "Epoch 447: train loss = 0.0001, val loss = 0.0260\n",
      "Epoch 448: train loss = 0.0000, val loss = 0.0257\n",
      "Epoch 449: train loss = 0.0000, val loss = 0.0253\n",
      "Epoch 450: train loss = 0.0001, val loss = 0.0263\n",
      "Epoch 451: train loss = 0.0000, val loss = 0.0261\n",
      "Epoch 452: train loss = 0.0000, val loss = 0.0258\n",
      "Epoch 453: train loss = 0.0000, val loss = 0.0256\n",
      "Epoch 454: train loss = 0.0000, val loss = 0.0263\n",
      "Epoch 455: train loss = 0.0000, val loss = 0.0254\n",
      "Epoch 456: train loss = 0.0001, val loss = 0.0260\n",
      "Epoch 457: train loss = 0.0001, val loss = 0.0261\n",
      "Epoch 458: train loss = 0.0001, val loss = 0.0258\n",
      "Epoch 459: train loss = 0.0001, val loss = 0.0258\n",
      "Epoch 460: train loss = 0.0001, val loss = 0.0252\n",
      "Epoch 461: train loss = 0.0001, val loss = 0.0257\n",
      "Epoch 462: train loss = 0.0001, val loss = 0.0260\n",
      "Epoch 463: train loss = 0.0001, val loss = 0.0255\n",
      "Epoch 464: train loss = 0.0001, val loss = 0.0254\n",
      "Epoch 465: train loss = 0.0001, val loss = 0.0249\n",
      "Epoch 466: train loss = 0.0001, val loss = 0.0253\n",
      "Epoch 467: train loss = 0.0001, val loss = 0.0251\n",
      "Epoch 468: train loss = 0.0001, val loss = 0.0248\n",
      "Epoch 469: train loss = 0.0001, val loss = 0.0253\n",
      "Epoch 470: train loss = 0.0001, val loss = 0.0257\n",
      "Epoch 471: train loss = 0.0001, val loss = 0.0248\n",
      "Epoch 472: train loss = 0.0000, val loss = 0.0252\n",
      "Epoch 473: train loss = 0.0000, val loss = 0.0253\n",
      "Epoch 474: train loss = 0.0000, val loss = 0.0252\n",
      "Epoch 475: train loss = 0.0000, val loss = 0.0244\n",
      "Epoch 476: train loss = 0.0000, val loss = 0.0249\n",
      "Epoch 477: train loss = 0.0000, val loss = 0.0246\n",
      "Epoch 478: train loss = 0.0000, val loss = 0.0247\n",
      "Epoch 479: train loss = 0.0000, val loss = 0.0242\n",
      "Epoch 480: train loss = 0.0000, val loss = 0.0243\n",
      "Epoch 481: train loss = 0.0001, val loss = 0.0252\n",
      "Epoch 482: train loss = 0.0000, val loss = 0.0246\n",
      "Epoch 483: train loss = 0.0000, val loss = 0.0247\n",
      "Epoch 484: train loss = 0.0000, val loss = 0.0247\n",
      "Epoch 485: train loss = 0.0000, val loss = 0.0249\n",
      "Epoch 486: train loss = 0.0000, val loss = 0.0246\n",
      "Epoch 487: train loss = 0.0000, val loss = 0.0245\n",
      "Epoch 488: train loss = 0.0000, val loss = 0.0249\n",
      "Epoch 489: train loss = 0.0000, val loss = 0.0247\n",
      "Epoch 490: train loss = 0.0000, val loss = 0.0249\n",
      "Epoch 491: train loss = 0.0000, val loss = 0.0250\n",
      "Epoch 492: train loss = 0.0000, val loss = 0.0245\n",
      "Epoch 493: train loss = 0.0000, val loss = 0.0249\n",
      "Epoch 494: train loss = 0.0000, val loss = 0.0245\n",
      "Epoch 495: train loss = 0.0001, val loss = 0.0241\n",
      "Epoch 496: train loss = 0.0001, val loss = 0.0249\n",
      "Epoch 497: train loss = 0.0001, val loss = 0.0241\n",
      "Epoch 498: train loss = 0.0001, val loss = 0.0249\n",
      "Epoch 499: train loss = 0.0001, val loss = 0.0254\n",
      "Epoch 500: train loss = 0.0001, val loss = 0.0241\n",
      "Epoch 501: train loss = 0.0001, val loss = 0.0235\n",
      "Epoch 502: train loss = 0.0001, val loss = 0.0249\n",
      "Epoch 503: train loss = 0.0001, val loss = 0.0234\n",
      "Epoch 504: train loss = 0.0001, val loss = 0.0243\n",
      "Epoch 505: train loss = 0.0001, val loss = 0.0238\n",
      "Epoch 506: train loss = 0.0001, val loss = 0.0241\n",
      "Epoch 507: train loss = 0.0001, val loss = 0.0239\n",
      "Epoch 508: train loss = 0.0000, val loss = 0.0239\n",
      "Epoch 509: train loss = 0.0000, val loss = 0.0240\n",
      "Epoch 510: train loss = 0.0000, val loss = 0.0243\n",
      "Epoch 511: train loss = 0.0000, val loss = 0.0241\n",
      "Epoch 512: train loss = 0.0000, val loss = 0.0242\n",
      "Epoch 513: train loss = 0.0000, val loss = 0.0236\n",
      "Epoch 514: train loss = 0.0000, val loss = 0.0240\n",
      "Epoch 515: train loss = 0.0000, val loss = 0.0239\n",
      "Epoch 516: train loss = 0.0000, val loss = 0.0238\n",
      "Epoch 517: train loss = 0.0001, val loss = 0.0236\n",
      "Epoch 518: train loss = 0.0001, val loss = 0.0237\n",
      "Epoch 519: train loss = 0.0001, val loss = 0.0241\n",
      "Epoch 520: train loss = 0.0001, val loss = 0.0242\n",
      "Epoch 521: train loss = 0.0001, val loss = 0.0236\n",
      "Epoch 522: train loss = 0.0001, val loss = 0.0236\n",
      "Epoch 523: train loss = 0.0000, val loss = 0.0239\n",
      "Epoch 524: train loss = 0.0000, val loss = 0.0237\n",
      "Epoch 525: train loss = 0.0000, val loss = 0.0239\n",
      "Epoch 526: train loss = 0.0000, val loss = 0.0240\n",
      "Epoch 527: train loss = 0.0000, val loss = 0.0235\n",
      "Epoch 528: train loss = 0.0000, val loss = 0.0240\n",
      "Epoch 529: train loss = 0.0000, val loss = 0.0237\n",
      "Epoch 530: train loss = 0.0000, val loss = 0.0237\n",
      "Epoch 531: train loss = 0.0000, val loss = 0.0239\n",
      "Epoch 532: train loss = 0.0000, val loss = 0.0242\n",
      "Epoch 533: train loss = 0.0000, val loss = 0.0236\n",
      "Epoch 534: train loss = 0.0000, val loss = 0.0236\n",
      "Epoch 535: train loss = 0.0000, val loss = 0.0238\n",
      "Epoch 536: train loss = 0.0000, val loss = 0.0236\n",
      "Epoch 537: train loss = 0.0000, val loss = 0.0239\n",
      "Epoch 538: train loss = 0.0000, val loss = 0.0238\n",
      "Epoch 539: train loss = 0.0000, val loss = 0.0238\n",
      "Epoch 540: train loss = 0.0000, val loss = 0.0238\n",
      "Epoch 541: train loss = 0.0000, val loss = 0.0242\n",
      "Epoch 542: train loss = 0.0000, val loss = 0.0240\n",
      "Epoch 543: train loss = 0.0000, val loss = 0.0235\n",
      "Epoch 544: train loss = 0.0000, val loss = 0.0241\n",
      "Epoch 545: train loss = 0.0000, val loss = 0.0234\n",
      "Epoch 546: train loss = 0.0000, val loss = 0.0235\n",
      "Epoch 547: train loss = 0.0001, val loss = 0.0232\n",
      "Epoch 548: train loss = 0.0001, val loss = 0.0239\n",
      "Epoch 549: train loss = 0.0001, val loss = 0.0238\n",
      "Epoch 550: train loss = 0.0001, val loss = 0.0233\n",
      "Epoch 551: train loss = 0.0001, val loss = 0.0231\n",
      "Epoch 552: train loss = 0.0001, val loss = 0.0232\n",
      "Epoch 553: train loss = 0.0001, val loss = 0.0227\n",
      "Epoch 554: train loss = 0.0001, val loss = 0.0235\n",
      "Epoch 555: train loss = 0.0001, val loss = 0.0228\n",
      "Epoch 556: train loss = 0.0001, val loss = 0.0229\n",
      "Epoch 557: train loss = 0.0001, val loss = 0.0231\n",
      "Epoch 558: train loss = 0.0001, val loss = 0.0233\n",
      "Epoch 559: train loss = 0.0001, val loss = 0.0231\n",
      "Epoch 560: train loss = 0.0001, val loss = 0.0230\n",
      "Epoch 561: train loss = 0.0000, val loss = 0.0231\n",
      "Epoch 562: train loss = 0.0000, val loss = 0.0233\n",
      "Epoch 563: train loss = 0.0000, val loss = 0.0236\n",
      "Epoch 564: train loss = 0.0000, val loss = 0.0233\n",
      "Epoch 565: train loss = 0.0000, val loss = 0.0232\n",
      "Epoch 566: train loss = 0.0000, val loss = 0.0233\n",
      "Epoch 567: train loss = 0.0000, val loss = 0.0229\n",
      "Epoch 568: train loss = 0.0000, val loss = 0.0233\n",
      "Epoch 569: train loss = 0.0000, val loss = 0.0232\n",
      "Epoch 570: train loss = 0.0000, val loss = 0.0231\n",
      "Epoch 571: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 572: train loss = 0.0000, val loss = 0.0233\n",
      "Epoch 573: train loss = 0.0000, val loss = 0.0232\n",
      "Epoch 574: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 575: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 576: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 577: train loss = 0.0001, val loss = 0.0226\n",
      "Epoch 578: train loss = 0.0001, val loss = 0.0227\n",
      "Epoch 579: train loss = 0.0000, val loss = 0.0229\n",
      "Epoch 580: train loss = 0.0000, val loss = 0.0226\n",
      "Epoch 581: train loss = 0.0000, val loss = 0.0228\n",
      "Epoch 582: train loss = 0.0000, val loss = 0.0232\n",
      "Epoch 583: train loss = 0.0000, val loss = 0.0229\n",
      "Epoch 584: train loss = 0.0000, val loss = 0.0231\n",
      "Epoch 585: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 586: train loss = 0.0000, val loss = 0.0233\n",
      "Epoch 587: train loss = 0.0000, val loss = 0.0232\n",
      "Epoch 588: train loss = 0.0000, val loss = 0.0228\n",
      "Epoch 589: train loss = 0.0000, val loss = 0.0237\n",
      "Epoch 590: train loss = 0.0000, val loss = 0.0228\n",
      "Epoch 591: train loss = 0.0000, val loss = 0.0230\n",
      "Epoch 592: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 593: train loss = 0.0001, val loss = 0.0232\n",
      "Epoch 594: train loss = 0.0001, val loss = 0.0227\n",
      "Epoch 595: train loss = 0.0001, val loss = 0.0228\n",
      "Epoch 596: train loss = 0.0001, val loss = 0.0230\n",
      "Epoch 597: train loss = 0.0001, val loss = 0.0230\n",
      "Epoch 598: train loss = 0.0001, val loss = 0.0230\n",
      "Epoch 599: train loss = 0.0001, val loss = 0.0232\n",
      "Epoch 600: train loss = 0.0001, val loss = 0.0223\n",
      "Epoch 601: train loss = 0.0001, val loss = 0.0223\n",
      "Epoch 602: train loss = 0.0000, val loss = 0.0231\n",
      "Epoch 603: train loss = 0.0000, val loss = 0.0229\n",
      "Epoch 604: train loss = 0.0000, val loss = 0.0230\n",
      "Epoch 605: train loss = 0.0000, val loss = 0.0226\n",
      "Epoch 606: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 607: train loss = 0.0000, val loss = 0.0225\n",
      "Epoch 608: train loss = 0.0001, val loss = 0.0226\n",
      "Epoch 609: train loss = 0.0001, val loss = 0.0229\n",
      "Epoch 610: train loss = 0.0001, val loss = 0.0224\n",
      "Epoch 611: train loss = 0.0001, val loss = 0.0226\n",
      "Epoch 612: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 613: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 614: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 615: train loss = 0.0000, val loss = 0.0225\n",
      "Epoch 616: train loss = 0.0000, val loss = 0.0229\n",
      "Epoch 617: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 618: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 619: train loss = 0.0000, val loss = 0.0230\n",
      "Epoch 620: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 621: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 622: train loss = 0.0000, val loss = 0.0225\n",
      "Epoch 623: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 624: train loss = 0.0000, val loss = 0.0231\n",
      "Epoch 625: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 626: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 627: train loss = 0.0000, val loss = 0.0226\n",
      "Epoch 628: train loss = 0.0000, val loss = 0.0227\n",
      "Epoch 629: train loss = 0.0000, val loss = 0.0226\n",
      "Epoch 630: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 631: train loss = 0.0000, val loss = 0.0222\n",
      "Epoch 632: train loss = 0.0000, val loss = 0.0224\n",
      "Epoch 633: train loss = 0.0000, val loss = 0.0223\n",
      "Epoch 634: train loss = 0.0000, val loss = 0.0225\n",
      "Epoch 635: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 636: train loss = 0.0001, val loss = 0.0216\n",
      "Epoch 637: train loss = 0.0001, val loss = 0.0219\n",
      "Epoch 638: train loss = 0.0001, val loss = 0.0225\n",
      "Epoch 639: train loss = 0.0001, val loss = 0.0225\n",
      "Epoch 640: train loss = 0.0001, val loss = 0.0218\n",
      "Epoch 641: train loss = 0.0001, val loss = 0.0216\n",
      "Epoch 642: train loss = 0.0001, val loss = 0.0219\n",
      "Epoch 643: train loss = 0.0001, val loss = 0.0220\n",
      "Epoch 644: train loss = 0.0001, val loss = 0.0221\n",
      "Epoch 645: train loss = 0.0001, val loss = 0.0217\n",
      "Epoch 646: train loss = 0.0000, val loss = 0.0222\n",
      "Epoch 647: train loss = 0.0000, val loss = 0.0221\n",
      "Epoch 648: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 649: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 650: train loss = 0.0000, val loss = 0.0221\n",
      "Epoch 651: train loss = 0.0000, val loss = 0.0221\n",
      "Epoch 652: train loss = 0.0000, val loss = 0.0221\n",
      "Epoch 653: train loss = 0.0000, val loss = 0.0221\n",
      "Epoch 654: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 655: train loss = 0.0000, val loss = 0.0219\n",
      "Epoch 656: train loss = 0.0000, val loss = 0.0219\n",
      "Epoch 657: train loss = 0.0000, val loss = 0.0221\n",
      "Epoch 658: train loss = 0.0000, val loss = 0.0219\n",
      "Epoch 659: train loss = 0.0000, val loss = 0.0217\n",
      "Epoch 660: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 661: train loss = 0.0000, val loss = 0.0221\n",
      "Epoch 662: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 663: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 664: train loss = 0.0000, val loss = 0.0222\n",
      "Epoch 665: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 666: train loss = 0.0000, val loss = 0.0216\n",
      "Epoch 667: train loss = 0.0000, val loss = 0.0217\n",
      "Epoch 668: train loss = 0.0000, val loss = 0.0219\n",
      "Epoch 669: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 670: train loss = 0.0000, val loss = 0.0215\n",
      "Epoch 671: train loss = 0.0000, val loss = 0.0219\n",
      "Epoch 672: train loss = 0.0000, val loss = 0.0218\n",
      "Epoch 673: train loss = 0.0000, val loss = 0.0215\n",
      "Epoch 674: train loss = 0.0000, val loss = 0.0220\n",
      "Epoch 675: train loss = 0.0001, val loss = 0.0218\n",
      "Epoch 676: train loss = 0.0001, val loss = 0.0222\n",
      "Epoch 677: train loss = 0.0001, val loss = 0.0223\n",
      "Epoch 678: train loss = 0.0001, val loss = 0.0215\n",
      "Epoch 679: train loss = 0.0001, val loss = 0.0216\n",
      "Epoch 680: train loss = 0.0001, val loss = 0.0221\n",
      "Epoch 681: train loss = 0.0001, val loss = 0.0219\n",
      "Epoch 682: train loss = 0.0001, val loss = 0.0218\n",
      "Epoch 683: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 684: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 685: train loss = 0.0000, val loss = 0.0217\n",
      "Epoch 686: train loss = 0.0000, val loss = 0.0216\n",
      "Epoch 687: train loss = 0.0000, val loss = 0.0216\n",
      "Epoch 688: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 689: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 690: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 691: train loss = 0.0000, val loss = 0.0215\n",
      "Epoch 692: train loss = 0.0000, val loss = 0.0215\n",
      "Epoch 693: train loss = 0.0000, val loss = 0.0215\n",
      "Epoch 694: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 695: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 696: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 697: train loss = 0.0000, val loss = 0.0215\n",
      "Epoch 698: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 699: train loss = 0.0000, val loss = 0.0215\n",
      "Epoch 700: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 701: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 702: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 703: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 704: train loss = 0.0000, val loss = 0.0216\n",
      "Epoch 705: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 706: train loss = 0.0000, val loss = 0.0216\n",
      "Epoch 707: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 708: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 709: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 710: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 711: train loss = 0.0000, val loss = 0.0216\n",
      "Epoch 712: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 713: train loss = 0.0000, val loss = 0.0218\n",
      "Epoch 714: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 715: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 716: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 717: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 718: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 719: train loss = 0.0000, val loss = 0.0216\n",
      "Epoch 720: train loss = 0.0001, val loss = 0.0214\n",
      "Epoch 721: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 722: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 723: train loss = 0.0000, val loss = 0.0214\n",
      "Epoch 724: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 725: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 726: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 727: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 728: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 729: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 730: train loss = 0.0001, val loss = 0.0208\n",
      "Epoch 731: train loss = 0.0001, val loss = 0.0208\n",
      "Epoch 732: train loss = 0.0001, val loss = 0.0209\n",
      "Epoch 733: train loss = 0.0001, val loss = 0.0213\n",
      "Epoch 734: train loss = 0.0001, val loss = 0.0215\n",
      "Epoch 735: train loss = 0.0001, val loss = 0.0208\n",
      "Epoch 736: train loss = 0.0001, val loss = 0.0211\n",
      "Epoch 737: train loss = 0.0001, val loss = 0.0214\n",
      "Epoch 738: train loss = 0.0001, val loss = 0.0211\n",
      "Epoch 739: train loss = 0.0000, val loss = 0.0209\n",
      "Epoch 740: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 741: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 742: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 743: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 744: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 745: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 746: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 747: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 748: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 749: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 750: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 751: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 752: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 753: train loss = 0.0000, val loss = 0.0213\n",
      "Epoch 754: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 755: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 756: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 757: train loss = 0.0000, val loss = 0.0209\n",
      "Epoch 758: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 759: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 760: train loss = 0.0001, val loss = 0.0208\n",
      "Epoch 761: train loss = 0.0001, val loss = 0.0208\n",
      "Epoch 762: train loss = 0.0000, val loss = 0.0212\n",
      "Epoch 763: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 764: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 765: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 766: train loss = 0.0000, val loss = 0.0210\n",
      "Epoch 767: train loss = 0.0000, val loss = 0.0209\n",
      "Epoch 768: train loss = 0.0000, val loss = 0.0211\n",
      "Epoch 769: train loss = 0.0000, val loss = 0.0208\n",
      "Epoch 770: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 771: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 772: train loss = 0.0000, val loss = 0.0208\n",
      "Epoch 773: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 774: train loss = 0.0000, val loss = 0.0209\n",
      "Epoch 775: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 776: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 777: train loss = 0.0000, val loss = 0.0208\n",
      "Epoch 778: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 779: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 780: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 781: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 782: train loss = 0.0000, val loss = 0.0208\n",
      "Epoch 783: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 784: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 785: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 786: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 787: train loss = 0.0000, val loss = 0.0209\n",
      "Epoch 788: train loss = 0.0000, val loss = 0.0208\n",
      "Epoch 789: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 790: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 791: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 792: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 793: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 794: train loss = 0.0000, val loss = 0.0208\n",
      "Epoch 795: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 796: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 797: train loss = 0.0001, val loss = 0.0207\n",
      "Epoch 798: train loss = 0.0001, val loss = 0.0203\n",
      "Epoch 799: train loss = 0.0001, val loss = 0.0204\n",
      "Epoch 800: train loss = 0.0000, val loss = 0.0209\n",
      "Epoch 801: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 802: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 803: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 804: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 805: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 806: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 807: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 808: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 809: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 810: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 811: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 812: train loss = 0.0000, val loss = 0.0209\n",
      "Epoch 813: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 814: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 815: train loss = 0.0000, val loss = 0.0207\n",
      "Epoch 816: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 817: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 818: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 819: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 820: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 821: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 822: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 823: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 824: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 825: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 826: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 827: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 828: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 829: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 830: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 831: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 832: train loss = 0.0000, val loss = 0.0206\n",
      "Epoch 833: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 834: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 835: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 836: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 837: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 838: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 839: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 840: train loss = 0.0000, val loss = 0.0205\n",
      "Epoch 841: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 842: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 843: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 844: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 845: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 846: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 847: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 848: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 849: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 850: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 851: train loss = 0.0000, val loss = 0.0208\n",
      "Epoch 852: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 853: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 854: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 855: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 856: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 857: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 858: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 859: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 860: train loss = 0.0000, val loss = 0.0203\n",
      "Epoch 861: train loss = 0.0001, val loss = 0.0204\n",
      "Epoch 862: train loss = 0.0001, val loss = 0.0198\n",
      "Epoch 863: train loss = 0.0001, val loss = 0.0202\n",
      "Epoch 864: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 865: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 866: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 867: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 868: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 869: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 870: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 871: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 872: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 873: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 874: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 875: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 876: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 877: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 878: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 879: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 880: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 881: train loss = 0.0000, val loss = 0.0201\n",
      "Epoch 882: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 883: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 884: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 885: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 886: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 887: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 888: train loss = 0.0001, val loss = 0.0199\n",
      "Epoch 889: train loss = 0.0001, val loss = 0.0197\n",
      "Epoch 890: train loss = 0.0001, val loss = 0.0203\n",
      "Epoch 891: train loss = 0.0001, val loss = 0.0200\n",
      "Epoch 892: train loss = 0.0001, val loss = 0.0199\n",
      "Epoch 893: train loss = 0.0001, val loss = 0.0201\n",
      "Epoch 894: train loss = 0.0000, val loss = 0.0202\n",
      "Epoch 895: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 896: train loss = 0.0000, val loss = 0.0204\n",
      "Epoch 897: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 898: train loss = 0.0001, val loss = 0.0200\n",
      "Epoch 899: train loss = 0.0001, val loss = 0.0194\n",
      "Epoch 900: train loss = 0.0001, val loss = 0.0200\n",
      "Epoch 901: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 902: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 903: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 904: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 905: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 906: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 907: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 908: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 909: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 910: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 911: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 912: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 913: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 914: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 915: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 916: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 917: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 918: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 919: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 920: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 921: train loss = 0.0000, val loss = 0.0199\n",
      "Epoch 922: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 923: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 924: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 925: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 926: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 927: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 928: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 929: train loss = 0.0000, val loss = 0.0200\n",
      "Epoch 930: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 931: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 932: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 933: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 934: train loss = 0.0001, val loss = 0.0198\n",
      "Epoch 935: train loss = 0.0001, val loss = 0.0194\n",
      "Epoch 936: train loss = 0.0001, val loss = 0.0201\n",
      "Epoch 937: train loss = 0.0001, val loss = 0.0192\n",
      "Epoch 938: train loss = 0.0001, val loss = 0.0193\n",
      "Epoch 939: train loss = 0.0001, val loss = 0.0199\n",
      "Epoch 940: train loss = 0.0001, val loss = 0.0195\n",
      "Epoch 941: train loss = 0.0000, val loss = 0.0193\n",
      "Epoch 942: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 943: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 944: train loss = 0.0000, val loss = 0.0192\n",
      "Epoch 945: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 946: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 947: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 948: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 949: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 950: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 951: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 952: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 953: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 954: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 955: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 956: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 957: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 958: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 959: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 960: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 961: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 962: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 963: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 964: train loss = 0.0000, val loss = 0.0198\n",
      "Epoch 965: train loss = 0.0001, val loss = 0.0191\n",
      "Epoch 966: train loss = 0.0001, val loss = 0.0194\n",
      "Epoch 967: train loss = 0.0001, val loss = 0.0193\n",
      "Epoch 968: train loss = 0.0001, val loss = 0.0186\n",
      "Epoch 969: train loss = 0.0001, val loss = 0.0192\n",
      "Epoch 970: train loss = 0.0000, val loss = 0.0192\n",
      "Epoch 971: train loss = 0.0000, val loss = 0.0188\n",
      "Epoch 972: train loss = 0.0001, val loss = 0.0191\n",
      "Epoch 973: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 974: train loss = 0.0000, val loss = 0.0192\n",
      "Epoch 975: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 976: train loss = 0.0000, val loss = 0.0190\n",
      "Epoch 977: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 978: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 979: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 980: train loss = 0.0000, val loss = 0.0193\n",
      "Epoch 981: train loss = 0.0000, val loss = 0.0193\n",
      "Epoch 982: train loss = 0.0000, val loss = 0.0191\n",
      "Epoch 983: train loss = 0.0000, val loss = 0.0193\n",
      "Epoch 984: train loss = 0.0000, val loss = 0.0193\n",
      "Epoch 985: train loss = 0.0000, val loss = 0.0192\n",
      "Epoch 986: train loss = 0.0000, val loss = 0.0192\n",
      "Epoch 987: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 988: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 989: train loss = 0.0000, val loss = 0.0192\n",
      "Epoch 990: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 991: train loss = 0.0000, val loss = 0.0197\n",
      "Epoch 992: train loss = 0.0000, val loss = 0.0196\n",
      "Epoch 993: train loss = 0.0000, val loss = 0.0191\n",
      "Epoch 994: train loss = 0.0000, val loss = 0.0192\n",
      "Epoch 995: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 996: train loss = 0.0000, val loss = 0.0191\n",
      "Epoch 997: train loss = 0.0000, val loss = 0.0194\n",
      "Epoch 998: train loss = 0.0000, val loss = 0.0195\n",
      "Epoch 999: train loss = 0.0000, val loss = 0.0191\n",
      "Epoch 1000: train loss = 0.0000, val loss = 0.0192\n"
     ]
    }
   ],
   "source": [
    "model = train_final_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e0c8626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mood_predictions(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d4b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1917\n"
     ]
    }
   ],
   "source": [
    "from mood_RNN_classifier import get_accuracy_rate\n",
    "accuracy = get_accuracy_rate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39a1aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  predicted_mood_class\n",
      "0   AS14.01                    14\n",
      "1   AS14.02                    14\n",
      "2   AS14.03                    12\n",
      "3   AS14.05                    10\n",
      "4   AS14.06                    11\n",
      "5   AS14.07                    10\n",
      "6   AS14.08                    12\n",
      "7   AS14.09                    11\n",
      "8   AS14.12                    10\n",
      "9   AS14.13                    14\n",
      "10  AS14.14                    15\n",
      "11  AS14.15                    13\n",
      "12  AS14.16                    11\n",
      "13  AS14.17                    13\n",
      "14  AS14.19                    11\n",
      "15  AS14.20                    13\n",
      "16  AS14.23                    16\n",
      "17  AS14.24                    12\n",
      "18  AS14.25                    12\n",
      "19  AS14.26                    13\n",
      "20  AS14.27                    13\n",
      "21  AS14.28                    15\n",
      "22  AS14.29                    14\n",
      "23  AS14.30                    12\n",
      "24  AS14.31                    15\n",
      "25  AS14.32                    15\n",
      "26  AS14.33                    14\n"
     ]
    }
   ],
   "source": [
    "# Run predictions on test_df\n",
    "test_predictions = predict(model, test_df, id_map, device)\n",
    "\n",
    "# Attach predictions to test_df\n",
    "test_df_with_preds = test_df.copy()\n",
    "test_df_with_preds['predicted_mood_class'] = test_predictions\n",
    "\n",
    "# Optional: save to CSV or examine\n",
    "print(test_df_with_preds[['id', 'predicted_mood_class']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
